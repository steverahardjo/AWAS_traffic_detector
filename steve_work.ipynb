{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, window, lit\n",
    "from pyspark.sql.types import StringType\n",
    "from util import kafkaConsumer\n",
    "import pandas as pd\n",
    "from operations import SparkInst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_job=SparkInst(\"AWAS SYSTEM\", 5, kafka_output_topic=\"violations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_pd = pd.read_csv(\"data/camera.csv\")\n",
    "if '_id' in df_pd.columns:\n",
    "    df_pd.drop(columns=['_id'], inplace=True)\n",
    "spark_df = spark_job.get_session().createDataFrame(df_pd)\n",
    "\n",
    "# Step 3: Broadcast your speed limit map\n",
    "speed_limit_map = {row['camera_id']: row['speed_limit'] for row in spark_df.select(\"camera_id\", \"speed_limit\").collect()}\n",
    "broadcast_map = spark_job.essentialData_broadcast(spark_df)\n",
    "# Step 4: Define your UDF using broadcast variable\n",
    "def mark_speeding(camera_id:str, speed:float, ops:str):\n",
    "    limit = broadcast_map.value.get(camera_id)\n",
    "    if limit is not None and ops == \"instant\":\n",
    "        return \"INSTANT_VIOLATION\" if speed > limit else None\n",
    "    elif limit is not None and ops == \"average\":\n",
    "        return \"AVERAGE_VIOLATION\" if speed > limit else None\n",
    "    return \"NONE\"\n",
    "\n",
    "speeding_udf = udf(mark_speeding, StringType())\n",
    "\n",
    "# Step 5: Apply UDF to each streaming dataframe\n",
    "def add_speed_flag(df, ops: str):\n",
    "    return df.withColumn(\"speed_flag\", speeding_udf(col(\"camera_id\"), col(\"speed_reading\"), lit(ops)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, lit\n",
    "\n",
    "# Attach Kafka streams\n",
    "stream_a = spark_job.attach_kafka_stream(\"camera_events_a\", \"172.17.0.1\", \"5 minutes\")\n",
    "stream_b = spark_job.attach_kafka_stream(\"camera_events_b\", \"172.17.0.1\", \"5 minutes\")\n",
    "stream_c = spark_job.attach_kafka_stream(\"camera_events_c\", \"172.17.0.1\", \"5 minutes\")\n",
    "\n",
    "# Drop unnecessary columns and apply speed flag\n",
    "stream_a_flagged = add_speed_flag(stream_a.drop(\"batch_id\", \"event_id\"), \"instant\")\n",
    "stream_b_flagged = add_speed_flag(stream_b.drop(\"batch_id\", \"event_id\"), \"instant\")\n",
    "stream_c_flagged = add_speed_flag(stream_c.drop(\"batch_id\", \"event_id\"), \"instant\")\n",
    "\n",
    "# Duplicate stream_b for two separate joins\n",
    "dup1_b = stream_b_flagged\n",
    "dup2_b = stream_b_flagged\n",
    "\n",
    "# Rename timestamp and camera_id for join logic\n",
    "stream_a_renamed = stream_a_flagged \\\n",
    "    .withColumnRenamed(\"timestamp\", \"timestamp_start\") \\\n",
    "    .withColumnRenamed(\"camera_id\", \"camera_id_start\")\n",
    "\n",
    "stream_b1_renamed = dup1_b \\\n",
    "    .withColumnRenamed(\"timestamp\", \"timestamp_end\") \\\n",
    "    .withColumnRenamed(\"camera_id\", \"camera_id_end\")\n",
    "\n",
    "stream_c_renamed = stream_c_flagged \\\n",
    "    .withColumnRenamed(\"timestamp\", \"timestamp_end\") \\\n",
    "    .withColumnRenamed(\"camera_id\", \"camera_id_end\")\n",
    "\n",
    "stream_b2_renamed = dup2_b \\\n",
    "    .withColumnRenamed(\"timestamp\", \"timestamp_start\") \\\n",
    "    .withColumnRenamed(\"camera_id\", \"camera_id_start\")\n",
    "\n",
    "# Apply watermarks\n",
    "stream_a_watermarked = stream_a_renamed.withWatermark(\"timestamp_start\", \"10 hours\")\n",
    "stream_b1_watermarked = stream_b1_renamed.withWatermark(\"timestamp_end\", \"10 hours\")\n",
    "stream_c_watermarked = stream_c_renamed.withWatermark(\"timestamp_end\", \"10 hours\")\n",
    "stream_b2_watermarked = stream_b2_renamed.withWatermark(\"timestamp_start\", \"10 hours\")\n",
    "\n",
    "# Join A and B\n",
    "ab_join = stream_b1_watermarked.alias(\"b\").join(\n",
    "    stream_a_watermarked.alias(\"a\"),\n",
    "    (\n",
    "        (col(\"a.car_plate\") == col(\"b.car_plate\")) &\n",
    "        (col(\"a.timestamp_start\") > col(\"b.timestamp_end\")) &\n",
    "        (col(\"a.timestamp_start\") <= col(\"b.timestamp_end\") + expr(\"interval 10 minutes\"))\n",
    "    ),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "ab_join = ab_join.withColumn(\n",
    "    \"avg_speed_reading\",\n",
    "    (col(\"a.speed_reading\") + col(\"b.speed_reading\")) / 2\n",
    ").withColumn(\n",
    "    \"speed_flag\",\n",
    "    speeding_udf(col(\"a.camera_id_start\"), col(\"a.speed_reading\"), lit(\"average\"))\n",
    ")\n",
    "\n",
    "# Join B and C\n",
    "bc_join = stream_b2_watermarked.alias(\"b\").join(\n",
    "    stream_c_watermarked.alias(\"c\"),\n",
    "    (\n",
    "        (col(\"b.car_plate\") == col(\"c.car_plate\")) &\n",
    "        (col(\"c.timestamp_end\") > col(\"b.timestamp_start\")) &\n",
    "        (col(\"c.timestamp_end\") <= col(\"b.timestamp_start\") + expr(\"interval 10 minutes\"))\n",
    "    ),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "bc_join = bc_join.withColumn(\n",
    "    \"avg_speed_reading\",\n",
    "    (col(\"b.speed_reading\") + col(\"c.speed_reading\")) / 2\n",
    ").withColumn(\n",
    "    \"speed_flag\",\n",
    "    speeding_udf(col(\"c.camera_id_end\"), col(\"c.speed_reading\"), lit(\"average\"))\n",
    ")\n",
    "\n",
    "res = ab_join.union(bc_join)\n",
    "res = res.dropDuplicates()\n",
    "\n",
    "# Write to the console\n",
    "query = (\n",
    "    res.writeStream\n",
    "    .format(\"console\")\n",
    "    .option(\"checkpointLocation\", \"./stream_checkpoints_3\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"truncate\", False)  # Optional: show full column contents\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Run query and handle termination gracefully\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by CTRL-C. Stopping query.\")\n",
    "except StreamingQueryException as exc:\n",
    "    print(f\"Streaming error: {exc}\")\n",
    "finally:\n",
    "    query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
