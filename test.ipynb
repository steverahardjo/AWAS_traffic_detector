{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4315715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1 pyspark-shell'\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StringType, IntegerType, DoubleType, TimestampType, BooleanType\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    col, expr, from_json\n",
    ")\n",
    "import uuid\n",
    "\n",
    "class SparkInst:\n",
    "    def __init__(self, app_name: str, batch_interval: int, kafka_output_topic: str):\n",
    "        \"\"\"\n",
    "        Initializes a Spark instance with the given application name, batch interval, and Kafka topic.\n",
    "\n",
    "        Args:\n",
    "            app_name (str): The name of the Spark application.\n",
    "            batch_interval (int): The interval (in seconds) at which streaming data is processed.\n",
    "            kafka_topic (str): The name of the Kafka topic to consume from.\n",
    "        \"\"\"\n",
    "        os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1 pyspark-shell'\n",
    "        self.batch_interval = batch_interval\n",
    "        self.kafka_output_topic = kafka_output_topic\n",
    "        self.eventSchema= StructType() \\\n",
    "                        .add(\"batch_id\", IntegerType()) \\\n",
    "                        .add(\"event_id\", StringType()) \\\n",
    "                        .add(\"car_plate\", StringType()) \\\n",
    "                        .add(\"camera_id\", IntegerType()) \\\n",
    "                        .add(\"timestamp\", TimestampType()) \\\n",
    "                        .add(\"speed_reading\", DoubleType()) \\\n",
    "                        .add(\"producer_id\", StringType()) \\\n",
    "                        .add(\"sent_at\", TimestampType())\n",
    "        self.spark = SparkSession.builder.appName(app_name).master(\"local[*]\").getOrCreate()\n",
    "        \n",
    "        # immediately bump the KafkaDataConsumer logger to ERROR\n",
    "        sc = self.spark.sparkContext\n",
    "        jvm = sc._jvm\n",
    "        LogManager = jvm.org.apache.log4j.LogManager\n",
    "        Level      = jvm.org.apache.log4j.Level\n",
    "        kafka_logger = LogManager.getLogger(\"org.apache.spark.sql.kafka010.KafkaDataConsumer\")\n",
    "        kafka_logger.setLevel(Level.ERROR)\n",
    "        \n",
    "\n",
    "    def get_session(self):\n",
    "        return self.spark\n",
    "    \n",
    "    def attach_kafka_stream(self, topic_name:str, hostip:str, watermark_time:str):\n",
    "        return (\n",
    "            self.spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", f\"{hostip}:9092\")\n",
    "            .option(\"subscribe\", topic_name)\n",
    "            .option(\"startingOffsets\", \"earliest\")\n",
    "            .load()\n",
    "            .selectExpr(\"CAST(value AS STRING) as json\")\n",
    "            .select(from_json(col(\"json\"), self.eventSchema).alias(\"data\"))\n",
    "            .select(\"data.*\")\n",
    "            .withWatermark(\"timestamp\", watermark_time)\n",
    "        )\n",
    "    \n",
    "\n",
    "    def essentialData_broadcast(self, sdf):\n",
    "        \"\"\"\n",
    "        Filter a Spark DataFrame by topic_id and broadcast it.\n",
    "\n",
    "        Args:\n",
    "            sdf (DataFrame): Spark DataFrame\n",
    "\n",
    "        Returns:\n",
    "            Broadcast variable containing a dictionary of camera_id to speed_limit\n",
    "        \"\"\"\n",
    "        # Select necessary columns\n",
    "        df_filtered = sdf.select(\"camera_id\", \"speed_limit\")\n",
    "\n",
    "        # Convert to a Python dictionary (camera_id -> speed_limit)\n",
    "        data = df_filtered.rdd.map(lambda row: (row[\"camera_id\"], row[\"speed_limit\"])).collectAsMap()\n",
    "\n",
    "        # Broadcast the dictionary\n",
    "        spark_context = self.spark.sparkContext\n",
    "        return spark_context.broadcast(data)\n",
    "\n",
    "\n",
    "\n",
    "class DbWriter():\n",
    "    def __init__(self, mongo_host, mongo_port, mongo_db, mongo_coll):\n",
    "        self.mongo_host = mongo_host\n",
    "        self.mongo_port = mongo_port\n",
    "        self.mongo_db   = mongo_db\n",
    "        self.mongo_coll = mongo_coll\n",
    "        self.client     = None\n",
    "        self.violation_coll  = None\n",
    "\n",
    "    def open(self, partition_id: str, epoch_id: str) -> bool:\n",
    "        from pymongo import MongoClient\n",
    "        self.client = MongoClient(host=self.mongo_host, port=self.mongo_port)\n",
    "        self.violation_coll  = self.client[self.mongo_db][self.mongo_coll]\n",
    "        return True\n",
    "\n",
    "    def process(self, row):\n",
    "        try:\n",
    "            print(f\"\\nProcessing: {row.asDict()}\")\n",
    "            t_a = row.timestamp_a\n",
    "            t_b = row.timestamp_b\n",
    "            t_c = row.timestamp_c\n",
    "\n",
    "            if isinstance(t_a, str):\n",
    "                t_a = datetime.fromisoformat(t_a)\n",
    "\n",
    "            if isinstance(t_b, str):\n",
    "                t_b = datetime.fromisoformat(t_b)\n",
    "\n",
    "            if isinstance(t_c, str):\n",
    "                t_c = datetime.fromisoformat(t_c)\n",
    "\n",
    "            date_bucket_a = datetime(t_a.year, t_a.month, t_a.day)\n",
    "            date_bucket_b = datetime(t_b.year, t_b.month, t_b.day)\n",
    "            date_bucket_c = datetime(t_c.year, t_c.month, t_c.day)\n",
    "\n",
    "            violations_a = []\n",
    "            violations_b = []\n",
    "            violations_c = []\n",
    "\n",
    "            if row.speed_flag_instant_a:\n",
    "                violations_a.append({\n",
    "                    \"type\": \"instantaneous\",\n",
    "                    \"camera_id_start\": row.camera_id_a,\n",
    "                    \"camera_id_end\": None,\n",
    "                    \"timestamp_start\": t_a,\n",
    "                    \"timestamp_end\": None,\n",
    "                    \"measured_speed\": row.speed_reading_a\n",
    "                })\n",
    "            if row.speed_flag_instant_b:\n",
    "                violations_b.append({\n",
    "                    \"type\": \"instantaneous\",\n",
    "                    \"camera_id_start\": row.camera_id_b,\n",
    "                    \"camera_id_end\": None,\n",
    "                    \"timestamp_start\": t_b,\n",
    "                    \"timestamp_end\": None,\n",
    "                    \"measured_speed\": row.speed_reading_b\n",
    "                })\n",
    "            if row.speed_flag_instant_c:\n",
    "                violations_c.append({\n",
    "                    \"type\": \"instantaneous\",\n",
    "                    \"camera_id_start\": row.camera_id_c,\n",
    "                    \"camera_id_end\": None,\n",
    "                    \"timestamp_start\": t_c,\n",
    "                    \"timestamp_end\": None,\n",
    "                    \"measured_speed\": row.speed_reading_c\n",
    "                })\n",
    "            if row.speed_flag_average_ab:\n",
    "                violations_b.append({\n",
    "                    \"type\": \"average\",\n",
    "                    \"camera_id_start\": row.camera_id_a,\n",
    "                    \"camera_id_end\": row.camera_id_b,\n",
    "                    \"timestamp_start\": t_a,\n",
    "                    \"timestamp_end\": t_b,\n",
    "                    \"measured_speed\": row.avg_speed_reading_ab\n",
    "                })\n",
    "            if row.speed_flag_average_bc:\n",
    "                violations_c.append({\n",
    "                    \"type\": \"average\",\n",
    "                    \"camera_id_start\": row.camera_id_b,\n",
    "                    \"camera_id_end\": row.camera_id_c,\n",
    "                    \"timestamp_start\": t_b,\n",
    "                    \"timestamp_end\": t_c,\n",
    "                    \"measured_speed\": row.avg_speed_reading_bc\n",
    "                })\n",
    "\n",
    "            existing_a = self.violation_coll.find_one({\"car_plate\": row.car_plate, \"date\": date_bucket_a})\n",
    "            if existing_a and len(violations_a) > 0:\n",
    "                for violation in violations_a:\n",
    "                    existing_a[\"violations\"].append(violation)\n",
    "                    self.violation_coll.update_one(\n",
    "                        {\"car_plate\": row.car_plate, \"date\": date_bucket_a},\n",
    "                        {\"$set\": {\"violations\": existing_a[\"violations\"]}},\n",
    "                    )\n",
    "            elif len(violations_a) > 0:\n",
    "                self.violation_coll.insert_one(\n",
    "                    {\n",
    "                        \"violation_id\": str(uuid.uuid4()),  # or f\"{data['car_plate']}_{date_bucket.date()}\"\n",
    "                        \"car_plate\":    row.car_plate,\n",
    "                        \"date\":         date_bucket_a,\n",
    "                        \"violations\":   violations_a\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            existing_b = self.violation_coll.find_one({\"car_plate\": row.car_plate, \"date\": date_bucket_b})\n",
    "            if existing_b and len(violations_b) > 0:\n",
    "                for violation in violations_b:\n",
    "                    existing_b[\"violations\"].append(violation)\n",
    "                    self.violation_coll.update_one(\n",
    "                        {\"car_plate\": row.car_plate, \"date\": date_bucket_b},\n",
    "                        {\"$set\": {\"violations\": existing_b[\"violations\"]}},\n",
    "                    )\n",
    "            elif len(violations_b) > 0:\n",
    "                self.violation_coll.insert_one(\n",
    "                    {\n",
    "                        \"violation_id\": str(uuid.uuid4()),  # or f\"{data['car_plate']}_{date_bucket.date()}\"\n",
    "                        \"car_plate\":    row.car_plate,\n",
    "                        \"date\":         date_bucket_b,\n",
    "                        \"violations\":   violations_b\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            existing_c = self.violation_coll.find_one({\"car_plate\": row.car_plate, \"date\": date_bucket_c})                                    \n",
    "            if existing_c and len(violations_c) > 0:\n",
    "                for violation in violations_c:\n",
    "                    existing_c[\"violations\"].append(violation)\n",
    "                    self.violation_coll.update_one(\n",
    "                        {\"car_plate\": row.car_plate, \"date\": date_bucket_c},\n",
    "                        {\"$set\": {\"violations\": existing_c[\"violations\"]}},\n",
    "                    )\n",
    "            elif len(violations_c) > 0:\n",
    "                self.violation_coll.insert_one(\n",
    "                    {\n",
    "                        \"violation_id\": str(uuid.uuid4()),  # or f\"{data['car_plate']}_{date_bucket.date()}\"\n",
    "                        \"car_plate\":    row.car_plate,\n",
    "                        \"date\":         date_bucket_c,\n",
    "                        \"violations\":   violations_c\n",
    "                    }\n",
    "                )\n",
    "#             print(f\"\\nAdded violations: {sum([len(violations_a),len(violations_b),len(violations_c)])}\")\n",
    "            if sum([len(violations_a),len(violations_b),len(violations_c)] == 0 :\n",
    "                   print(\"No violations detected for {row.car_plate} from {t_a} to {t_c}\")\n",
    "        except Exception as e:\n",
    "            # this will print on the executor logs\n",
    "            print(f\"[DbWriter][ERROR] failed to process row {row}: {e}\")\n",
    "            # optionally, you could write to a dead‐letter collection instead\n",
    "                                                  \n",
    "    def close(self, error):\n",
    "        if error:\n",
    "            # this also shows up in the executor log\n",
    "            print(f\"[DbWriter][ERROR] task shutting down due to: {error}\")\n",
    "        if self.client:\n",
    "            self.client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46ea4f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1 pyspark-shell'\n",
    "spark_job=SparkInst(\"AWAS SYSTEM\", 5, kafka_output_topic=\"violations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7c4b1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# add the folder where util.py lives\n",
    "from pyspark.sql.functions import udf, col, window, lit\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "\n",
    "df_pd = pd.read_csv(\"data/camera.csv\")\n",
    "if '_id' in df_pd.columns:\n",
    "    df_pd.drop(columns=['_id'], inplace=True)\n",
    "spark_df = spark_job.get_session().createDataFrame(df_pd)\n",
    "\n",
    "speed_limit_map = {row['camera_id']: row['speed_limit'] for row in spark_df.select(\"camera_id\", \"speed_limit\").collect()}\n",
    "broadcast_map = spark_job.essentialData_broadcast(spark_df)\n",
    "\n",
    "def mark_speeding(camera_id:str, speed:float, ops:str)-> str:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    limit = broadcast_map.value.get(camera_id)\n",
    "    if limit is not None and ops == \"instant\":\n",
    "        return True if speed > limit else False\n",
    "    elif limit is not None and ops == \"average\":\n",
    "        return True  if speed > limit else False\n",
    "    return False\n",
    "\n",
    "speeding_udf = udf(mark_speeding, BooleanType())\n",
    "\n",
    "# Step 5: Apply UDF to each streaming dataframe\n",
    "def add_speed_flag(df, ops: str):\n",
    "    return df.withColumn(f\"speed_flag_{ops}\", speeding_udf(col(\"camera_id\"), col(\"speed_reading\"), lit(ops)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61183149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, lit\n",
    "\n",
    "# Attach Kafka streams\n",
    "stream_a = spark_job.attach_kafka_stream(\"camera_event_a\", \"172.22.32.1\", \"24 hours\")\n",
    "stream_b = spark_job.attach_kafka_stream(\"camera_event_b\", \"172.22.32.1\", \"24 hours\")\n",
    "stream_c = spark_job.attach_kafka_stream(\"camera_event_c\", \"172.22.32.1\", \"24 hours\")\n",
    "from pyspark.sql.functions import expr, col, lit\n",
    "\n",
    "# Flag and drop unnecessary fields\n",
    "# stream_a_flagged = add_speed_flag(stream_a.drop(\"batch_id\", \"event_id\", \"sent_at\"), \"instant\")\n",
    "# stream_b_flagged = add_speed_flag(stream_b.drop(\"batch_id\", \"event_id\", \"sent_at\"), \"instant\")\n",
    "# stream_c_flagged = add_speed_flag(stream_c.drop(\"batch_id\", \"event_id\", \"sent_at\"), \"instant\")\n",
    "stream_a_flagged = add_speed_flag(stream_a.drop(\"event_id\", \"batch_id\", \"sent_at\"), \"instant\")\n",
    "stream_b_flagged = add_speed_flag(stream_b.drop(\"event_id\", \"batch_id\", \"sent_at\"), \"instant\")\n",
    "stream_c_flagged = add_speed_flag(stream_c.drop(\"event_id\", \"batch_id\", \"sent_at\"), \"instant\")\n",
    "\n",
    "# Rename for joining\n",
    "a = stream_a_flagged.selectExpr(\n",
    "    \"car_plate\",\n",
    "    \"batch_id as batch_id_a\",\n",
    "    \"camera_id as camera_id_a\",\n",
    "    \"timestamp as timestamp_a\",\n",
    "    \"speed_reading as speed_reading_a\",\n",
    "    \"producer_id as producer_a\",\n",
    "    \"speed_flag_instant as speed_flag_instant_a\"\n",
    ")\n",
    "\n",
    "b = stream_b_flagged.selectExpr(\n",
    "    \"car_plate\",\n",
    "    \"batch_id as batch_id_b\",\n",
    "    \"camera_id as camera_id_b\",\n",
    "    \"timestamp as timestamp_b\",\n",
    "    \"speed_reading as speed_reading_b\",\n",
    "    \"producer_id as producer_b\",\n",
    "    \"speed_flag_instant as speed_flag_instant_b\"\n",
    ")\n",
    "\n",
    "c = stream_c_flagged.selectExpr(\n",
    "    \"car_plate\",\n",
    "    \"batch_id as batch_id_c\",\n",
    "    \"camera_id as camera_id_c\",\n",
    "    \"timestamp as timestamp_c\",\n",
    "    \"speed_reading as speed_reading_c\",\n",
    "    \"producer_id as producer_c\",\n",
    "    \"speed_flag_instant as speed_flag_instant_c\"\n",
    ")\n",
    "\n",
    "# Join A & B\n",
    "ab_join = b.alias(\"b\").join(\n",
    "    a.alias(\"a\"),\n",
    "    (col(\"a.car_plate\") == col(\"b.car_plate\")) &\n",
    "    (col(\"a.timestamp_a\") < col(\"b.timestamp_b\")) &\n",
    "    (col(\"b.timestamp_b\") <= col(\"a.timestamp_a\") + expr(\"interval 10 minutes\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"a.car_plate\"),\n",
    "    col(\"a.batch_id_a\"),\n",
    "    col(\"a.camera_id_a\"),\n",
    "    col(\"a.timestamp_a\"),\n",
    "    col(\"a.speed_reading_a\"),\n",
    "    col(\"a.speed_flag_instant_a\"),\n",
    "    ((col(\"a.speed_reading_a\") + col(\"b.speed_reading_b\")) / 2).alias(\"avg_speed_reading_ab\"),\n",
    "    speeding_udf(\n",
    "        col(\"a.camera_id_a\"),\n",
    "        ((col(\"a.speed_reading_a\") + col(\"b.speed_reading_b\")) / 2),\n",
    "        lit(\"average\")\n",
    "    ).alias(\"speed_flag_average_ab\"),\n",
    "    col(\"b.batch_id_b\"),\n",
    "    col(\"b.camera_id_b\"),\n",
    "    col(\"b.timestamp_b\"),\n",
    "    col(\"b.speed_reading_b\"),\n",
    "    col(\"b.speed_flag_instant_b\")\n",
    ")\n",
    "\n",
    "# Join AB & C\n",
    "abc_join = ab_join.alias(\"ab\").join(\n",
    "    c.alias(\"c\"),\n",
    "    (col(\"ab.car_plate\") == col(\"c.car_plate\")) &\n",
    "    (col(\"c.timestamp_c\") > col(\"ab.timestamp_b\")) &\n",
    "    (col(\"c.timestamp_c\") <= col(\"ab.timestamp_b\") + expr(\"interval 10 minutes\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"ab.*\"),\n",
    "    ((col(\"ab.speed_reading_b\") + col(\"c.speed_reading_c\")) / 2).alias(\"avg_speed_reading_bc\"),\n",
    "    speeding_udf(\n",
    "        col(\"ab.camera_id_b\"),\n",
    "        ((col(\"ab.speed_reading_b\") + col(\"c.speed_reading_c\")) / 2),\n",
    "        lit(\"average\")\n",
    "    ).alias(\"speed_flag_average_bc\"),\n",
    "    col(\"c.batch_id_c\"),\n",
    "    col(\"c.camera_id_c\"),\n",
    "    col(\"c.timestamp_c\"),\n",
    "    col(\"c.speed_reading_c\"),\n",
    "    col(\"c.speed_flag_instant_c\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f96ac44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing checkpoint directory: ./stream_checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by CTRL-C. Stopping query.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql.streaming import StreamingQueryException\n",
    "\n",
    "checkpoint_dir = \"./stream_checkpoints\"\n",
    "\n",
    "# 1) Clean up any existing checkpoint directory before starting\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    print(f\"Deleted existing checkpoint directory: {checkpoint_dir}\")\n",
    "\n",
    "# Write to the console\n",
    "query = (\n",
    "    abc_join.writeStream\n",
    "    .format(\"console\")\n",
    "    .option(\"checkpointLocation\", \"./stream_checkpoints\")\n",
    "    .outputMode(\"append\")\n",
    "    .foreachBatch(DbWriter(\n",
    "        mongo_host=\"172.22.32.1\",\n",
    "        mongo_port=27017,\n",
    "        mongo_db=\"fit3182_db\",\n",
    "        mongo_coll=\"Violation\"\n",
    "    ))\n",
    "    .option(\"numRows\", 1000)\n",
    "    .option(\"truncate\", False)  # Optional: show full column contents\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Write to the console\n",
    "# query = (\n",
    "#     abc_join.writeStream\n",
    "#     .format(\"console\")\n",
    "#     .option(\"checkpointLocation\", \"./stream_checkpoints\")\n",
    "#     .outputMode(\"append\")\n",
    "#     .option(\"numRows\", 1000)\n",
    "#     .option(\"truncate\", False)  # Optional: show full column contents\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "# Run query and handle termination gracefully\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by CTRL-C. Stopping query.\")\n",
    "except StreamingQueryException as exc:\n",
    "    print(f\"Streaming error: {exc}\")\n",
    "finally:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5fbe0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
