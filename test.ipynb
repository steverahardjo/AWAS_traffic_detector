{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d8dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import csv\n",
    "client = MongoClient(\"172.27.65.143\", 27017)\n",
    "db = client.fit3182_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ── Your starter connection ──\n",
    "camera_coll = db.Camera\n",
    "\n",
    "# ── Skip if already imported ──\n",
    "if camera_coll.estimated_document_count() > 0:\n",
    "    print(\"Camera collection already contains data. Skipping import.\")\n",
    "else:\n",
    "    idx_name = camera_coll.create_index(\n",
    "        [(\"pos\", pymongo.ASCENDING)],\n",
    "        name=\"pos_idx\"\n",
    "    )\n",
    "    print(f\"Ensured index on 'pos': {idx_name}\")\n",
    "    \n",
    "    # ── Load CSV and insert ──\n",
    "    csv_path = 'data/camera.csv'\n",
    "    docs = []\n",
    "    with open(csv_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            docs.append({\n",
    "                \"_id\":        int(row['camera_id']),\n",
    "                \"lat\":        float(row['latitude']),\n",
    "                \"long\":       float(row['longitude']),\n",
    "                \"pos\":        float(row['position']),\n",
    "                \"speed_limit\": int(row['speed_limit'])\n",
    "            })\n",
    "\n",
    "    if docs:\n",
    "        result = camera_coll.insert_many(docs)\n",
    "        print(f\"Inserted {len(result.inserted_ids)} camera documents.\")\n",
    "        print(\"Current indexes on Camera:\")\n",
    "        for name, info in camera_coll.index_information().items():\n",
    "            print(f\" • {name}: {info['key']}\")\n",
    "    else:\n",
    "        print(\"No camera records found in CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared collection. Deleted 868 documents.\n",
      "Inserted 9844 new vehicle documents.\n",
      "Updated 69 existing vehicle documents.\n",
      "Current indexes on Vehicle:\n",
      " • _id_: [('_id', 1)]\n"
     ]
    }
   ],
   "source": [
    "# ── Connection ──\n",
    "vehicle_coll = db.Vehicle\n",
    "\n",
    "#clear out the collection first \n",
    "deleted = vehicle_coll.delete_many({})\n",
    "print(f\"Cleared collection. Deleted {deleted.deleted_count} documents.\")\n",
    "\n",
    "if vehicle_coll.estimated_document_count() > 0:\n",
    "    print(\"Vehicle collection already contains data. Skipping import.\")\n",
    "else:\n",
    "    # ── Prepare sets & counters ──\n",
    "    existing_ids = set(vehicle_coll.distinct('_id'))\n",
    "    seen_in_file = set()\n",
    "    docs_to_insert = []\n",
    "    update_count = 0\n",
    "    added_count = 0\n",
    "\n",
    "    csv_path = 'data/vehicle.csv'\n",
    "    with open(csv_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            plate = row['car_plate']\n",
    "\n",
    "            # Parse the incoming registration_date\n",
    "            ts = row['registration_date'].rstrip(\"Z\")\n",
    "            reg_date = datetime.fromisoformat(ts)\n",
    "\n",
    "            if plate in seen_in_file:\n",
    "                # Plate already in DB → check whether to update\n",
    "                existing = vehicle_coll.find_one(\n",
    "                    {\"_id\": plate}\n",
    "                )\n",
    "                if existing and reg_date > existing['registration_date']:\n",
    "                    # Only update if the CSV date is newer\n",
    "                    vehicle_coll.update_one(\n",
    "                        {\"_id\": plate},\n",
    "                        {\"$set\": {\n",
    "                            \"registration_date\": reg_date,\n",
    "                            \"owner_name\":        row['owner_name'],\n",
    "                            \"owner_addr\":        row['owner_addr'],\n",
    "                            \"vehicle_type\":      row['vehicle_type']\n",
    "                        }}\n",
    "                    )\n",
    "                    update_count += 1\n",
    "            else:\n",
    "                seen_in_file.add(plate)\n",
    "                # Brand-new plate → schedule for insert\n",
    "                vehicle_coll.insert_one({\n",
    "                    \"_id\":               plate,\n",
    "                    \"owner_name\":        row['owner_name'],\n",
    "                    \"owner_addr\":        row['owner_addr'],\n",
    "                    \"vehicle_type\":      row['vehicle_type'],\n",
    "                    \"registration_date\": reg_date\n",
    "                })\n",
    "                added_count += 1\n",
    "\n",
    "    # ── Do the batch insert, if any ──\n",
    "    if added_count > 0:\n",
    "        print(f\"Inserted {added_count} new vehicle documents.\")\n",
    "    else:\n",
    "        print(\"No new vehicle records to insert.\")\n",
    "\n",
    "    # ── Report on any updates we made ──\n",
    "    if update_count:\n",
    "        print(f\"Updated {update_count} existing vehicle document{'s' if update_count>1 else ''}.\")\n",
    "\n",
    "    # ── (Optional) show your indexes ──\n",
    "    print(\"Current indexes on Vehicle:\")\n",
    "    for name, info in vehicle_coll.index_information().items():\n",
    "        print(f\" • {name}: {info['key']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c20a5",
   "metadata": {},
   "source": [
    "# Create Violation and put historic.csv into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4757a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared collection. Deleted 50000 documents.\n",
      "Inserted 50000 violation documents.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# ── MongoDB Connection ──\n",
    "client = MongoClient(\"172.27.65.143\", 27017)\n",
    "db = client.fit3182_db\n",
    "violation_coll = db.Violation\n",
    "#clear out the collection first \n",
    "deleted = violation_coll.delete_many({})\n",
    "print(f\"Cleared collection. Deleted {deleted.deleted_count} documents.\")\n",
    "\n",
    "# ── CSV Read ──\n",
    "csv_path = \"data/camera_event_historic.csv\"\n",
    "docs = []\n",
    "\n",
    "with open(csv_path, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        # Parse timestamp_start to datetime\n",
    "        timestamp_start = datetime.fromisoformat(row['timestamp_start'].rstrip(\"Z\"))\n",
    "        timestamp_end = datetime.fromisoformat(row['timestamp_end'].rstrip(\"Z\")) if row.get('timestamp_end') else None\n",
    "\n",
    "        # Create date bucket from timestamp_start (just the date part)\n",
    "        date_bucket = datetime(timestamp_start.year, timestamp_start.month, timestamp_start.day)\n",
    "\n",
    "        # Construct document\n",
    "        violation_doc = {\n",
    "            \"car_plate\": row['car_plate'],\n",
    "            \"date\": date_bucket,\n",
    "            \"violations\": [\n",
    "                {\n",
    "                    \"violation_id\": str(uuid.uuid4()),\n",
    "                    \"type\": \"average\",\n",
    "                    \"camera_id_start\": row['camera_id_start'],\n",
    "                    \"camera_id_end\": row['camera_id_end'] if row.get('camera_id_end') else None,\n",
    "                    \"timestamp_start\": timestamp_start,\n",
    "                    \"timestamp_end\": timestamp_end,\n",
    "                    \"measured_speed\": float(row['speed_reading']) if row.get('speed_reading') else None,                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        docs.append(violation_doc)\n",
    "\n",
    "# ── Insert All Documents ──\n",
    "if docs:\n",
    "    violation_coll.insert_many(docs)\n",
    "    print(f\"Inserted {len(docs)} violation documents.\")\n",
    "else:\n",
    "    print(\"No documents to insert.\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4315715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1 pyspark-shell'\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StringType, IntegerType, DoubleType, TimestampType, BooleanType\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    col, expr, from_json\n",
    ")\n",
    "import uuid\n",
    "\n",
    "class SparkInst:\n",
    "    def __init__(self, app_name: str, batch_interval: int, kafka_output_topic: str):\n",
    "        \"\"\"\n",
    "        Initializes a Spark instance with the given application name, batch interval, and Kafka topic.\n",
    "\n",
    "        Args:\n",
    "            app_name (str): The name of the Spark application.\n",
    "            batch_interval (int): The interval (in seconds) at which streaming data is processed.\n",
    "            kafka_topic (str): The name of the Kafka topic to consume from.\n",
    "        \"\"\"\n",
    "        os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1 pyspark-shell'\n",
    "        self.batch_interval = batch_interval\n",
    "        self.kafka_output_topic = kafka_output_topic\n",
    "        self.eventSchema= StructType() \\\n",
    "                        .add(\"batch_id\", IntegerType()) \\\n",
    "                        .add(\"event_id\", StringType()) \\\n",
    "                        .add(\"car_plate\", StringType()) \\\n",
    "                        .add(\"camera_id\", IntegerType()) \\\n",
    "                        .add(\"timestamp\", TimestampType()) \\\n",
    "                        .add(\"speed_reading\", DoubleType()) \\\n",
    "                        .add(\"producer_id\", StringType()) \\\n",
    "                        .add(\"sent_at\", TimestampType())\n",
    "        self.spark = SparkSession.builder.appName(app_name).master(\"local[*]\").getOrCreate()\n",
    "        \n",
    "        # immediately bump the KafkaDataConsumer logger to ERROR\n",
    "        sc = self.spark.sparkContext\n",
    "        jvm = sc._jvm\n",
    "        LogManager = jvm.org.apache.log4j.LogManager\n",
    "        Level      = jvm.org.apache.log4j.Level\n",
    "        kafka_logger = LogManager.getLogger(\"org.apache.spark.sql.kafka010.KafkaDataConsumer\")\n",
    "        kafka_logger.setLevel(Level.ERROR)\n",
    "        \n",
    "\n",
    "    def get_session(self):\n",
    "        return self.spark\n",
    "    \n",
    "    def attach_kafka_stream(self, topic_name:str, hostip:str, watermark_time:str):\n",
    "        return (\n",
    "            self.spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", f\"{hostip}:9092\")\n",
    "            .option(\"subscribe\", topic_name)\n",
    "            .option(\"startingOffsets\", \"earliest\")\n",
    "            .load()\n",
    "            .selectExpr(\"CAST(value AS STRING) as json\")\n",
    "            .select(from_json(col(\"json\"), self.eventSchema).alias(\"data\"))\n",
    "            .select(\"data.*\")\n",
    "            .withWatermark(\"timestamp\", watermark_time)\n",
    "        )\n",
    "    \n",
    "\n",
    "    def essentialData_broadcast(self, sdf):\n",
    "        \"\"\"\n",
    "        Filter a Spark DataFrame by topic_id and broadcast it.\n",
    "\n",
    "        Args:\n",
    "            sdf (DataFrame): Spark DataFrame\n",
    "\n",
    "        Returns:\n",
    "            Broadcast variable containing a dictionary of camera_id to speed_limit\n",
    "        \"\"\"\n",
    "        # Select necessary columns\n",
    "        df_filtered = sdf.select(\"camera_id\", \"speed_limit\")\n",
    "\n",
    "        # Convert to a Python dictionary (camera_id -> speed_limit)\n",
    "        data = df_filtered.rdd.map(lambda row: (row[\"camera_id\"], row[\"speed_limit\"])).collectAsMap()\n",
    "\n",
    "        # Broadcast the dictionary\n",
    "        spark_context = self.spark.sparkContext\n",
    "        return spark_context.broadcast(data)\n",
    "\n",
    "\n",
    "\n",
    "class DbWriter():\n",
    "    def __init__(self, mongo_host, mongo_port, mongo_db, mongo_coll):\n",
    "        self.mongo_host = mongo_host\n",
    "        self.mongo_port = mongo_port\n",
    "        self.mongo_db   = mongo_db\n",
    "        self.mongo_coll = mongo_coll\n",
    "        self.client     = None\n",
    "        self.violation_coll  = None\n",
    "\n",
    "    def open(self, partition_id: str, epoch_id: str) -> bool:\n",
    "        self.client = MongoClient(host=self.mongo_host, port=self.mongo_port)\n",
    "        self.violation_coll  = self.client[self.mongo_db][self.mongo_coll]\n",
    "\n",
    "        self.violation_coll.create_index([(\"violation_id\", 1)],unique=True, name=\"idx_violation_id\")\n",
    "        self.violation_coll.create_index([(\"date\", 1)],name=\"idx_date\")\n",
    "        self.violation_coll.create_index([(\"violations.camera_id_start\", 1)],name=\"idx_camera_start\")\n",
    "        self.violation_coll.create_index([(\"violations.camera_id_end\", 1)],name=\"idx_camera_end\")\n",
    "        self.violation_coll.create_index([(\"date\", 1), (\"violations.measured_speed\", -1)], name=\"idx_measured_speed\")\n",
    "        self.violation_coll.create_index([(\"violations.timestamp_start\", 1)])\n",
    "        return True\n",
    "\n",
    "    def process(self, row):\n",
    "        try:\n",
    "            print(f\"\\nProcessing: {row.asDict()}\")\n",
    "            t_a = row.timestamp_a\n",
    "            t_b = row.timestamp_b\n",
    "            t_c = row.timestamp_c\n",
    "\n",
    "            if isinstance(t_a, str):\n",
    "                t_a = datetime.fromisoformat(t_a)\n",
    "\n",
    "            if isinstance(t_b, str):\n",
    "                t_b = datetime.fromisoformat(t_b)\n",
    "\n",
    "            if isinstance(t_c, str):\n",
    "                t_c = datetime.fromisoformat(t_c)\n",
    "\n",
    "            date_bucket_a = datetime(t_a.year, t_a.month, t_a.day)\n",
    "            date_bucket_b = datetime(t_b.year, t_b.month, t_b.day)\n",
    "            date_bucket_c = datetime(t_c.year, t_c.month, t_c.day)\n",
    "\n",
    "            violations_a = []\n",
    "            violations_b = []\n",
    "            violations_c = []\n",
    "\n",
    "            if row.speed_flag_instant_a:\n",
    "                violations_a.append({\n",
    "                    \"violation_id\": str(uuid.uuid4()),\n",
    "                    \"type\": \"instantaneous\",\n",
    "                    \"camera_id_start\": row.camera_id_a,\n",
    "                    \"camera_id_end\": None,\n",
    "                    \"timestamp_start\": t_a,\n",
    "                    \"timestamp_end\": None,\n",
    "                    \"measured_speed\": row.speed_reading_a\n",
    "                })\n",
    "            if row.speed_flag_instant_b:\n",
    "                violations_b.append({\n",
    "                    \"violation_id\": str(uuid.uuid4()),\n",
    "                    \"type\": \"instantaneous\",\n",
    "                    \"camera_id_start\": row.camera_id_b,\n",
    "                    \"camera_id_end\": None,\n",
    "                    \"timestamp_start\": t_b,\n",
    "                    \"timestamp_end\": None,\n",
    "                    \"measured_speed\": row.speed_reading_b\n",
    "                })\n",
    "            if row.speed_flag_instant_c:\n",
    "                violations_c.append({\n",
    "                    \"violation_id\": str(uuid.uuid4()),\n",
    "                    \"type\": \"instantaneous\",\n",
    "                    \"camera_id_start\": row.camera_id_c,\n",
    "                    \"camera_id_end\": None,\n",
    "                    \"timestamp_start\": t_c,\n",
    "                    \"timestamp_end\": None,\n",
    "                    \"measured_speed\": row.speed_reading_c\n",
    "                })\n",
    "            if row.speed_flag_average_ab:\n",
    "                violations_b.append({\n",
    "                    \"violation_id\": str(uuid.uuid4()),\n",
    "                    \"type\": \"average\",\n",
    "                    \"camera_id_start\": row.camera_id_a,\n",
    "                    \"camera_id_end\": row.camera_id_b,\n",
    "                    \"timestamp_start\": t_a,\n",
    "                    \"timestamp_end\": t_b,\n",
    "                    \"measured_speed\": row.avg_speed_reading_ab\n",
    "                })\n",
    "            if row.speed_flag_average_bc:\n",
    "                violations_c.append({\n",
    "                    \"violation_id\": str(uuid.uuid4()),\n",
    "                    \"type\": \"average\",\n",
    "                    \"camera_id_start\": row.camera_id_b,\n",
    "                    \"camera_id_end\": row.camera_id_c,\n",
    "                    \"timestamp_start\": t_b,\n",
    "                    \"timestamp_end\": t_c,\n",
    "                    \"measured_speed\": row.avg_speed_reading_bc\n",
    "                })\n",
    "\n",
    "            existing_a = self.violation_coll.find_one({\"car_plate\": row.car_plate, \"date\": date_bucket_a})\n",
    "            if existing_a and len(violations_a) > 0:\n",
    "                for violation in violations_a:\n",
    "                    existing_a[\"violations\"].append(violation)\n",
    "                    self.violation_coll.update_one(\n",
    "                        {\"car_plate\": row.car_plate, \"date\": date_bucket_a},\n",
    "                        {\"$set\": {\"violations\": existing_a[\"violations\"]}},\n",
    "                    )\n",
    "            elif len(violations_a) > 0:\n",
    "                self.violation_coll.insert_one(\n",
    "                    {\n",
    "                        \"violation_id\": str(uuid.uuid4()),\n",
    "                        \"car_plate\":    row.car_plate,\n",
    "                        \"date\":         date_bucket_a,\n",
    "                        \"violations\":   violations_a\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            existing_b = self.violation_coll.find_one({\"car_plate\": row.car_plate, \"date\": date_bucket_b})\n",
    "            if existing_b and len(violations_b) > 0:\n",
    "                for violation in violations_b:\n",
    "                    existing_b[\"violations\"].append(violation)\n",
    "                    self.violation_coll.update_one(\n",
    "                        {\"car_plate\": row.car_plate, \"date\": date_bucket_b},\n",
    "                        {\"$set\": {\"violations\": existing_b[\"violations\"]}},\n",
    "                    )\n",
    "            elif len(violations_b) > 0:\n",
    "                self.violation_coll.insert_one(\n",
    "                    {\n",
    "                        \"car_plate\":    row.car_plate,\n",
    "                        \"date\":         date_bucket_b,\n",
    "                        \"violations\":   violations_b\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            existing_c = self.violation_coll.find_one({\"car_plate\": row.car_plate, \"date\": date_bucket_c})                                    \n",
    "            if existing_c and len(violations_c) > 0:\n",
    "                for violation in violations_c:\n",
    "                    existing_c[\"violations\"].append(violation)\n",
    "                    self.violation_coll.update_one(\n",
    "                        {\"car_plate\": row.car_plate, \"date\": date_bucket_c},\n",
    "                        {\"$set\": {\"violations\": existing_c[\"violations\"]}},\n",
    "                    )\n",
    "            elif len(violations_c) > 0:\n",
    "                self.violation_coll.insert_one(\n",
    "                    {\n",
    "                        \"violation_id\": str(uuid.uuid4()),  # or f\"{data['car_plate']}_{date_bucket.date()}\"\n",
    "                        \"car_plate\":    row.car_plate,\n",
    "                        \"date\":         date_bucket_c,\n",
    "                        \"violations\":   violations_c\n",
    "                    }\n",
    "                )\n",
    "#             print(f\"\\nAdded violations: {sum([len(violations_a),len(violations_b),len(violations_c)])}\")\n",
    "            if sum([len(violations_a),len(violations_b),len(violations_c)]) == 0 :\n",
    "                   print(\"No violations detected for {row.car_plate} from {t_a} to {t_c}\")\n",
    "        except Exception as e:\n",
    "            # this will print on the executor logs\n",
    "            print(f\"[DbWriter][ERROR] failed to process row {row}: {e}\")\n",
    "            # optionally, you could write to a dead‐letter collection instead\n",
    "                                                  \n",
    "    def close(self, error):\n",
    "        if error:\n",
    "            # this also shows up in the executor log\n",
    "            print(f\"[DbWriter][ERROR] task shutting down due to: {error}\")\n",
    "        if self.client:\n",
    "            self.client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46ea4f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1 pyspark-shell'\n",
    "spark_job=SparkInst(\"AWAS SYSTEM\", 5, kafka_output_topic=\"violations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7c4b1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# add the folder where util.py lives\n",
    "from pyspark.sql.functions import udf, col, window, lit\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "\n",
    "camera_coll=client.fit3182_db.Camera\n",
    "cursor = camera_coll.find()\n",
    "df_pd = pd.DataFrame(list(cursor))\n",
    "#rename mongo index _id as camera_id\n",
    "if '_id' in df_pd.columns:\n",
    "    df_pd.rename(columns={'_id': 'camera_id'}, inplace=True)\n",
    "\n",
    "# Convert the pandas DataFrame into a Spark DataFrame\n",
    "spark_df = spark_job.get_session().createDataFrame(df_pd)\n",
    "\n",
    "\n",
    "speed_limit_map = {row['camera_id']: row['speed_limit'] for row in spark_df.select(\"camera_id\", \"speed_limit\").collect()}\n",
    "broadcast_map = spark_job.essentialData_broadcast(spark_df)\n",
    "\n",
    "def mark_speeding(camera_id:str, speed:float, ops:str)-> str:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    limit = broadcast_map.value.get(camera_id)\n",
    "    if limit is not None and ops == \"instant\":\n",
    "        return True if speed > limit else False\n",
    "    elif limit is not None and ops == \"average\":\n",
    "        return True  if speed > limit else False\n",
    "    return False\n",
    "\n",
    "speeding_udf = udf(mark_speeding, BooleanType())\n",
    "\n",
    "# Step 5: Apply UDF to each streaming dataframe\n",
    "def add_speed_flag(df, ops: str):\n",
    "    return df.withColumn(f\"speed_flag_{ops}\", speeding_udf(col(\"camera_id\"), col(\"speed_reading\"), lit(ops)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61183149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, lit\n",
    "\n",
    "# Attach Kafka streams\n",
    "stream_a = spark_job.attach_kafka_stream(\"camera_event_a\", \"172.17.0.1\", \"24 hours\")\n",
    "stream_b = spark_job.attach_kafka_stream(\"camera_event_b\", \"172.17.0.1\", \"24 hours\")\n",
    "stream_c = spark_job.attach_kafka_stream(\"camera_event_c\", \"172.17.0.1\", \"24 hours\")\n",
    "\n",
    "# Flag and drop unnecessary fields\n",
    "stream_a_flagged = add_speed_flag(stream_a.drop(\"event_id\", \"sent_at\", \"batch_id\"), \"instant\")\n",
    "stream_b_flagged = add_speed_flag(stream_b.drop(\"event_id\", \"sent_at\", \"batch_id\"), \"instant\")\n",
    "stream_c_flagged = add_speed_flag(stream_c.drop(\"event_id\", \"sent_at\", \"batch_id\"), \"instant\")\n",
    "\n",
    "# Rename for joining\n",
    "a = stream_a_flagged.selectExpr(\n",
    "    \"car_plate\",\n",
    "    \"camera_id as camera_id_a\",\n",
    "    \"timestamp as timestamp_a\",\n",
    "    \"speed_reading as speed_reading_a\",\n",
    "    \"producer_id as producer_a\",\n",
    "    \"speed_flag_instant as speed_flag_instant_a\"\n",
    ")\n",
    "\n",
    "b = stream_b_flagged.selectExpr(\n",
    "    \"car_plate\",\n",
    "    \"camera_id as camera_id_b\",\n",
    "    \"timestamp as timestamp_b\",\n",
    "    \"speed_reading as speed_reading_b\",\n",
    "    \"producer_id as producer_b\",\n",
    "    \"speed_flag_instant as speed_flag_instant_b\"\n",
    ")\n",
    "\n",
    "c = stream_c_flagged.selectExpr(\n",
    "    \"car_plate\",\n",
    "    \"camera_id as camera_id_c\",\n",
    "    \"timestamp as timestamp_c\",\n",
    "    \"speed_reading as speed_reading_c\",\n",
    "    \"producer_id as producer_c\",\n",
    "    \"speed_flag_instant as speed_flag_instant_c\"\n",
    ")\n",
    "\n",
    "# Join A & B\n",
    "ab_join = b.alias(\"b\").join(\n",
    "    a.alias(\"a\"),\n",
    "    (col(\"a.car_plate\") == col(\"b.car_plate\")) &\n",
    "    (col(\"a.timestamp_a\") < col(\"b.timestamp_b\")) &\n",
    "    (col(\"b.timestamp_b\") <= col(\"a.timestamp_a\") + expr(\"interval 10 minutes\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"a.car_plate\"),\n",
    "    col(\"a.camera_id_a\"),\n",
    "    col(\"a.timestamp_a\"),\n",
    "    col(\"a.speed_reading_a\"),\n",
    "    col(\"a.speed_flag_instant_a\"),\n",
    "    ((col(\"a.speed_reading_a\") + col(\"b.speed_reading_b\")) / 2).alias(\"avg_speed_reading_ab\"),\n",
    "    speeding_udf(\n",
    "        col(\"a.camera_id_a\"),\n",
    "        ((col(\"a.speed_reading_a\") + col(\"b.speed_reading_b\")) / 2),\n",
    "        lit(\"average\")\n",
    "    ).alias(\"speed_flag_average_ab\"),\n",
    "    col(\"b.camera_id_b\"),\n",
    "    col(\"b.timestamp_b\"),\n",
    "    col(\"b.speed_reading_b\"),\n",
    "    col(\"b.speed_flag_instant_b\")\n",
    ")\n",
    "\n",
    "# Join AB & C\n",
    "abc_join = ab_join.alias(\"ab\").join(\n",
    "    c.alias(\"c\"),\n",
    "    (col(\"ab.car_plate\") == col(\"c.car_plate\")) &\n",
    "    (col(\"c.timestamp_c\") > col(\"ab.timestamp_b\")) &\n",
    "    (col(\"c.timestamp_c\") <= col(\"ab.timestamp_b\") + expr(\"interval 10 minutes\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"ab.*\"),\n",
    "    ((col(\"ab.speed_reading_b\") + col(\"c.speed_reading_c\")) / 2).alias(\"avg_speed_reading_bc\"),\n",
    "    speeding_udf(\n",
    "        col(\"ab.camera_id_b\"),\n",
    "        ((col(\"ab.speed_reading_b\") + col(\"c.speed_reading_c\")) / 2),\n",
    "        lit(\"average\")\n",
    "    ).alias(\"speed_flag_average_bc\"),\n",
    "    col(\"c.camera_id_c\"),\n",
    "    col(\"c.timestamp_c\"),\n",
    "    col(\"c.speed_reading_c\"),\n",
    "    col(\"c.speed_flag_instant_c\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ac44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing checkpoint directory: ./stream_checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.streaming import StreamingQueryException\n",
    "\n",
    "# Import your custom DbWriter class here\n",
    "# from your_module import DbWriter\n",
    "\n",
    "# === Configuration ===\n",
    "checkpoint_dir = \"./stream_checkpoints\"\n",
    "mongo_host = \"172.27.65.143\"\n",
    "mongo_port = 27017\n",
    "mongo_db = \"fit3182_db\"\n",
    "mongo_coll = \"new_violations\"\n",
    "\n",
    "# === 1. Clean Up Checkpoint Directory ===\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    print(f\"Deleted existing checkpoint directory: {checkpoint_dir}\")\n",
    "\n",
    "# === 2. Define Batch Processing Function ===\n",
    "def process_batch(batch_df: DataFrame, batch_id: int):\n",
    "    writer = DbWriter(\n",
    "        mongo_host=mongo_host,\n",
    "        mongo_port=mongo_port,\n",
    "        mongo_db=mongo_db,\n",
    "        mongo_coll=mongo_coll\n",
    "    )\n",
    "    \n",
    "    writer.open(partition_id=str(batch_id), epoch_id=str(batch_id))\n",
    "\n",
    "    for row in batch_df.rdd.collect():\n",
    "        writer.process(row)\n",
    "    \n",
    "    writer.close(None)\n",
    "\n",
    "# === 3. Start the Stream ===\n",
    "query = (\n",
    "    abc_join.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .foreachBatch(process_batch)\n",
    "    .option(\"checkpointLocation\", checkpoint_dir)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# === 4. Run the Stream and Handle Termination ===\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by CTRL-C. Stopping query.\")\n",
    "except StreamingQueryException as exc:\n",
    "    print(f\"Streaming error: {exc}\")\n",
    "finally:\n",
    "    query.stop()\n",
    "    print(\"Query stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd25fa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5fbe0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
