{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d8dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import csv\n",
    "client = MongoClient(\"172.27.65.143\", 27017)\n",
    "db = client.fit3182_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ── Your starter connection ──\n",
    "camera_coll = db.Camera\n",
    "\n",
    "# ── Skip if already imported ──\n",
    "if camera_coll.estimated_document_count() > 0:\n",
    "    print(\"Camera collection already contains data. Skipping import.\")\n",
    "else:\n",
    "    idx_name = camera_coll.create_index(\n",
    "        [(\"pos\", pymongo.ASCENDING)],\n",
    "        name=\"pos_idx\"\n",
    "    )\n",
    "    print(f\"Ensured index on 'pos': {idx_name}\")\n",
    "    \n",
    "    # ── Load CSV and insert ──\n",
    "    csv_path = 'data/camera.csv'\n",
    "    docs = []\n",
    "    with open(csv_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            docs.append({\n",
    "                \"_id\":        int(row['camera_id']),\n",
    "                \"lat\":        float(row['latitude']),\n",
    "                \"long\":       float(row['longitude']),\n",
    "                \"pos\":        float(row['position']),\n",
    "                \"speed_limit\": int(row['speed_limit'])\n",
    "            })\n",
    "\n",
    "    if docs:\n",
    "        result = camera_coll.insert_many(docs)\n",
    "        print(f\"Inserted {len(result.inserted_ids)} camera documents.\")\n",
    "        print(\"Current indexes on Camera:\")\n",
    "        for name, info in camera_coll.index_information().items():\n",
    "            print(f\" • {name}: {info['key']}\")\n",
    "    else:\n",
    "        print(\"No camera records found in CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared collection. Deleted 868 documents.\n",
      "Inserted 9844 new vehicle documents.\n",
      "Updated 69 existing vehicle documents.\n",
      "Current indexes on Vehicle:\n",
      " • _id_: [('_id', 1)]\n"
     ]
    }
   ],
   "source": [
    "# ── Connection ──\n",
    "vehicle_coll = db.Vehicle\n",
    "\n",
    "#clear out the collection first \n",
    "deleted = vehicle_coll.delete_many({})\n",
    "print(f\"Cleared collection. Deleted {deleted.deleted_count} documents.\")\n",
    "\n",
    "if vehicle_coll.estimated_document_count() > 0:\n",
    "    print(\"Vehicle collection already contains data. Skipping import.\")\n",
    "else:\n",
    "    # ── Prepare sets & counters ──\n",
    "    existing_ids = set(vehicle_coll.distinct('_id'))\n",
    "    seen_in_file = set()\n",
    "    docs_to_insert = []\n",
    "    update_count = 0\n",
    "    added_count = 0\n",
    "\n",
    "    csv_path = 'data/vehicle.csv'\n",
    "    with open(csv_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            plate = row['car_plate']\n",
    "\n",
    "            # Parse the incoming registration_date\n",
    "            ts = row['registration_date'].rstrip(\"Z\")\n",
    "            reg_date = datetime.fromisoformat(ts)\n",
    "\n",
    "            if plate in seen_in_file:\n",
    "                # Plate already in DB → check whether to update\n",
    "                existing = vehicle_coll.find_one(\n",
    "                    {\"_id\": plate}\n",
    "                )\n",
    "                if existing and reg_date > existing['registration_date']:\n",
    "                    # Only update if the CSV date is newer\n",
    "                    vehicle_coll.update_one(\n",
    "                        {\"_id\": plate},\n",
    "                        {\"$set\": {\n",
    "                            \"registration_date\": reg_date,\n",
    "                            \"owner_name\":        row['owner_name'],\n",
    "                            \"owner_addr\":        row['owner_addr'],\n",
    "                            \"vehicle_type\":      row['vehicle_type']\n",
    "                        }}\n",
    "                    )\n",
    "                    update_count += 1\n",
    "            else:\n",
    "                seen_in_file.add(plate)\n",
    "                # Brand-new plate → schedule for insert\n",
    "                vehicle_coll.insert_one({\n",
    "                    \"_id\":               plate,\n",
    "                    \"owner_name\":        row['owner_name'],\n",
    "                    \"owner_addr\":        row['owner_addr'],\n",
    "                    \"vehicle_type\":      row['vehicle_type'],\n",
    "                    \"registration_date\": reg_date\n",
    "                })\n",
    "                added_count += 1\n",
    "\n",
    "    # ── Do the batch insert, if any ──\n",
    "    if added_count > 0:\n",
    "        print(f\"Inserted {added_count} new vehicle documents.\")\n",
    "    else:\n",
    "        print(\"No new vehicle records to insert.\")\n",
    "\n",
    "    # ── Report on any updates we made ──\n",
    "    if update_count:\n",
    "        print(f\"Updated {update_count} existing vehicle document{'s' if update_count>1 else ''}.\")\n",
    "\n",
    "    # ── (Optional) show your indexes ──\n",
    "    print(\"Current indexes on Vehicle:\")\n",
    "    for name, info in vehicle_coll.index_information().items():\n",
    "        print(f\" • {name}: {info['key']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c20a5",
   "metadata": {},
   "source": [
    "# Create Violation and put historic.csv into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared collection. Deleted 0 documents.\n",
      "Inserted 50000 violation documents.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# ── MongoDB Connection ──\n",
    "client = MongoClient(\"172.27.65.143\", 27017)\n",
    "db = client.fit3182_db\n",
    "violation_coll = db.Violation\n",
    "violation_coll.drop()\n",
    "\n",
    "# Clear out the collection first \n",
    "deleted = violation_coll.delete_many({})\n",
    "print(f\"Cleared collection. Deleted {deleted.deleted_count} documents.\")\n",
    "\n",
    "# ── Index Creation ──\n",
    "violation_coll.create_index([(\"violations.violation_id\", 1)], name=\"idx_violation_id\")  # ❌ removed unique=True\n",
    "violation_coll.create_index([(\"date\", 1)], name=\"idx_date\")\n",
    "violation_coll.create_index([(\"violations.camera_id_start\", 1)], name=\"idx_camera_start\")\n",
    "violation_coll.create_index([(\"violations.camera_id_end\", 1)], name=\"idx_camera_end\")\n",
    "violation_coll.create_index([(\"date\", 1), (\"violations.measured_speed\", -1)], name=\"idx_measured_speed\")\n",
    "violation_coll.create_index([(\"violations.timestamp_start\", 1)], name=\"idx_timestamp_start\")\n",
    "\n",
    "\n",
    "# ── CSV Read ──\n",
    "csv_path = \"data/camera_event_historic.csv\"\n",
    "docs = []\n",
    "\n",
    "with open(csv_path, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        # Parse timestamp_start to datetime\n",
    "        if row.get('timestamp_start'):\n",
    "            timestamp_start = datetime.fromisoformat(row['timestamp_start'].rstrip(\"Z\"))\n",
    "        else:\n",
    "            # skip row if no timestamp_start\n",
    "            continue\n",
    "\n",
    "        timestamp_end = None\n",
    "        if row.get('timestamp_end'):\n",
    "            timestamp_end = datetime.fromisoformat(row['timestamp_end'].rstrip(\"Z\"))\n",
    "\n",
    "        # Create date bucket from timestamp_start (just the date part)\n",
    "        date_bucket = datetime(timestamp_start.year, timestamp_start.month, timestamp_start.day)\n",
    "\n",
    "        # Construct document\n",
    "        violation_doc = {\n",
    "            \"car_plate\": row['car_plate'],\n",
    "            \"date\": date_bucket,\n",
    "            \"violations\": [\n",
    "                {\n",
    "                    \"violation_id\": str(uuid.uuid4()),\n",
    "                    \"type\": \"average\",\n",
    "                    \"camera_id_start\": row['camera_id_start'],\n",
    "                    \"camera_id_end\": row['camera_id_end'] if row.get('camera_id_end') else None,\n",
    "                    \"timestamp_start\": timestamp_start,\n",
    "                    \"timestamp_end\": timestamp_end,\n",
    "                    \"measured_speed\": float(row['speed_reading']) if row.get('speed_reading') else None,\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        docs.append(violation_doc)\n",
    "\n",
    "# ── Insert All Documents ──\n",
    "if docs:\n",
    "    violation_coll.insert_many(docs)\n",
    "    print(f\"Inserted {len(docs)} violation documents.\")\n",
    "else:\n",
    "    print(\"No documents to insert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea4f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Operations import SparkInst\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1 pyspark-shell'\n",
    "spark_job=SparkInst(\"AWAS SYSTEM\", 5, kafka_output_topic=\"violations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4b1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "# add the folder where util.py lives\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql.types import BooleanType\n",
    "import pandas as pd\n",
    "\n",
    "camera_coll=client.fit3182_db.Camera\n",
    "cursor = camera_coll.find()\n",
    "df_pd = pd.DataFrame(list(cursor))\n",
    "#rename mongo index _id as camera_id\n",
    "if '_id' in df_pd.columns:\n",
    "    df_pd.rename(columns={'_id': 'camera_id'}, inplace=True)\n",
    "\n",
    "# Convert the pandas DataFrame into a Spark DataFrame\n",
    "spark_df = spark_job.get_session().createDataFrame(df_pd)\n",
    "\n",
    "\n",
    "speed_limit_map = {row['camera_id']: row['speed_limit'] for row in spark_df.select(\"camera_id\", \"speed_limit\").collect()}\n",
    "broadcast_map = spark_job.essentialData_broadcast(spark_df)\n",
    "\n",
    "def mark_speeding(camera_id:str, speed:float, ops:str)-> str:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    limit = broadcast_map.value.get(camera_id)\n",
    "    if limit is not None and ops == \"instant\":\n",
    "        return True if speed > limit else False\n",
    "    elif limit is not None and ops == \"average\":\n",
    "        return True  if speed > limit else False\n",
    "    return False\n",
    "\n",
    "speeding_udf = udf(mark_speeding, BooleanType())\n",
    "\n",
    "# Step 5: Apply UDF to each streaming dataframe\n",
    "def add_speed_flag(df, ops: str):\n",
    "    return df.withColumn(f\"speed_flag_{ops}\", speeding_udf(col(\"camera_id\"), col(\"speed_reading\"), lit(ops)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61183149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, lit\n",
    "\n",
    "# Attach Kafka streams\n",
    "stream_a = spark_job.attach_kafka_stream(\"camera_event_a\", \"172.17.0.1\", \"24 hours\")\n",
    "stream_b = spark_job.attach_kafka_stream(\"camera_event_b\", \"172.17.0.1\", \"24 hours\")\n",
    "stream_c = spark_job.attach_kafka_stream(\"camera_event_c\", \"172.17.0.1\", \"24 hours\")\n",
    "\n",
    "# Flag and drop unnecessary fields\n",
    "stream_a_flagged = add_speed_flag(stream_a.drop(\"event_id\", \"sent_at\", \"batch_id\"), \"instant\")\n",
    "stream_b_flagged = add_speed_flag(stream_b.drop(\"event_id\", \"sent_at\", \"batch_id\"), \"instant\")\n",
    "stream_c_flagged = add_speed_flag(stream_c.drop(\"event_id\", \"sent_at\", \"batch_id\"), \"instant\")\n",
    "\n",
    "# Rename for joining\n",
    "a = stream_a_flagged.selectExpr(\n",
    "    \"car_plate\",\n",
    "    \"camera_id as camera_id_a\",\n",
    "    \"timestamp as timestamp_a\",\n",
    "    \"speed_reading as speed_reading_a\",\n",
    "    \"producer_id as producer_a\",\n",
    "    \"speed_flag_instant as speed_flag_instant_a\"\n",
    ")\n",
    "\n",
    "b = stream_b_flagged.selectExpr(\n",
    "    \"car_plate\",\n",
    "    \"camera_id as camera_id_b\",\n",
    "    \"timestamp as timestamp_b\",\n",
    "    \"speed_reading as speed_reading_b\",\n",
    "    \"producer_id as producer_b\",\n",
    "    \"speed_flag_instant as speed_flag_instant_b\"\n",
    ")\n",
    "\n",
    "c = stream_c_flagged.selectExpr(\n",
    "    \"car_plate\",\n",
    "    \"camera_id as camera_id_c\",\n",
    "    \"timestamp as timestamp_c\",\n",
    "    \"speed_reading as speed_reading_c\",\n",
    "    \"producer_id as producer_c\",\n",
    "    \"speed_flag_instant as speed_flag_instant_c\"\n",
    ")\n",
    "\n",
    "# Join A & B\n",
    "ab_join = b.alias(\"b\").join(\n",
    "    a.alias(\"a\"),\n",
    "    (col(\"a.car_plate\") == col(\"b.car_plate\")) &\n",
    "    (col(\"a.timestamp_a\") < col(\"b.timestamp_b\")) &\n",
    "    (col(\"b.timestamp_b\") <= col(\"a.timestamp_a\") + expr(\"interval 10 minutes\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"a.car_plate\"),\n",
    "    col(\"a.camera_id_a\"),\n",
    "    col(\"a.timestamp_a\"),\n",
    "    col(\"a.speed_reading_a\"),\n",
    "    col(\"a.speed_flag_instant_a\"),\n",
    "    ((col(\"a.speed_reading_a\") + col(\"b.speed_reading_b\")) / 2).alias(\"avg_speed_reading_ab\"),\n",
    "    speeding_udf(\n",
    "        col(\"a.camera_id_a\"),\n",
    "        ((col(\"a.speed_reading_a\") + col(\"b.speed_reading_b\")) / 2),\n",
    "        lit(\"average\")\n",
    "    ).alias(\"speed_flag_average_ab\"),\n",
    "    col(\"b.camera_id_b\"),\n",
    "    col(\"b.timestamp_b\"),\n",
    "    col(\"b.speed_reading_b\"),\n",
    "    col(\"b.speed_flag_instant_b\")\n",
    ")\n",
    "\n",
    "# Join AB & C\n",
    "abc_join = ab_join.alias(\"ab\").join(\n",
    "    c.alias(\"c\"),\n",
    "    (col(\"ab.car_plate\") == col(\"c.car_plate\")) &\n",
    "    (col(\"c.timestamp_c\") > col(\"ab.timestamp_b\")) &\n",
    "    (col(\"c.timestamp_c\") <= col(\"ab.timestamp_b\") + expr(\"interval 10 minutes\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"ab.*\"),\n",
    "    ((col(\"ab.speed_reading_b\") + col(\"c.speed_reading_c\")) / 2).alias(\"avg_speed_reading_bc\"),\n",
    "    speeding_udf(\n",
    "        col(\"ab.camera_id_b\"),\n",
    "        ((col(\"ab.speed_reading_b\") + col(\"c.speed_reading_c\")) / 2),\n",
    "        lit(\"average\")\n",
    "    ).alias(\"speed_flag_average_bc\"),\n",
    "    col(\"c.camera_id_c\"),\n",
    "    col(\"c.timestamp_c\"),\n",
    "    col(\"c.speed_reading_c\"),\n",
    "    col(\"c.speed_flag_instant_c\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql.streaming import StreamingQueryException\n",
    "from Operations import DbWriter\n",
    "checkpoint_dir = \"./stream_checkpoints\"\n",
    "\n",
    "# 1) Clean up any existing checkpoint directory before starting\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    print(f\"Deleted existing checkpoint directory: {checkpoint_dir}\")\n",
    "\n",
    "# Write to the console\n",
    "query = (\n",
    "    abc_join.writeStream\n",
    "    .format(\"console\")\n",
    "    .option(\"checkpointLocation\", \"./stream_checkpoints\")\n",
    "    .outputMode(\"append\")\n",
    "    .foreach(DbWriter(\n",
    "        mongo_host=\"172.22.32.1\",\n",
    "        mongo_port=27017,\n",
    "        mongo_db=\"fit3182_db\",\n",
    "        mongo_coll=\"Violation\"\n",
    "    ))\n",
    "    .option(\"numRows\", 1000)\n",
    "    .option(\"truncate\", False)  # Optional: show full column contents\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Write to the console\n",
    "# query = (\n",
    "#     abc_join.writeStream\n",
    "#     .format(\"console\")\n",
    "#     .option(\"checkpointLocation\", \"./stream_checkpoints\")\n",
    "#     .outputMode(\"append\")\n",
    "#     .option(\"numRows\", 1000)\n",
    "#     .option(\"truncate\", False)  # Optional: show full column contents\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "# Run query and handle termination gracefully\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by CTRL-C. Stopping query.\")\n",
    "except StreamingQueryException as exc:\n",
    "    print(f\"Streaming error: {exc}\")\n",
    "finally:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ac44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing checkpoint directory: ./stream_checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.streaming import StreamingQueryException\n",
    "from Operations import DbWriter\n",
    "\n",
    "# Import your custom DbWriter class here\n",
    "# from your_module import DbWriter\n",
    "\n",
    "# === Configuration ===\n",
    "checkpoint_dir = \"./stream_checkpoints\"\n",
    "mongo_host = \"172.27.65.143\"\n",
    "mongo_port = 27017\n",
    "mongo_db = \"fit3182_db\"\n",
    "mongo_coll = \"new_violations\"\n",
    "\n",
    "# === 1. Clean Up Checkpoint Directory ===\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    print(f\"Deleted existing checkpoint directory: {checkpoint_dir}\")\n",
    "\n",
    "# === 2. Define Batch Processing Function ===\n",
    "def process_batch(batch_df: DataFrame, batch_id: int):\n",
    "    writer = DbWriter(\n",
    "        mongo_host=mongo_host,\n",
    "        mongo_port=mongo_port,\n",
    "        mongo_db=mongo_db,\n",
    "        mongo_coll=mongo_coll\n",
    "    )\n",
    "    \n",
    "    writer.open(partition_id=str(batch_id), epoch_id=str(batch_id))\n",
    "\n",
    "    for row in batch_df.rdd.collect():\n",
    "        writer.process(row)\n",
    "    \n",
    "    writer.close(None)\n",
    "\n",
    "# === 3. Start the Stream ===\n",
    "query = (\n",
    "    abc_join.writeStream\n",
    "    .format(\"console\")\n",
    "    .option(\"checkpointLocation\", \"./stream_checkpoints\")\n",
    "    .outputMode(\"append\")\n",
    "    .foreach(DbWriter(\n",
    "        mongo_host=\"172.22.32.1\",\n",
    "        mongo_port=27017,\n",
    "        mongo_db=\"fit3182_db\",\n",
    "        mongo_coll=\"Violation\"\n",
    "    ))\n",
    "    .option(\"numRows\", 1000)\n",
    "    .option(\"truncate\", False) \n",
    "    .start()\n",
    ")\n",
    "\n",
    "# === 4. Run the Stream and Handle Termination ===\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by CTRL-C. Stopping query.\")\n",
    "except StreamingQueryException as exc:\n",
    "    print(f\"Streaming error: {exc}\")\n",
    "finally:\n",
    "    query.stop()\n",
    "    print(\"Query stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd25fa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
