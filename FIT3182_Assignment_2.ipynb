{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d28accc5",
      "metadata": {
        "id": "d28accc5"
      },
      "source": [
        "# <span style=\"color:#0b486b\"> FIT3182: Big Data Management and Processing (2025) </span>\n",
        "---\n",
        "\n",
        "Teaching Team:\n",
        "\n",
        "Faculty of Information Technology, Monash University, Australia\n",
        "* A/Prof. David Taniar (Chief Examiner) | david.taniar@monash.edu\n",
        "\n",
        "School of Information Technology, Monash University, Malaysia\n",
        "* Vishnu Monn (Unit Coordinator) | vishnu.monn@monash.edu\n",
        "* Shageenderan Sapai | shageenderan.sapai@monash.edu\n",
        "* Henry Quan Bi Pay | quan.pay@monash.edu\n",
        "* Ruturaj Reddy | ruturaj.reddy@monash.edu\n",
        "* Chai Wai Jin (Class Assistant) | wcha0106@student.monash.edu\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8af82432",
      "metadata": {
        "id": "8af82432"
      },
      "source": [
        "# <span style=\"color:#0b486b\">  Group Information</span>\n",
        "---\n",
        "Note: Group members need to be enrolled in the same tutorial day and time slot.\n",
        "\n",
        "Your tutorial day and time: **2:00pm Friday**     <br/>\n",
        "\n",
        "1st group member\n",
        "\n",
        "Surname: **Chow**  <br/>\n",
        "Firstname: **Louis Meng Hoe**    <br/>\n",
        "Student ID: **32937350**    <br/>\n",
        "Email: **lcho0013@student.monash.edu**    <br/>\n",
        "\n",
        "2nd group member\n",
        "\n",
        "Surname: **[Enter your surname here]**  <br/>\n",
        "Firstname: **[Enter your firstname here ]**    <br/>\n",
        "Student ID: **[Enter your ID here ]**    <br/>\n",
        "Email: **[Enter your email  here ]**    <br/>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10027af0",
      "metadata": {
        "id": "10027af0"
      },
      "source": [
        "# Streaming Application\n",
        "### Due: <span style=\"color:red\">11:55pm MYT, 27th May 2025</span>  (Tuesday)\n",
        "\n",
        "#### <span style=\"color:red\">Important note:</span> This is an **group** assignment with two students (max) per group. You or your group partner can share the code and outcomes of this assignment. However, you should not attempt to post questions on EdForum or any other online platform seeking solutions to the answers. If you require clarification on the assignment questions, you can post a post on EdForum or seek consultation from the tutors. In addition, AI and generative tools may be used in Guided ways.  However, students will be required to demonstrate a comprehensive understanding of the submitted work, failing which significant marks will be deducted from the submitted work. Even though this is a group work, each student is required to submit the assignment work in Moodle."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51cb43d2",
      "metadata": {
        "id": "51cb43d2"
      },
      "source": [
        "## Instructions\n",
        "This notebook has been prepared for you to complete Assignment 2. The theme of this assignment is about practical knowledge and skills in streaming application using Spark and Kafka. **The total marks for this notebook is 30 marks, which is equivalent to 30 percentage points of the total coursework marks for this unit.**\n",
        "\n",
        "* Before getting started, you should read the entire notebook carefully once to understand what you need to do.\n",
        "\n",
        "* Always use the data from the provided `.csv` files to answer the questions unless stated otherwise.\n",
        "\n",
        "This assignment contain **3 parts**:\n",
        "\n",
        "* **Part 1**: MongoDB Data Model (5 Marks)\n",
        "* **Part 2**: Streaming Application (20 Marks)\n",
        "* **Part 3**: Documentation and comments to describe the proposed solution in the submitted notebook (5 Marks)\n",
        "* **Part 4**: Code demo and interview (Negative marking)\n",
        "\n",
        "Required Software:\n",
        "\n",
        "* You will be using Python 3. Answer all questions inside this Jupyter Notebook\n",
        "* Please use the provided Docker to load the Jupyter Notebook\n",
        "\n",
        "**Hint**: This assignment was essentially designed based on the seminars and applied sessions covered from Week 6 to Week 11. You are strongly encouraged to go through these contents thoroughly which might help you to complete the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2d6830c",
      "metadata": {
        "id": "b2d6830c"
      },
      "source": [
        "### Assignment Marking\n",
        "\n",
        "The marking of this assignment is based on quality of work you have submitted rather than just quantity. Marking starts from 0 and goes up based on tasks you have successfully completed and their quality, for example, how well the code submitted follows programming standards, code documentation, presentation of the assignment, readability of the code, organization of the code and so on. Please find the PEP 8 -- Style Guide for Python Code [here](https://www.python.org/dev/peps/pep-0008/) for your reference. Please refer to marking guidelines in Moodle for further details."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9652a7b4",
      "metadata": {
        "id": "9652a7b4"
      },
      "source": [
        "### What to Submit\n",
        "\n",
        "This assignment is to be completed individually and submitted to Moodle unit site. By the due date, you are required to submit the following files to the corresponding Assignment (Dropbox) in Moodle.\n",
        "\n",
        "* **xxx_assignment02_data_design_streaming.ipynb**: this is your main Python notebook solution source file (the data design and streaming application).\n",
        "* **xxx_assignment02_producer_a/b/c.ipynb**: this is your Python notebook solution to run the Kafka producer that reads from one of the camera event files. If you are running multiple producers concurrently in the main notebook, then this file is optional.\n",
        "* **xxx_assignment02_visualisation.ipynb**: this is your Python notebook solution containing the data visualisation.\n",
        "* **xxx_assignment02_code.zip** (if applicable): this is a zip file that contains python files with custom-defined classes and functions to be used in notebook.\n",
        "\n",
        "where `xxx` represents the student ID of each group member. For example, if your student ID is <span style=\"color:red\">12345</span> and your group partner's ID is is <span style=\"color:red\">54321</span>, then your submission file name would be <span style=\"color:red\">12345_54321_assignment02_data_design_streaming.ipynb</span>. Please do the same for all of the submission files.\n",
        "\n",
        "Your assignment will be assessed based on the content of the submitted files in Moodle. We will use the same docker image as provided in this unit when marking your assignment. **If you used additional libraries, please include pip commands in your Jupyter notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f37f836d",
      "metadata": {
        "id": "f37f836d"
      },
      "source": [
        "### Plagiarism and Collusion\n",
        "\n",
        "Plagiarism and collusion are serious academic offenses at Monash University. Students must not share their work with any student. Students should consult policy linked [here](https://www.monash.edu/students/academic/policies/academic-integrity) for more information. See also the video linked on the Moodle page under the Assignment block.\n",
        "\n",
        "The submitted notebook files will be checked for collusion or plagiarism. Students suspected of colluding or plagiarising the assignment will be reported to the Student Conduct and Complaints Department for academic misconduct. Consequently, your grade for this unit will be withheld until the investigation is complete."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99713db4",
      "metadata": {
        "id": "99713db4"
      },
      "source": [
        "### Generative AI usage\n",
        "\n",
        "AI & Generative AI tools may be used in GUIDED ways within this assessment / task as per the guidelines provided.\n",
        "\n",
        "In this task, AI can be used as specified for one or more parts of the assessment task as per the instructions.\n",
        "You may use AI to help you learn how to solve the assignment.\n",
        "\n",
        "Where used, AI must be used responsibly, clearly documented and appropriately acknowledged (see [Learn HQ](https://www.monash.edu/student-academic-success/build-digital-capabilities/create-online/acknowledging-the-use-of-generative-artificial-intelligence)).\n",
        "\n",
        "Any work submitted for a mark must:\n",
        "represent a sincere demonstration of your human efforts, skills and subject knowledge that you will be accountable for.\n",
        "adhere to the guidelines for AI use set for the assessment task.\n",
        "reflect the University’s commitment to academic integrity and ethical behaviour.\n",
        "Inappropriate AI use and/or AI use without acknowledgement will be considered a breach of academic integrity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6496b475",
      "metadata": {
        "id": "6496b475"
      },
      "source": [
        "### Late submissions\n",
        "Extensions and other individual alterations to the assessment regime will only be considered using the University’s [Special Consideration Policy](https://www.monash.edu/students/admin/exams/changes/special-consideration). There is a 10% penalty per day, including weekends, for late submission. Please note that short extensions are not allowed for group submissions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe384a47",
      "metadata": {
        "id": "fe384a47"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Preliminary</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddbe6061",
      "metadata": {
        "id": "ddbe6061"
      },
      "source": [
        "### Scenario Background\n",
        "\n",
        "Malaysia’s road network consistently ranks among the busiest and most accident‑prone in Southeast Asia. Federal roads alone account for a significant proportion of traffic incidents, particularly during peak travel periods and festive seasons, when speed limits of up to 90 km/h (and 110 km/h on expressways) are frequently exceeded in an effort to cover long distances quickly. Since 2012, the Automated Enforcement System (AES) has deployed static speed‑light and red‑light cameras at fixed points to deter speeding and dangerous cornering. However, these point‑capture devices suffer from well‑documented loopholes: drivers can simply decelerate when approaching a camera and then accelerate immediately afterward, rendering enforcement uneven and often ineffective.\n",
        "\n",
        "To address these shortcomings, the Malaysian Government has begun rolling out the Automated Awareness Safety System (AWAS), a point‑to‑point average‑speed enforcement mechanism (Jamil et al., 2022). Figure 1 illustrates an overview of the AWAS system. AWAS leverages pairs of Ekin Spotter modular cameras equipped with 360° video surveillance and Automatic Number Plate Recognition (ANPR) to record each vehicle’s passage at two distinct checkpoints along a highway segment. By logging the exact timestamps at “Point A” and “Point B,” the system computes the travel time over a known distance (typically 1–5 km) and derives the average speed. Any average exceeding the legal limit (e.g., 110 km/h on expressways) automatically triggers a violation notice, regardless of momentary decelerations.\n",
        "\n",
        "While AWAS promises more consistent enforcement, it also introduces significant data‑processing challenges. Each camera pair generates a continuous stream of high‑volume events—potentially thousands per minute during peak hours—that must be matched by license plate, ordered by event time, and joined across streams to compute speeds in near real time. The system must tolerate out‑of‑order or late‑arriving events (e.g., network delays), bound state growth via watermarks, and guarantee end‑to‑end exactly‑once processing to prevent duplicate violation records. These requirements make AWAS an ideal case study for a streaming Big Data architecture using Apache Kafka for ingestion, Apache Spark Structured Streaming for stateful stream–stream joins, and MongoDB for scalable storage of both raw events and flagged violations.\n",
        "\n",
        "Reference:\n",
        "\n",
        "Jamil, H. M., Shabadin, A., & Ibrahim, M. K. A. (2022). Automated Awareness Safety System (AwAS) for Red Light Running in Malaysia: An Analysis of Four-year Data on Its Effectiveness. Journal of the Society of Automotive Engineers Malaysia, 6(1), 19-29."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4fc8e2b",
      "metadata": {
        "id": "d4fc8e2b"
      },
      "source": [
        "<div style=\"text-align: center\">\n",
        "    <img src=\"FIT3182_A2_Fig_1.png\"></img>\n",
        "    <p style=\"text-align: center\">Figure 1 - Overview of AWAS</p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9965391",
      "metadata": {
        "id": "b9965391"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "In this assignment, you are provided the following `.csv` files to help you simulate the AWAS streaming application. The following details the information about the dataset.\n",
        "\n",
        "#### vehicle.csv\n",
        "* car_plate (a string-based unique identifier to each vehicle)\n",
        "* owner_name (a string that contains name of the owner)\n",
        "* owner_addr (a string that contains the address of the owner)\n",
        "* vechicle_type (a string that represents the vechile model)\n",
        "* registration_date (date and time when the vehicle was registered)\n",
        "\n",
        "#### camera.csv\n",
        "* camera_id (an integer-based unique identifier to camera location)\n",
        "* latitude (a float value representing latitude of camera)\n",
        "* longitude (a float value representing longitude of camera)\n",
        "* position (a float value tells at which kilometer point is the camera)\n",
        "* speed_limit (a float value of maximum legal speed for the segment)\n",
        "\n",
        "#### camera_event.csv\n",
        "* event_id (a string-based unique identifier to camera reading)\n",
        "* batch_id (a integer-based identifier to batch reading)\n",
        "* car_plate (a string-based unique identifier to each vehicle)\n",
        "* camera_id (an integer-based unique identifier to camera location)\n",
        "* timestamp (a string that tells the timestamp when the vehicle passed the camera)\n",
        "* speed_reading (a float value that tells the instantaneous speed, recorded in km/h, by that camera)\n",
        "\n",
        "#### camera_event_historic.csv\n",
        "* violation_id (a string-based unique identifier for violation record)\n",
        "* car_plate (a string-based unique identifier to each vehicle)\n",
        "* camera_id_start (an integer-based unique identifier to starting camera location)\n",
        "* camera_id_end (an integer-based unique identifier to ending camera location)\n",
        "* timestamp_start (a string that tells the timestamp when the vehicle passed the starting camera)\n",
        "* timestamp_end (a string that tells the timestamp when the vehicle passed the ending camera)\n",
        "* speed_reading (a float value that tells the average speed, recorded in km/h, within the camera segment)\n",
        "\n",
        "<span style=\"color:red\">Important note:</span> Multiple files of camera_event.csv will be provided, each corresponds to a camera respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "080a9aab",
      "metadata": {
        "id": "080a9aab"
      },
      "source": [
        "### Required Imports\n",
        "\n",
        "Import necessary Python modules in the cell below. Include `pip` statement if external libraries/modules are used."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "727b0dee",
      "metadata": {
        "id": "727b0dee"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Part 1: MongoDB Data Model</span>\n",
        "\n",
        "This section consists of 3 sub-questions\n",
        "\n",
        "In this task, you will study the data model of a streaming application. You will demonstrate the theoretical knowledge by designing appropriate data model based on the provided dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a97125d",
      "metadata": {
        "id": "3a97125d"
      },
      "source": [
        "### Task 1.1 Collection Design\n",
        "\n",
        "In this task, design **at least** the following 3 collections. Add other collections if they are necessary.\n",
        "* Vehicle (Store static metadata about each vehicle)\n",
        "* Camera (Store static definitions of each camera)\n",
        "* Violation (Records of flagged violations)\n",
        "\n",
        "For each collection, provide\n",
        "* 1-2 sentence description of why this collection exists\n",
        "* document schema and a sample document\n",
        "* indexes (if any) by specifying\n",
        "    * Fields (and sort order if applicable)\n",
        "    * Type\n",
        "    * Purpose of the index\n",
        "* shard key strategy (if any) by specifying\n",
        "    * Chosen shard key\n",
        "    * Shard key type\n",
        "    * Rationale\n",
        "* data retention policy (if applicable)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9NsYb_gbB9GP",
      "metadata": {
        "id": "9NsYb_gbB9GP"
      },
      "source": [
        "+# Collections\n",
        "## Camera\n",
        "### Schema\n",
        "```\n",
        "{\n",
        "  \"_id\"; int, // camera_id\n",
        "  \"lat\": float,\n",
        "  \"long\":float,\n",
        "  \"pos\":float,\n",
        "  \"speed_limit\":int\n",
        "}\n",
        "```\n",
        "### Example\n",
        "```\n",
        "{\n",
        "  \"_id\"; 1,\n",
        "  \"lat\": 2.157730731,\n",
        "  \"long\":102.6601002,\n",
        "  \"pos\":152.5,\n",
        "  \"speed_limit\":110\n",
        "}\n",
        "```\n",
        "- Purpose: Holds the static definition of every AWAS checkpoint (including its position on the highway and speed limit), to keep track of the location of the camera whenever a vehicle violated collected in Collection Violation\n",
        "- Indexes:\n",
        "    - _id (single-field unique): To make it easier to search through start and end via fast lookup during distance calculations\n",
        "    - pos (single-field): To support \"next-camera\" queries whenever we want to dynamically find the next camera downstream\n",
        "- Shard Key: None\n",
        "    - Reason: This collection is small and mostly static; would introduce additional complexity if sharded\n",
        "___\n",
        "## Vehicle\n",
        "### Schema\n",
        "```\n",
        "{\n",
        "    \"_id\": str, // car_plate\n",
        "    \"owner_name\":str,\n",
        "    \"owner_addr\":str,\n",
        "    \"vehicle_type\": {\n",
        "        type: String,\n",
        "        enum: ['Sedan\", \"SUV\", \"Coupe\", \"Hatchback\", \"Truck\"],\n",
        "        required: true\n",
        "    },\n",
        "    \"registration_date\": ISODate\n",
        "}\n",
        "```\n",
        "### Example\n",
        "```\n",
        "{\n",
        "    \"_id\": CJ 9,\n",
        "    \"owner_name\": \"John Doe\",\n",
        "    \"owner_addr\": \"123 Maple Street\",\n",
        "    \"vehicle_type\": \"Coupe\",\n",
        "    \"registration_date\": ISODate(\"2023-04-25T10:30:00Z\")\n",
        "}\n",
        "```\n",
        "- Purpose: Holds the static registry of car plates and their details (owner info, vehicle type, etc.), which later can be retrieved to track the owner of a violation\n",
        "- Indexes:\n",
        "    - _id (implicit): Direct plate to document lookup\n",
        "- Shard Key: None\n",
        "    - Reason: This collection is also small and mostly static; lookups by plate are mostly cheap as the primary key\n",
        "___\n",
        "## Violation\n",
        "### Schema\n",
        "```\n",
        "{\n",
        "    \"violation_id\": string,\n",
        "    \"car_plate\": string,\n",
        "    \"date\": ISODate, // date at midnight UTC (or locally)\n",
        "    \"violations\": [\n",
        "        {\n",
        "            \"type\": {\n",
        "                type: string,\n",
        "                enum: [\"instantaneous\",\"average\"],\n",
        "                required: true\n",
        "            },\n",
        "            \"camera_id_start\": int,\n",
        "            \"camera_id_end\": int, // will be null if instantenous\n",
        "            \"timestamp_start\": ISODate,\n",
        "            \"timestamp_end\": ISODate, // will be null if instantenous\n",
        "            \"measured_speed\": float,\n",
        "            \"speed_limit\": int\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "```\n",
        "### Example\n",
        "```\n",
        "{\n",
        "    \"violation_id\": \"afa0f670-41b2-4f73-85e7-c8968ca1752d\",\n",
        "    \"car_plate\": \"ZZY 1\",\n",
        "    \"date\": ISODate(\"2025-05-14T00:00:00Z\"),\n",
        "    \"violations\": [\n",
        "        {\n",
        "            \"type\": \"instantaneous\"\n",
        "            \"camera_id_start\": 2,\n",
        "            \"camera_id_end\": null,\n",
        "            \"timestamp_start\": ISODate(\"2025-05-14T08:10:03Z\"),\n",
        "            \"timestamp_end\": null,\n",
        "            \"measured_speed\": 125.2,\n",
        "            \"speed_limit\": 110\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"average\"\n",
        "            \"camera_id_start\": 2,\n",
        "            \"camera_id_end\": 3,\n",
        "            \"timestamp_start\": ISODate(\"2025-05-14T09:15:10Z\"),\n",
        "            \"timestamp_end\": ISODate(\"2025-05-14T09:16:40Z\"),\n",
        "            \"measured_speed\": 112.5,\n",
        "            \"speed_limit\": 110\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "```\n",
        "- Purpose: Each document summarizes all speed-limit violations for one vehicle on one calendar day. Instantaneous and camera-to-camera (average) violations both live in an embedded array, and a unique constraint on (car_plate, date) guarantees at most one document per vehicle per day.\n",
        "- Indexes:\n",
        "    - (car_plate (ascending), date (descending)) (compound, unique): Prevents two seperate documents from having the same car and date, so the data streaming logic can push extra infractions on that day into the `violations` field instead. Also allows fast lookup of a car's most recent violation document.\n",
        "    - violation_id (ascending) (unique): To allow quick fetching of individual violations themself\n",
        "    - date (ascending) (single-field): Allows speedup of queries what wants to locate violations between a certain period of time\n",
        "    - violations.camera_id_start (ascending): Allows quick find of all violations at a specific starting camera\n",
        "    - violations.camera_id_end (ascending): Allows quick find of all violations at a specific ending camera (applicable only to average violations)\n",
        "    - (date (ascending), violations.measured_speed (descending)): Allows analytics for top speeds during the day\n",
        "- Shard Key: (car_plate: hashed: date: range):\n",
        "    - Reason: Hashing `car_plate` spreads write evenly across the whole cluster (as plates are high-cardinality), while sharding date allows efficiency when targetting a date or date range when needed\n",
        "- Data Retention Policy: Data expires after 5 years, so a TTL index on `date` will be applied to remove any affected docs after 5 years, using `expireAfterSeconds: 5 * 365 * 24 * 60 * 60`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed9db5c3",
      "metadata": {
        "id": "ed9db5c3"
      },
      "source": [
        "### Task 1.2 Collection Relationship\n",
        "\n",
        "In this task, specify the relationships between collections and explain whether you choose to embed data or store references. Justify your choice in terms of:\n",
        "* Read/write patterns\n",
        "* Data duplication versus Join cost\n",
        "* Consistency requirements (if applicable)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R6Xys-rnwBtg",
      "metadata": {
        "id": "R6Xys-rnwBtg"
      },
      "source": [
        "### Violation to Camera\n",
        "- **Cardinality**: Many violations reference One camera\n",
        "- **Read and Write patterns**:\n",
        "    - **Writes**: Only store `camera_id` in Violation whenever an instantaneous or average violation has been performed\n",
        "    - **Reads**: Camera metadata (geolocation, speed_limit, etc.) might occasionally be looked up when displaying or analyzing a violation.\n",
        "- **Embed or Reference**: Data Reference, since there is little application where we need to get camera metadata even though data points is small and static\n",
        "- **Data Duplication vs Join Cost**:\n",
        "    - Camera documents are small and static; if embedded the camera's metadata would duplicate that data in every violation document, which is wastefl and unnecessary.\n",
        "    - A single indexed lookup in Camera is very cheap\n",
        "- **Consistency**:\n",
        "    - If theres ever any need to adjust a camera's position or limit, it's possible to update at one place.\n",
        "\n",
        "### Violation to Vehicle\n",
        "- **Cardinality**: Many violations reference One vehicle\n",
        "- **Read and Write patterns**:\n",
        "    - **Writes**: Records just the `car_plate` (`_id`) in Violation\n",
        "    - **Reads**: For law enforcement or traffic authority use cases, vehicle information needs to be quickly accessible when reviewing a violation. This requires frequent reads of vehicle metadata when querying violations.\n",
        "- **Embed or Reference**: Data Reference, because police officers frequently need to retrieve detailed vehicle information during enforcement or investigation, making joins necessary to ensure flexibility and Violation's retention policy still in play.\n",
        "- **Data Duplication vs Join Cost**:\n",
        "    - Owner name, address, vehicle type, and registration date can be worth hundreds of bytes, so duplicating that on every violation quickly inflates storage.\n",
        "    - A good indexed lookup on `car_plate` (`_id`) is very cheap in MongoDB\n",
        "- **Consistency**:\n",
        "    - Important that Collection Vehicle must be up-to-update and correct in term of vehicle ownership. This is why we use reference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e797ff17",
      "metadata": {
        "id": "e797ff17"
      },
      "source": [
        "### Task 1.3 Discussion\n",
        "\n",
        "In this task, discuss whether your model supports\n",
        "* Consistency and Idempotency\n",
        "    * Does it support idempotent writes?\n",
        "    * Explain any upsert pattern in `violation` collection\n",
        "* Scalability and Fault-Tolerance\n",
        "    * Can your data model support high ingest rates?\n",
        "    * Can your data model support low-latency lookups?\n",
        "\n",
        "Justify and explain the trade-off made in your design."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BnatQrFtv7Ti",
      "metadata": {
        "id": "BnatQrFtv7Ti"
      },
      "source": [
        "### Idempotency\n",
        "Yes, the model supports idempotent writes, as it enforced using the compound unique index on (car_plate: 1, date: 1) plus an upsert operation in MongoDB. That means if Spark retries the same event, it will hit the same daily document rather than creating a new one. For example, writing a violation record with the same data (same car_plate, same date, camera_id_start, etc.) multiple times will have no effect on the existing record but still making sure reference still making further drill down operation possible.\n",
        "\n",
        "### Upsert Pattern\n",
        "Through filtering and select of the eligible window (one day) we can append new row into the sharded collections. As the update and merge operation are handled by the Spark cluster there is no need to update as a part of system design\n",
        "\n",
        "### High Ingest Rate\n",
        "Collection Violation should be able to support this since the allowed operation is append and drop per batch, since filter, merge is already handled by the PySpark Cluster.\n",
        "\n",
        "### Low Latency Lookup\n",
        "Through a sharded key design we added, query based on the car plate is efficient and to join it with the Collection Vehicle and still able to search through different indexes.\n",
        "\n",
        "### Trade off\n",
        "- Scalability and Complexity: to support high ingest rate and Query based on car plate we added complexity through a sharded key\n",
        "\n",
        "- Dependent on Spark Cluster: to enable idempotency & high ingest rate, we are dependent on a consistent and efficient Spark operations to filter and merge properly, making sure there is no duplicate write.\n",
        "\n",
        "- Performance:The system is optimized for write-heavy workloads, which is important for handling a high rate of incoming violations. By using batch processing with Spark and append-only writes, we can process high volume of data. However, the complexity introduced by sharding and Spark's batch processing should be monitored to ensure performance remains optimal."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eafe9a9",
      "metadata": {
        "id": "4eafe9a9"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Part 2: Streaming Application</span>\n",
        "\n",
        "This section consists of 2 sub-questions.\n",
        "\n",
        "In this task, you will implement a streaming application to simulate the AWAS system. Figure 2 illustrates an overview of the streaming architecture that is to be developed to simulate AWAS. Implementation is expected to be following programming standards with high readability (supported with documentation)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b51a153",
      "metadata": {
        "id": "5b51a153"
      },
      "source": [
        "<div style=\"text-align: center\">\n",
        "    <img src=\"FIT3182_A2_Fig_2.png\"></img>\n",
        "    <p style=\"text-align: center\">Figure 2 - Overview of streaming application to simulate AWAS </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf9d7403",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "474fc3a8",
      "metadata": {
        "id": "474fc3a8"
      },
      "source": [
        "### Task 2.1 Data Stream Processing\n",
        "\n",
        "In this task, you will implement multiple **Apache Kafka** producers to simulate the real-time streaming of the data, which will be processed by **Apache Spark Structured Streaming** client and then inserted into MongoDB.\n",
        "\n",
        "*<span style=\"color:red\">Important note:</span> You are expected to use the same data model from Task 1. To make the streaming data consistent for the model, you may need to make some changes to the streaming data before building the model or inserting it to MongoDB.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a989065f",
      "metadata": {
        "id": "a989065f"
      },
      "source": [
        "#### Event Producer\n",
        "\n",
        "**Event Producer A**: Write a python program that loads all the data from `camera_event_A.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_a.ipynb**, where **xxx** represents the student IDs of the group members.\n",
        "\n",
        "**Event Producer B**: Write a python program that loads all the data from `camera_event_B.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_b.ipynb**, where **xxx** represents the student IDs of the group members.\n",
        "\n",
        "**Event Producer C**: Write a python program that loads all the data from `camera_event_C.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_c.ipynb**, where **xxx** represents the student IDs of the group members."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_eaXhH3iD_KW",
      "metadata": {
        "id": "_eaXhH3iD_KW"
      },
      "source": [
        "### assignment02_producer_a.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vby1IVhnEFGW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "vby1IVhnEFGW",
        "outputId": "b1aa7a45-3c02-4499-c6b7-ed706d264e2c"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'kafka3'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9f73614decd7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkafka3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKafkaProducer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kafka3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7n0ei9p2E8hU",
      "metadata": {
        "id": "7n0ei9p2E8hU"
      },
      "source": [
        "### assignment02_producer_b.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0c7a85d",
      "metadata": {
        "id": "d0c7a85d"
      },
      "source": [
        "#### Streaming Application\n",
        "\n",
        "Write a streaming application using Apache Spark Structured Streaming API which processes data in batches. Each batch should contain 0 or more camera event (from event producer). The streaming application should process the data as follows.\n",
        "* Join the streaming data from the producers and determine if a vehicle should be flagged as violation. You should drop any data pair if the timestamp and the order of the camera does not match.\n",
        "* If there is a violation detected, store it into MongoDB straight away.\n",
        "* If there is no violation detected, drop the record.\n",
        "* Due to the dynamic nature of moving vehicle, the time of vehicle completing the camera segment may vary and you should decide how many records and how long the records should be stored in the buffer until a pair is identified.\n",
        "\n",
        "##### Violation Detection Rule\n",
        "A vehicle is flagged as violating the speed limit if any one of the following happens.\n",
        "* Instantaneous speed of vehicle exceed the speed limit of the recording camera\n",
        "* Average speed of vehicle exceed the speed limit of the ending camera\n",
        "\n",
        "<span style=\"color:red\">Important Note:</span> *Only one record for a car per day is recorded in the database. If the car violates at **different cameras**, the record should be merged together into a single record to be stored in the database.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8d11c4a",
      "metadata": {
        "id": "e8d11c4a"
      },
      "source": [
        "### Task 2.2 Data Visualisation\n",
        "\n",
        "In this task, you will implement a program to visualize the joined streaming data. For the incoming camera event(s),\n",
        "* plot the number of violation against arrival time. You need to label some interesting points such as maximum and minimum values.\n",
        "* In addition to that, plot the speed against arrival time. You need to include some interesting points such as average and maximum values.\n",
        "\n",
        "For visualization on the data stored in the database, you have to plot a map using camera location. On the map, annotate\n",
        "* number of violations between the checkpoints\n",
        "* identify hotspot (e.g. when number of violations exceed certain threshold within a time in a day)\n",
        "\n",
        "Explain and justify the plots and the inclusion of the interesting points. Set your own threshold for the hotspot.\n",
        "\n",
        "If you are running this task in a separate Jupyter notebook file, save the file as **xxx_assignment02_visualisation.ipynb**, where **xxx** represents the student IDs of the group members."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4456e6c7",
      "metadata": {},
      "source": [
        "## MongoDB Collections Creation\n",
        "Before starting with Kafka Streaming and Spark Job queries, we need to prepare datsets we have into mongoDB collections to access during the queries, Sink writer, and vizualisation creation. The schema, index choices and reasoning are outlined in the task 1. \n",
        "But first, we instantiate the MongoClient variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d95168",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pymongo \n",
        "from pymongo import MongoClient\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "\n",
        "client = MongoClient(\"172.27.65.143\", 27017)\n",
        "db = client.fit3182_db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41834b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "CAMERA.CSV BULK IMPORT TO COLLECTION CAMERA\n",
        "-----------------------------------------------\n",
        "This cell are created to bulk upload of camera information into a mongoDB collectin\n",
        "key steps:\n",
        "- Checking whether Camera collection has been populated before\n",
        "- parse through the file using library csv and create convert them into:\n",
        "        - `_id`: camera ID (used as a unique identifier)\n",
        "        - `lat`: latitude of the camera\n",
        "        - `long`: longitude of the camera\n",
        "        - `pos`: position in the sequence\n",
        "        - `speed_limit`: speed limit at the camera location\n",
        "        \n",
        "Output:\n",
        "- Log whether we can import or upload has been done succesfully.\n",
        "\"\"\"\n",
        "# ── Your starter connection ──\n",
        "camera_coll = db.Camera\n",
        "\n",
        "# ── Skip if already imported ──\n",
        "if camera_coll.estimated_document_count() > 0:\n",
        "    print(\"Camera collection already contains data. Skipping import.\")\n",
        "else:\n",
        "    idx_name = camera_coll.create_index(\n",
        "        [(\"pos\", pymongo.ASCENDING)],\n",
        "        name=\"pos_idx\"\n",
        "    )\n",
        "    print(f\"Ensured index on 'pos': {idx_name}\")\n",
        "    \n",
        "    # ── Load CSV and insert ──\n",
        "    csv_path = 'data/camera.csv'\n",
        "    docs = []\n",
        "    with open(csv_path, newline='') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            docs.append({\n",
        "                \"_id\":        int(row['camera_id']),\n",
        "                \"lat\":        float(row['latitude']),\n",
        "                \"long\":       float(row['longitude']),\n",
        "                \"pos\":        float(row['position']),\n",
        "                \"speed_limit\": int(row['speed_limit'])\n",
        "            })\n",
        "\n",
        "    if docs:\n",
        "        result = camera_coll.insert_many(docs)\n",
        "        print(f\"Inserted {len(result.inserted_ids)} camera documents.\")\n",
        "        print(\"Current indexes on Camera:\")\n",
        "        for name, info in camera_coll.index_information().items():\n",
        "            print(f\" • {name}: {info['key']}\")\n",
        "    else:\n",
        "        print(\"No camera records found in CSV.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "272485e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "VEHICLE BULK UPLOAD TO MONGODB\n",
        "-------------------------------\n",
        "This cell are implemented to bulk upload into collection Vehicle, \n",
        "because of the nature of the vehicle.csv dataset, there are bound to be \n",
        "duplicate car plate which tells us there are same vehicle that change ownership \n",
        "throughout the year, because of this we implement a updating mechanism \n",
        "\n",
        "key steps:\n",
        "- Deletes all documents in the `Vehicle` collection at the start for a clean slate.\n",
        "- Parse through the Vehicle.csv and convert them into: \n",
        "    - car_plate → used as _id in MongoDB\n",
        "    - owner_name\n",
        "    - owner_addr\n",
        "    - vehicle_type\n",
        "    - registration_date` (to ISO format)\n",
        "- insert and update using insert_one(), update_one()\n",
        "- output to log if we update or insert\n",
        "\"\"\"\n",
        "\n",
        "# ── Connection ──\n",
        "vehicle_coll = db.Vehicle\n",
        "\n",
        "#clear out the collection first \n",
        "deleted = vehicle_coll.delete_many({})\n",
        "print(f\"Cleared collection. Deleted {deleted.deleted_count} documents.\")\n",
        "\n",
        "if vehicle_coll.estimated_document_count() > 0:\n",
        "    print(\"Vehicle collection already contains data. Skipping import.\")\n",
        "else:\n",
        "    # ── Prepare sets & counters ──\n",
        "    existing_ids = set(vehicle_coll.distinct('_id'))\n",
        "    seen_in_file = set()\n",
        "    docs_to_insert = []\n",
        "    update_count = 0\n",
        "    added_count = 0\n",
        "\n",
        "    csv_path = 'data/vehicle.csv'\n",
        "    with open(csv_path, newline='') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            plate = row['car_plate']\n",
        "\n",
        "            # Parse the incoming registration_date\n",
        "            ts = row['registration_date'].rstrip(\"Z\")\n",
        "            reg_date = datetime.fromisoformat(ts)\n",
        "\n",
        "            if plate in seen_in_file:\n",
        "                # Plate already in DB → check whether to update\n",
        "                existing = vehicle_coll.find_one(\n",
        "                    {\"_id\": plate}\n",
        "                )\n",
        "                if existing and reg_date > existing['registration_date']:\n",
        "                    # Only update if the CSV date is newer\n",
        "                    vehicle_coll.update_one(\n",
        "                        {\"_id\": plate},\n",
        "                        {\"$set\": {\n",
        "                            \"registration_date\": reg_date,\n",
        "                            \"owner_name\":        row['owner_name'],\n",
        "                            \"owner_addr\":        row['owner_addr'],\n",
        "                            \"vehicle_type\":      row['vehicle_type']\n",
        "                        }}\n",
        "                    )\n",
        "                    update_count += 1\n",
        "            else:\n",
        "                seen_in_file.add(plate)\n",
        "                # Brand-new plate → schedule for insert\n",
        "                vehicle_coll.insert_one({\n",
        "                    \"_id\":               plate,\n",
        "                    \"owner_name\":        row['owner_name'],\n",
        "                    \"owner_addr\":        row['owner_addr'],\n",
        "                    \"vehicle_type\":      row['vehicle_type'],\n",
        "                    \"registration_date\": reg_date\n",
        "                })\n",
        "                added_count += 1\n",
        "\n",
        "    # ── Do the batch insert, if any ──\n",
        "    if added_count > 0:\n",
        "        print(f\"Inserted {added_count} new vehicle documents.\")\n",
        "    else:\n",
        "        print(\"No new vehicle records to insert.\")\n",
        "\n",
        "    # ── Report on any updates we made ──\n",
        "    if update_count:\n",
        "        print(f\"Updated {update_count} existing vehicle document{'s' if update_count>1 else ''}.\")\n",
        "\n",
        "    # ── (Optional) show your indexes ──\n",
        "    print(\"Current indexes on Vehicle:\")\n",
        "    for name, info in vehicle_coll.index_information().items():\n",
        "        print(f\" • {name}: {info['key']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "255487a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "INSTANTIATE COLLECTION VIOLATION AND BULK UPLOAD HISTORIC VIOLATION\n",
        "-------------------------------------------------------------------\n",
        "This script initializes the `Violation` collection in the `fit3182_db` MongoDB database\n",
        "and bulk imports historical average-speed violation data from the CSV file: `camera_event_historic.csv`.\n",
        "Key steps:\n",
        "- drop the violation_coll to make sure we start with clean slate\n",
        "- create indexes as specified in the task 1.\n",
        "- parse through the camera_event_historic.csv using library csv, convert them into:\n",
        "    - timestamp_start (ISO format)\n",
        "    - timestamp_end (ISO format)\n",
        "    - date buckets from timestamp_start\n",
        "    - parse through violation point into a list inside a row.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ── MongoDB Connection ──\n",
        "violation_coll = db.Violation\n",
        "violation_coll.drop()\n",
        "\n",
        "# Clear out the collection first \n",
        "deleted = violation_coll.delete_many({})\n",
        "print(f\"Cleared collection. Deleted {deleted.deleted_count} documents.\")\n",
        "\n",
        "# ── Index Creation ──\n",
        "violation_coll.create_index([(\"violations.violation_id\", 1)], name=\"idx_violation_id\")\n",
        "violation_coll.create_index([(\"date\", 1)], name=\"idx_date\")\n",
        "violation_coll.create_index([(\"violations.camera_id_start\", 1)], name=\"idx_camera_start\")\n",
        "violation_coll.create_index([(\"violations.camera_id_end\", 1)], name=\"idx_camera_end\")\n",
        "violation_coll.create_index([(\"date\", 1), (\"violations.measured_speed\", -1)], name=\"idx_measured_speed\")\n",
        "violation_coll.create_index([(\"violations.timestamp_start\", 1)], name=\"idx_timestamp_start\")\n",
        "\n",
        "\n",
        "# ── CSV Read ──\n",
        "csv_path = \"data/camera_event_historic.csv\"\n",
        "docs = []\n",
        "\n",
        "with open(csv_path, newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "        # Parse timestamp_start to datetime\n",
        "        if row.get('timestamp_start'):\n",
        "            timestamp_start = datetime.fromisoformat(row['timestamp_start'].rstrip(\"Z\"))\n",
        "        else:\n",
        "            # skip row if no timestamp_start\n",
        "            continue\n",
        "\n",
        "        timestamp_end = None\n",
        "        if row.get('timestamp_end'):\n",
        "            timestamp_end = datetime.fromisoformat(row['timestamp_end'].rstrip(\"Z\"))\n",
        "\n",
        "        # Create date bucket from timestamp_start (just the date part)\n",
        "        date_bucket = datetime(timestamp_start.year, timestamp_start.month, timestamp_start.day)\n",
        "\n",
        "        # Construct document\n",
        "        violation_doc = {\n",
        "            \"car_plate\": row['car_plate'],\n",
        "            \"date\": date_bucket,\n",
        "            \"violations\": [\n",
        "                {\n",
        "                    \"violation_id\": str(uuid.uuid4()),\n",
        "                    \"type\": \"average\",\n",
        "                    \"camera_id_start\": row['camera_id_start'],\n",
        "                    \"camera_id_end\": row['camera_id_end'] if row.get('camera_id_end') else None,\n",
        "                    \"timestamp_start\": timestamp_start,\n",
        "                    \"timestamp_end\": timestamp_end,\n",
        "                    \"measured_speed\": float(row['speed_reading']) if row.get('speed_reading') else None,\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        docs.append(violation_doc)\n",
        "\n",
        "# ── Insert All Documents ──\n",
        "if docs:\n",
        "    violation_coll.insert_many(docs)\n",
        "    print(f\"Inserted {len(docs)} violation documents.\")\n",
        "else:\n",
        "    print(\"No documents to insert.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa3883bc",
      "metadata": {},
      "source": [
        "## Implement Kafka Producers\n",
        "To enable steraming, we need to take use of Kafka streaming, creating 3 different streams for each of:\n",
        "- Camera A\n",
        "- Camera B\n",
        "- Camera C\n",
        "\n",
        "Kafka stream consist of two components, Producers and in this case, 1 Consumer, communicating to stream data from different camera through Zookeeper.\n",
        "A Producer are instantiated by inputting:\n",
        "- kafka server ip address and port\n",
        "- topic assigned to the stream\n",
        "- batch interval being transported\n",
        "\n",
        "Additionally, we attach metadata to every message to help us track and debug the data flow. This metadata includes:\n",
        "\n",
        "- `sent_at`: Timestamp indicating when the message was sent\n",
        "\n",
        "- `producer_id`: Identifier for the specific producer\n",
        "\n",
        "Since we need to run separate producers for each camera (three producers with topics: `camera-a`, `camera-b`, and `camera-c`), we use Python's threading library to execute all producers concurrently from a single script and kernel.\n",
        "\n",
        "See: `producer_all.ipynb` for the implementation details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c5f42e",
      "metadata": {},
      "source": [
        "## Spark Instance Processing of 3 different camera streams\n",
        "In this setup, a single Spark Structured Streaming application processes three separate Kafka streams, each corresponding to a different traffic camera feed (Camera A, B, and C). These streams deliver real-time vehicle event data such as car plate number, timestamp, and speed readings.\n",
        "\n",
        "The Spark instance performs the following key operations:\n",
        "- Stream Joins: For average violation detection, joins are performed between camera streams (e.g., between Camera A and B or B and C) based on car plate numbers and timestamps. This allows for the correlation of entries to compute travel duration and average speed between checkpoints.\n",
        "- Violation Detection: Each stream is independently analyzed to detect two types of violations:\n",
        "    - Instant Violation: When a vehicle exceeds the speed limit at a single camera.\n",
        "    - Average Violation: When a vehicle’s average speed between two cameras exceeds the limit.\n",
        "- Filtering: After initial violation tagging, the application applies filter operations to drop messages that do not meet required conditions — for instance, records that are missing a valid timestamp_start or fall outside of a defined watermark window.\n",
        "\n",
        "Utimately, these are the different components used in this part:\n",
        "- 3 kafka streams (topic: camera_a, camera_b, camera_c)\n",
        "- Instantiated spark session\n",
        "- MongoDB client to read and write\n",
        "- DBwriter to write the results\n",
        "\n",
        "<div style=\"text-align: center\">\n",
        "    <img src=\"docs/stream_processing.png\"></img>\n",
        "    <p style=\"text-align: center\">Figure 4 - Flow Chart of Stream Processing </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "422aff03",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf, col, lit\n",
        "from pyspark.sql.types import BooleanType, DataFrame\n",
        "import pandas as pd\n",
        "from Operations import SparkInst\n",
        "import os\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1 pyspark-shell'\n",
        "spark_job=SparkInst(\"AWAS SYSTEM\", 5, kafka_output_topic=\"violations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34a135d",
      "metadata": {},
      "source": [
        "### Enable Istant Violation Flagging\n",
        "In here, we are required to flag which car plate violate instant violation based on their speed reasing given in each camera, compared with the speed limit listed in each camera we saved inside the MongoDB collection's `Camera`\n",
        "\n",
        "As this task require us to access and read through a small static data from a mongoDB collection, we decided to implement Pyspark's broadcast. Broadcasting allows us to distribute small, read-only datasets to all worker nodes, ensuring that each executor can access this data locally without repeated shuffling or lookups from the driver. This is particularly useful in our use case where we need to use a small amount of data (`camera_id` & `speed_limit`) over and over \n",
        "\n",
        "By doing this, we:\n",
        "- reduce network overhead occured when we need to lookup mongoDB during stream processing\n",
        "- improving scalability in case where we need to scale number of data or number of streams.\n",
        "\n",
        "But `broadcast()` itself doesn't actually perform the comparison and flagging mechanism, we need to create a custom function that run across different nodes that owned the broadcasted data that do this comparison. To do this , we need to transformed into a `User-defined-Function (UDF)` so that the kafka session can actually use them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84b0ca92",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "BROADCAST AND UDF INSTANTIATION\n",
        "------------------------------------\n",
        "This cell are created to enable brodcasting of collection Camera dataframe \n",
        "and UDF function that create specific column to the operation being done (average / instant)\n",
        "this UDF function going to mark which row violate or not.\n",
        "\"\"\"\n",
        "#read through the mongoDB's collection and transformed into Pandas\n",
        "camera_coll=client.fit3182_db.Camera\n",
        "cursor = camera_coll.find()\n",
        "df_pd = pd.DataFrame(list(cursor))\n",
        "\n",
        "#rename mongo index _id as camera_id\n",
        "if '_id' in df_pd.columns:\n",
        "    df_pd.rename(columns={'_id': 'camera_id'}, inplace=True)\n",
        "    \n",
        "# Convert the pandas DataFrame into a Spark DataFrame\n",
        "spark_df = spark_job.get_session().createDataFrame(df_pd)\n",
        "\n",
        "#broacast just as a dictionary into all nodes\n",
        "speed_limit_map = {row['camera_id']: row['speed_limit'] for row in spark_df.select(\"camera_id\", \"speed_limit\").collect()}\n",
        "broadcast_map = spark_job.essentialData_broadcast(spark_df)\n",
        "\n",
        "def mark_speeding(camera_id: str, speed: float, ops: str) -> bool:\n",
        "    \"\"\"\n",
        "    Marks whether a given speed reading is a violation based on the speed limit \n",
        "    for the specified camera for given operations (e.g instant & average)\n",
        "\n",
        "    Input:\n",
        "    - camera_id (str): The ID of the camera that recorded the speed.\n",
        "    - speed (float): The speed measured by the camera.\n",
        "    - ops (str): The type of operation (\"instant\" or \"average\").\n",
        "\n",
        "    Output:\n",
        "    - bool: True if the speed exceeds the limit (i.e., it's a violation), False otherwise.\n",
        "    \"\"\"\n",
        "    limit = broadcast_map.value.get(camera_id)\n",
        "    if limit is not None and ops == \"instant\":\n",
        "        return True if speed > limit else False\n",
        "    elif limit is not None and ops == \"average\":\n",
        "        return True  if speed > limit else False\n",
        "    return False\n",
        "\n",
        "#turn mark_speeding(camera_id, speed, ops)\n",
        "speeding_udf = udf(mark_speeding, BooleanType())\n",
        "\n",
        "# Step 5: Apply UDF to each streaming dataframe\n",
        "def add_speed_flag(df:DataFrame, ops: str):\n",
        "    \"\"\"\n",
        "    Function that trigger the mark_speeding() and transform the streaming processed\n",
        "    into different columns based on the operation\n",
        "    Input:\n",
        "        - df (DataFrame): Spark DataFrame containing 'camera_id' and 'speed' columns.\n",
        "        - ops (str): The type of operation (\"instant\" or \"average\").\n",
        "    Output:\n",
        "        - a spark Dataframe with additional column, speed_flag_instant / speed_flag_average\n",
        "   \n",
        "    \"\"\"\n",
        "    return df.withColumn(f\"speed_flag_{ops}\", speeding_udf(col(\"camera_id\"), col(\"speed_reading\"), lit(ops)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4721763f",
      "metadata": {},
      "source": [
        "## DB Writer and Query Execution\n",
        "Finally, after we instantiate  stream processing we wish to do, what's left is to trigger this as a query and output it into a mongoDB collection using"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dcd481b",
      "metadata": {},
      "source": [
        "## Applying Instant Violation & Prepare for Join\n",
        "This section outlines the ingestion and initial preprocessing steps for three real-time traffic camera streams using Apache Kafka and PySpark Structured Streaming. Each Kafka topic represents a distinct traffic camera, and the data includes vehicle speed readings, timestamps, and other relevant metadata. \n",
        "\n",
        "Because of this we need to: \n",
        "- Attach 3 streams into Spark Session `attach_kafka_stream(topic_id:str, kafka_server:str, watermark:str)`\n",
        "- Add instant violation to using the UDf function we created `add_speed_flag(df:DataFrame, ops:str)` we implement earlier.\n",
        "- Renaming stream camera a, stream camera b, camera c except the car_plate. \n",
        "\n",
        "---\n",
        "### Design Choice Justification\n",
        "- We add 24 hr watermark so that late data can still join in 1 day duration as outlined in the system specification, when we run the  join query.\n",
        "- Maintains traceability across stages during debugging and ensuring there is no data corruption when we run `PySpark's query()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "788d2107",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr, col, lit\n",
        "\n",
        "# Attach Kafka streams into Spark Session we instantiate\n",
        "stream_a = spark_job.attach_kafka_stream(\"camera_event_a\", \"172.17.0.1\", \"24 hours\")\n",
        "stream_b = spark_job.attach_kafka_stream(\"camera_event_b\", \"172.17.0.1\", \"24 hours\")\n",
        "stream_c = spark_job.attach_kafka_stream(\"camera_event_c\", \"172.17.0.1\", \"24 hours\")\n",
        "\n",
        "# Flag and drop unnecessary fields\n",
        "stream_a_flagged = add_speed_flag(stream_a.drop(\"event_id\", \"sent_at\", \"batch_id\"), \"instant\")\n",
        "stream_b_flagged = add_speed_flag(stream_b.drop(\"event_id\", \"sent_at\", \"batch_id\"), \"instant\")\n",
        "stream_c_flagged = add_speed_flag(stream_c.drop(\"event_id\", \"sent_at\", \"batch_id\"), \"instant\")\n",
        "\n",
        "# Rename for joining\n",
        "a = stream_a_flagged.selectExpr(\n",
        "    \"car_plate\",\n",
        "    \"camera_id as camera_id_a\",\n",
        "    \"timestamp as timestamp_a\",\n",
        "    \"speed_reading as speed_reading_a\",\n",
        "    \"producer_id as producer_a\",\n",
        "    \"speed_flag_instant as speed_flag_instant_a\"\n",
        ")\n",
        "\n",
        "b = stream_b_flagged.selectExpr(\n",
        "    \"car_plate\",\n",
        "    \"camera_id as camera_id_b\",\n",
        "    \"timestamp as timestamp_b\",\n",
        "    \"speed_reading as speed_reading_b\",\n",
        "    \"producer_id as producer_b\",\n",
        "    \"speed_flag_instant as speed_flag_instant_b\"\n",
        ")\n",
        "\n",
        "c = stream_c_flagged.selectExpr(\n",
        "    \"car_plate\",\n",
        "    \"camera_id as camera_id_c\",\n",
        "    \"timestamp as timestamp_c\",\n",
        "    \"speed_reading as speed_reading_c\",\n",
        "    \"producer_id as producer_c\",\n",
        "    \"speed_flag_instant as speed_flag_instant_c\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d5516e",
      "metadata": {},
      "source": [
        "## Multi-Camera Stream Join and Filtering\n",
        "\n",
        "This section describes the process of joining traffic event streams from three independent cameras (A, B, and C) to reconstruct a vehicle’s movement path and evaluate both instant and average speed violations across checkpoints. Inside the join operations, we also fulfilled the drop specification outlined in the system requirement.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Join Between Camera A and Camera B\n",
        "\n",
        "An inner join is performed between streams `a` and `b` based on the following conditions:\n",
        "\n",
        "- The `car_plate` value must match to ensure both events correspond to the same vehicle.\n",
        "- The timestamp from Camera A must occur before the timestamp from Camera B (`timestamp_a > timestamp_b`).\n",
        "- The time difference between these two events must be less than or equal to the window, 10 minutes. \n",
        "\n",
        "After the join, the following fields are selected:\n",
        "\n",
        "- Metadata from Camera A and Camera B.\n",
        "- The average speed between the two observations\n",
        "- An average speed violation flag (`speed_flag_average_ab`) determined using a custom user-defined function (`speeding_udf`).\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Join Between AB Join Result and Camera C\n",
        "\n",
        "The intermediate result from the A-B join (`ab_join`) is further joined with Camera C's stream using the following criteria:\n",
        "\n",
        "- The `car_plate` must match to track the same vehicle.\n",
        "- The timestamp from Camera B must occur before the timestamp from Camera C (`timestamp_b > timestamp_c`).\n",
        "- The time difference between these two events must be less than or equal to the window, 10 minutes. \n",
        "\n",
        "This second join yields:\n",
        "- A complete trace of the vehicle across all three cameras.\n",
        "- The average speed between Camera B and C labelled as `average_speed_bc`\n",
        "- An average speed violation flag (`speed_flag_average_bc`) using `\n",
        "\n",
        "### Design Choice Justification\n",
        "\n",
        "- We decide to use a`\"10 minutes\"` tumbling window since we know the system sent data based on batch interval of 5 seconds, so we see this 10 minutes as sufficient enough to catch even a late batch to join, of course, there is a 24-hour watermark that can guarantee.\n",
        "- We include the drop (filtering) action directly in the join to remove irrelevant records early. This reduces data processing and memory use, improves performance by limiting intermediate data, and simplifies the overall pipeline, while logging can still be done inside the Spark Session.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48187e92",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Join A & B\n",
        "ab_join = b.alias(\"b\").join(\n",
        "    a.alias(\"a\"),\n",
        "    (col(\"a.car_plate\") == col(\"b.car_plate\")) &\n",
        "    (col(\"a.timestamp_a\") < col(\"b.timestamp_b\")) &\n",
        "    (col(\"b.timestamp_b\") <= col(\"a.timestamp_a\") + expr(\"interval 10 minutes\")),\n",
        "    \"inner\"\n",
        ").select(\n",
        "    col(\"a.car_plate\"),\n",
        "    col(\"a.camera_id_a\"),\n",
        "    col(\"a.timestamp_a\"),\n",
        "    col(\"a.speed_reading_a\"),\n",
        "    col(\"a.speed_flag_instant_a\"),\n",
        "    ((col(\"a.speed_reading_a\") + col(\"b.speed_reading_b\")) / 2).alias(\"avg_speed_reading_ab\"),\n",
        "    speeding_udf(\n",
        "        col(\"a.camera_id_b\"),\n",
        "        ((col(\"a.speed_reading_a\") + col(\"b.speed_reading_b\")) / 2),\n",
        "        lit(\"average\")\n",
        "    ).alias(\"speed_flag_average_ab\"),\n",
        "    col(\"b.camera_id_b\"),\n",
        "    col(\"b.timestamp_b\"),\n",
        "    col(\"b.speed_reading_b\"),\n",
        "    col(\"b.speed_flag_instant_b\")\n",
        ")\n",
        "\n",
        "# Join AB & C\n",
        "abc_join = ab_join.alias(\"ab\").join(\n",
        "    c.alias(\"c\"),\n",
        "    (col(\"ab.car_plate\") == col(\"c.car_plate\")) &\n",
        "    (col(\"c.timestamp_c\") > col(\"ab.timestamp_b\")) &\n",
        "    (col(\"c.timestamp_c\") <= col(\"ab.timestamp_b\") + expr(\"interval 10 minutes\")),\n",
        "    \"inner\"\n",
        ").select(\n",
        "    col(\"ab.*\"),\n",
        "    ((col(\"ab.speed_reading_b\") + col(\"c.speed_reading_c\")) / 2).alias(\"avg_speed_reading_bc\"),\n",
        "    speeding_udf(\n",
        "        col(\"ab.camera_id_c\"),\n",
        "        ((col(\"ab.speed_reading_b\") + col(\"c.speed_reading_c\")) / 2),\n",
        "        lit(\"average\")\n",
        "    ).alias(\"speed_flag_average_bc\"),\n",
        "    col(\"c.camera_id_c\"),\n",
        "    col(\"c.timestamp_c\"),\n",
        "    col(\"c.speed_reading_c\"),\n",
        "    col(\"c.speed_flag_instant_c\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f748b82f",
      "metadata": {
        "id": "f748b82f"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Part 3: Documentation and comments to describe the proposed solution in the submitted notebook</span>\n",
        "\n",
        "You should include sufficient comments and explanation Tasks 1 and 2 to describe your algorithm and/or code implementation. Please add additional markdown cells to explain your work. Adding extra illustrations to describe your method will also add to the marks in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f29b622",
      "metadata": {
        "id": "1f29b622"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Part 4: Code demo and interview</span>\n",
        "\n",
        "In this task, you will present and showcase the simulation. After the assignment due date, you will be asked to attend an interview/demo session to showcase your application. Your interviewer will ask you a few questions in relation to your application and assess your understanding.\n",
        "\n",
        "During the code demo, your work will be evaluated and assessed based on the marking guideline. Group members will obtain the same marks based on the code demo, unless there is an imbalance in contributions between students in a team. Additionally, each team member will be interviewed to explain the submitted work. The interview represents an individual assessment and a score between 0 and 1 will be awarded, which is then multipled with the marks obtained during the code demo.\n",
        "\n",
        "Interviews for Assignment-2 will be conducted during Week 12 lab sessions. If you are granted an extension from special consideration, the interview will be conducted during SWOT-VAC week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0vMU09gMgOTY",
      "metadata": {
        "id": "0vMU09gMgOTY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
