{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d28accc5",
      "metadata": {
        "id": "d28accc5"
      },
      "source": [
        "# <span style=\"color:#0b486b\"> FIT3182: Big Data Management and Processing (2025) </span>\n",
        "---\n",
        "\n",
        "Teaching Team:\n",
        "\n",
        "Faculty of Information Technology, Monash University, Australia\n",
        "* A/Prof. David Taniar (Chief Examiner) | david.taniar@monash.edu\n",
        "\n",
        "School of Information Technology, Monash University, Malaysia\n",
        "* Vishnu Monn (Unit Coordinator) | vishnu.monn@monash.edu\n",
        "* Shageenderan Sapai | shageenderan.sapai@monash.edu\n",
        "* Henry Quan Bi Pay | quan.pay@monash.edu\n",
        "* Ruturaj Reddy | ruturaj.reddy@monash.edu\n",
        "* Chai Wai Jin (Class Assistant) | wcha0106@student.monash.edu\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8af82432",
      "metadata": {
        "id": "8af82432"
      },
      "source": [
        "# <span style=\"color:#0b486b\">  Group Information</span>\n",
        "---\n",
        "Note: Group members need to be enrolled in the same tutorial day and time slot.\n",
        "\n",
        "Your tutorial day and time: **2:00pm Friday**     <br/>\n",
        "\n",
        "1st group member\n",
        "\n",
        "Surname: **Chow**  <br/>\n",
        "Firstname: **Louis Meng Hoe**    <br/>\n",
        "Student ID: **32937350**    <br/>\n",
        "Email: **lcho0013@student.monash.edu**    <br/>\n",
        "\n",
        "2nd group member\n",
        "\n",
        "Surname: **[Enter your surname here]**  <br/>\n",
        "Firstname: **[Enter your firstname here ]**    <br/>\n",
        "Student ID: **[Enter your ID here ]**    <br/>\n",
        "Email: **[Enter your email  here ]**    <br/>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10027af0",
      "metadata": {
        "id": "10027af0"
      },
      "source": [
        "# Streaming Application\n",
        "### Due: <span style=\"color:red\">11:55pm MYT, 27th May 2025</span>  (Tuesday)\n",
        "\n",
        "#### <span style=\"color:red\">Important note:</span> This is an **group** assignment with two students (max) per group. You or your group partner can share the code and outcomes of this assignment. However, you should not attempt to post questions on EdForum or any other online platform seeking solutions to the answers. If you require clarification on the assignment questions, you can post a post on EdForum or seek consultation from the tutors. In addition, AI and generative tools may be used in Guided ways.  However, students will be required to demonstrate a comprehensive understanding of the submitted work, failing which significant marks will be deducted from the submitted work. Even though this is a group work, each student is required to submit the assignment work in Moodle."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51cb43d2",
      "metadata": {
        "id": "51cb43d2"
      },
      "source": [
        "## Instructions\n",
        "This notebook has been prepared for you to complete Assignment 2. The theme of this assignment is about practical knowledge and skills in streaming application using Spark and Kafka. **The total marks for this notebook is 30 marks, which is equivalent to 30 percentage points of the total coursework marks for this unit.**\n",
        "\n",
        "* Before getting started, you should read the entire notebook carefully once to understand what you need to do.\n",
        "\n",
        "* Always use the data from the provided `.csv` files to answer the questions unless stated otherwise.\n",
        "\n",
        "This assignment contain **3 parts**:\n",
        "\n",
        "* **Part 1**: MongoDB Data Model (5 Marks)\n",
        "* **Part 2**: Streaming Application (20 Marks)\n",
        "* **Part 3**: Documentation and comments to describe the proposed solution in the submitted notebook (5 Marks)\n",
        "* **Part 4**: Code demo and interview (Negative marking)\n",
        "\n",
        "Required Software:\n",
        "\n",
        "* You will be using Python 3. Answer all questions inside this Jupyter Notebook\n",
        "* Please use the provided Docker to load the Jupyter Notebook\n",
        "\n",
        "**Hint**: This assignment was essentially designed based on the seminars and applied sessions covered from Week 6 to Week 11. You are strongly encouraged to go through these contents thoroughly which might help you to complete the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2d6830c",
      "metadata": {
        "id": "b2d6830c"
      },
      "source": [
        "### Assignment Marking\n",
        "\n",
        "The marking of this assignment is based on quality of work you have submitted rather than just quantity. Marking starts from 0 and goes up based on tasks you have successfully completed and their quality, for example, how well the code submitted follows programming standards, code documentation, presentation of the assignment, readability of the code, organization of the code and so on. Please find the PEP 8 -- Style Guide for Python Code [here](https://www.python.org/dev/peps/pep-0008/) for your reference. Please refer to marking guidelines in Moodle for further details."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9652a7b4",
      "metadata": {
        "id": "9652a7b4"
      },
      "source": [
        "### What to Submit\n",
        "\n",
        "This assignment is to be completed individually and submitted to Moodle unit site. By the due date, you are required to submit the following files to the corresponding Assignment (Dropbox) in Moodle.\n",
        "\n",
        "* **xxx_assignment02_data_design_streaming.ipynb**: this is your main Python notebook solution source file (the data design and streaming application).\n",
        "* **xxx_assignment02_producer_a/b/c.ipynb**: this is your Python notebook solution to run the Kafka producer that reads from one of the camera event files. If you are running multiple producers concurrently in the main notebook, then this file is optional.\n",
        "* **xxx_assignment02_visualisation.ipynb**: this is your Python notebook solution containing the data visualisation.\n",
        "* **xxx_assignment02_code.zip** (if applicable): this is a zip file that contains python files with custom-defined classes and functions to be used in notebook.\n",
        "\n",
        "where `xxx` represents the student ID of each group member. For example, if your student ID is <span style=\"color:red\">12345</span> and your group partner's ID is is <span style=\"color:red\">54321</span>, then your submission file name would be <span style=\"color:red\">12345_54321_assignment02_data_design_streaming.ipynb</span>. Please do the same for all of the submission files.\n",
        "\n",
        "Your assignment will be assessed based on the content of the submitted files in Moodle. We will use the same docker image as provided in this unit when marking your assignment. **If you used additional libraries, please include pip commands in your Jupyter notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f37f836d",
      "metadata": {
        "id": "f37f836d"
      },
      "source": [
        "### Plagiarism and Collusion\n",
        "\n",
        "Plagiarism and collusion are serious academic offenses at Monash University. Students must not share their work with any student. Students should consult policy linked [here](https://www.monash.edu/students/academic/policies/academic-integrity) for more information. See also the video linked on the Moodle page under the Assignment block.\n",
        "\n",
        "The submitted notebook files will be checked for collusion or plagiarism. Students suspected of colluding or plagiarising the assignment will be reported to the Student Conduct and Complaints Department for academic misconduct. Consequently, your grade for this unit will be withheld until the investigation is complete."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99713db4",
      "metadata": {
        "id": "99713db4"
      },
      "source": [
        "### Generative AI usage\n",
        "\n",
        "AI & Generative AI tools may be used in GUIDED ways within this assessment / task as per the guidelines provided.\n",
        "\n",
        "In this task, AI can be used as specified for one or more parts of the assessment task as per the instructions.\n",
        "You may use AI to help you learn how to solve the assignment.\n",
        "\n",
        "Where used, AI must be used responsibly, clearly documented and appropriately acknowledged (see [Learn HQ](https://www.monash.edu/student-academic-success/build-digital-capabilities/create-online/acknowledging-the-use-of-generative-artificial-intelligence)).\n",
        "\n",
        "Any work submitted for a mark must:\n",
        "represent a sincere demonstration of your human efforts, skills and subject knowledge that you will be accountable for.\n",
        "adhere to the guidelines for AI use set for the assessment task.\n",
        "reflect the University’s commitment to academic integrity and ethical behaviour.\n",
        "Inappropriate AI use and/or AI use without acknowledgement will be considered a breach of academic integrity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6496b475",
      "metadata": {
        "id": "6496b475"
      },
      "source": [
        "### Late submissions\n",
        "Extensions and other individual alterations to the assessment regime will only be considered using the University’s [Special Consideration Policy](https://www.monash.edu/students/admin/exams/changes/special-consideration). There is a 10% penalty per day, including weekends, for late submission. Please note that short extensions are not allowed for group submissions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe384a47",
      "metadata": {
        "id": "fe384a47"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Preliminary</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddbe6061",
      "metadata": {
        "id": "ddbe6061"
      },
      "source": [
        "### Scenario Background\n",
        "\n",
        "Malaysia’s road network consistently ranks among the busiest and most accident‑prone in Southeast Asia. Federal roads alone account for a significant proportion of traffic incidents, particularly during peak travel periods and festive seasons, when speed limits of up to 90 km/h (and 110 km/h on expressways) are frequently exceeded in an effort to cover long distances quickly. Since 2012, the Automated Enforcement System (AES) has deployed static speed‑light and red‑light cameras at fixed points to deter speeding and dangerous cornering. However, these point‑capture devices suffer from well‑documented loopholes: drivers can simply decelerate when approaching a camera and then accelerate immediately afterward, rendering enforcement uneven and often ineffective.\n",
        "\n",
        "To address these shortcomings, the Malaysian Government has begun rolling out the Automated Awareness Safety System (AWAS), a point‑to‑point average‑speed enforcement mechanism (Jamil et al., 2022). Figure 1 illustrates an overview of the AWAS system. AWAS leverages pairs of Ekin Spotter modular cameras equipped with 360° video surveillance and Automatic Number Plate Recognition (ANPR) to record each vehicle’s passage at two distinct checkpoints along a highway segment. By logging the exact timestamps at “Point A” and “Point B,” the system computes the travel time over a known distance (typically 1–5 km) and derives the average speed. Any average exceeding the legal limit (e.g., 110 km/h on expressways) automatically triggers a violation notice, regardless of momentary decelerations.\n",
        "\n",
        "While AWAS promises more consistent enforcement, it also introduces significant data‑processing challenges. Each camera pair generates a continuous stream of high‑volume events—potentially thousands per minute during peak hours—that must be matched by license plate, ordered by event time, and joined across streams to compute speeds in near real time. The system must tolerate out‑of‑order or late‑arriving events (e.g., network delays), bound state growth via watermarks, and guarantee end‑to‑end exactly‑once processing to prevent duplicate violation records. These requirements make AWAS an ideal case study for a streaming Big Data architecture using Apache Kafka for ingestion, Apache Spark Structured Streaming for stateful stream–stream joins, and MongoDB for scalable storage of both raw events and flagged violations.\n",
        "\n",
        "Reference:\n",
        "\n",
        "Jamil, H. M., Shabadin, A., & Ibrahim, M. K. A. (2022). Automated Awareness Safety System (AwAS) for Red Light Running in Malaysia: An Analysis of Four-year Data on Its Effectiveness. Journal of the Society of Automotive Engineers Malaysia, 6(1), 19-29."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4fc8e2b",
      "metadata": {
        "id": "d4fc8e2b"
      },
      "source": [
        "<div style=\"text-align: center\">\n",
        "    <img src=\"FIT3182_A2_Fig_1.png\"></img>\n",
        "    <p style=\"text-align: center\">Figure 1 - Overview of AWAS</p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9965391",
      "metadata": {
        "id": "b9965391"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "In this assignment, you are provided the following `.csv` files to help you simulate the AWAS streaming application. The following details the information about the dataset.\n",
        "\n",
        "#### vehicle.csv\n",
        "* car_plate (a string-based unique identifier to each vehicle)\n",
        "* owner_name (a string that contains name of the owner)\n",
        "* owner_addr (a string that contains the address of the owner)\n",
        "* vechicle_type (a string that represents the vechile model)\n",
        "* registration_date (date and time when the vehicle was registered)\n",
        "\n",
        "#### camera.csv\n",
        "* camera_id (an integer-based unique identifier to camera location)\n",
        "* latitude (a float value representing latitude of camera)\n",
        "* longitude (a float value representing longitude of camera)\n",
        "* position (a float value tells at which kilometer point is the camera)\n",
        "* speed_limit (a float value of maximum legal speed for the segment)\n",
        "\n",
        "#### camera_event.csv\n",
        "* event_id (a string-based unique identifier to camera reading)\n",
        "* batch_id (a integer-based identifier to batch reading)\n",
        "* car_plate (a string-based unique identifier to each vehicle)\n",
        "* camera_id (an integer-based unique identifier to camera location)\n",
        "* timestamp (a string that tells the timestamp when the vehicle passed the camera)\n",
        "* speed_reading (a float value that tells the instantaneous speed, recorded in km/h, by that camera)\n",
        "\n",
        "#### camera_event_historic.csv\n",
        "* violation_id (a string-based unique identifier for violation record)\n",
        "* car_plate (a string-based unique identifier to each vehicle)\n",
        "* camera_id_start (an integer-based unique identifier to starting camera location)\n",
        "* camera_id_end (an integer-based unique identifier to ending camera location)\n",
        "* timestamp_start (a string that tells the timestamp when the vehicle passed the starting camera)\n",
        "* timestamp_end (a string that tells the timestamp when the vehicle passed the ending camera)\n",
        "* speed_reading (a float value that tells the average speed, recorded in km/h, within the camera segment)\n",
        "\n",
        "<span style=\"color:red\">Important note:</span> Multiple files of camera_event.csv will be provided, each corresponds to a camera respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "080a9aab",
      "metadata": {
        "id": "080a9aab"
      },
      "source": [
        "### Required Imports\n",
        "\n",
        "Import necessary Python modules in the cell below. Include `pip` statement if external libraries/modules are used."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "727b0dee",
      "metadata": {
        "id": "727b0dee"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Part 1: MongoDB Data Model</span>\n",
        "\n",
        "This section consists of 3 sub-questions\n",
        "\n",
        "In this task, you will study the data model of a streaming application. You will demonstrate the theoretical knowledge by designing appropriate data model based on the provided dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a97125d",
      "metadata": {
        "id": "3a97125d"
      },
      "source": [
        "### Task 1.1 Collection Design\n",
        "\n",
        "In this task, design **at least** the following 3 collections. Add other collections if they are necessary.\n",
        "* Vehicle (Store static metadata about each vehicle)\n",
        "* Camera (Store static definitions of each camera)\n",
        "* Violation (Records of flagged violations)\n",
        "\n",
        "For each collection, provide\n",
        "* 1-2 sentence description of why this collection exists\n",
        "* document schema and a sample document\n",
        "* indexes (if any) by specifying\n",
        "    * Fields (and sort order if applicable)\n",
        "    * Type\n",
        "    * Purpose of the index\n",
        "* shard key strategy (if any) by specifying\n",
        "    * Chosen shard key\n",
        "    * Shard key type\n",
        "    * Rationale\n",
        "* data retention policy (if applicable)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9NsYb_gbB9GP",
      "metadata": {
        "id": "9NsYb_gbB9GP"
      },
      "source": [
        "+# Collections\n",
        "## Camera\n",
        "### Schema\n",
        "```\n",
        "{\n",
        "  \"_id\"; int, // camera_id\n",
        "  \"lat\": float,\n",
        "  \"long\":float,\n",
        "  \"pos\":float,\n",
        "  \"speed_limit\":int\n",
        "}\n",
        "```\n",
        "### Example\n",
        "```\n",
        "{\n",
        "  \"_id\"; 1,\n",
        "  \"lat\": 2.157730731,\n",
        "  \"long\":102.6601002,\n",
        "  \"pos\":152.5,\n",
        "  \"speed_limit\":110\n",
        "}\n",
        "```\n",
        "- Purpose: Holds the static definition of every AWAS checkpoint (including its position on the highway and speed limit), to keep track of the location of the camera whenever a vehicle violated collected in Collection Violation\n",
        "- Indexes:\n",
        "    - _id (single-field unique): To make it easier to search through start and end via fast lookup during distance calculations\n",
        "    - pos (single-field): To support \"next-camera\" queries whenever we want to dynamically find the next camera downstream\n",
        "- Shard Key: None\n",
        "    - Reason: This collection is small and mostly static; would introduce additional complexity if sharded\n",
        "___\n",
        "## Vehicle\n",
        "### Schema\n",
        "```\n",
        "{\n",
        "    \"_id\": str, // car_plate\n",
        "    \"owner_name\":str,\n",
        "    \"owner_addr\":str,\n",
        "    \"vehicle_type\": {\n",
        "        type: String,\n",
        "        enum: ['Sedan\", \"SUV\", \"Coupe\", \"Hatchback\", \"Truck\"],\n",
        "        required: true\n",
        "    },\n",
        "    \"registration_date\": ISODate\n",
        "}\n",
        "```\n",
        "### Example\n",
        "```\n",
        "{\n",
        "    \"_id\": CJ 9,\n",
        "    \"owner_name\": \"John Doe\",\n",
        "    \"owner_addr\": \"123 Maple Street\",\n",
        "    \"vehicle_type\": \"Coupe\",\n",
        "    \"registration_date\": ISODate(\"2023-04-25T10:30:00Z\")\n",
        "}\n",
        "```\n",
        "- Purpose: Holds the static registry of car plates and their details (owner info, vehicle type, etc.), which later can be retrieved to track the owner of a violation\n",
        "- Indexes:\n",
        "    - _id (implicit): Direct plate to document lookup\n",
        "- Shard Key: None\n",
        "    - Reason: This collection is also small and mostly static; lookups by plate are mostly cheap as the primary key\n",
        "___\n",
        "## Violation\n",
        "### Schema\n",
        "```\n",
        "{\n",
        "    \"violation_id\": string,\n",
        "    \"car_plate\": string,\n",
        "    \"date\": ISODate, // date at midnight UTC (or locally)\n",
        "    \"violations\": [\n",
        "        {\n",
        "            \"type\": {\n",
        "                type: string,\n",
        "                enum: [\"instantaneous\",\"average\"],\n",
        "                required: true\n",
        "            },\n",
        "            \"camera_id_start\": int,\n",
        "            \"camera_id_end\": int, // will be null if instantenous\n",
        "            \"timestamp_start\": ISODate,\n",
        "            \"timestamp_end\": ISODate, // will be null if instantenous\n",
        "            \"measured_speed\": float,\n",
        "            \"speed_limit\": int\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "```\n",
        "### Example\n",
        "```\n",
        "{\n",
        "    \"violation_id\": \"afa0f670-41b2-4f73-85e7-c8968ca1752d\",\n",
        "    \"car_plate\": \"ZZY 1\",\n",
        "    \"date\": ISODate(\"2025-05-14T00:00:00Z\"),\n",
        "    \"violations\": [\n",
        "        {\n",
        "            \"type\": \"instantaneous\"\n",
        "            \"camera_id_start\": 2,\n",
        "            \"camera_id_end\": null,\n",
        "            \"timestamp_start\": ISODate(\"2025-05-14T08:10:03Z\"),\n",
        "            \"timestamp_end\": null,\n",
        "            \"measured_speed\": 125.2,\n",
        "            \"speed_limit\": 110\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"average\"\n",
        "            \"camera_id_start\": 2,\n",
        "            \"camera_id_end\": 3,\n",
        "            \"timestamp_start\": ISODate(\"2025-05-14T09:15:10Z\"),\n",
        "            \"timestamp_end\": ISODate(\"2025-05-14T09:16:40Z\"),\n",
        "            \"measured_speed\": 112.5,\n",
        "            \"speed_limit\": 110\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "```\n",
        "- Purpose: Each document summarizes all speed-limit violations for one vehicle on one calendar day. Instantaneous and camera-to-camera (average) violations both live in an embedded array, and a unique constraint on (car_plate, date) guarantees at most one document per vehicle per day.\n",
        "- Indexes:\n",
        "    - (car_plate (ascending), date (descending)) (compound, unique): Prevents two seperate documents from having the same car and date, so the data streaming logic can push extra infractions on that day into the `violations` field instead. Also allows fast lookup of a car's most recent violation document.\n",
        "    - violation_id (ascending) (unique): To allow quick fetching of individual violations themself\n",
        "    - date (ascending) (single-field): Allows speedup of queries what wants to locate violations between a certain period of time\n",
        "    - violations.camera_id_start (ascending): Allows quick find of all violations at a specific starting camera\n",
        "    - violations.camera_id_end (ascending): Allows quick find of all violations at a specific ending camera (applicable only to average violations)\n",
        "    - (date (ascending), violations.measured_speed (descending)): Allows analytics for top speeds during the day\n",
        "- Shard Key: (car_plate: hashed: date: range):\n",
        "    - Reason: Hashing `car_plate` spreads write evenly across the whole cluster (as plates are high-cardinality), while sharding date allows efficiency when targetting a date or date range when needed\n",
        "- Data Retention Policy: Data expires after 5 years, so a TTL index on `date` will be applied to remove any affected docs after 5 years, using `expireAfterSeconds: 5 * 365 * 24 * 60 * 60`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed9db5c3",
      "metadata": {
        "id": "ed9db5c3"
      },
      "source": [
        "### Task 1.2 Collection Relationship\n",
        "\n",
        "In this task, specify the relationships between collections and explain whether you choose to embed data or store references. Justify your choice in terms of:\n",
        "* Read/write patterns\n",
        "* Data duplication versus Join cost\n",
        "* Consistency requirements (if applicable)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R6Xys-rnwBtg",
      "metadata": {
        "id": "R6Xys-rnwBtg"
      },
      "source": [
        "### Violation to Camera\n",
        "- **Cardinality**: Many violations reference One camera\n",
        "- **Read and Write patterns**:\n",
        "    - **Writes**: Only store `camera_id` in Violation whenever an instantaneous or average violation has been performed\n",
        "    - **Reads**: Camera metadata (geolocation, speed_limit, etc.) might occasionally be looked up when displaying or analyzing a violation.\n",
        "- **Embed or Reference**: Data Reference, since there is little application where we need to get camera metadata even though data points is small and static\n",
        "- **Data Duplication vs Join Cost**:\n",
        "    - Camera documents are small and static; if embedded the camera's metadata would duplicate that data in every violation document, which is wastefl and unnecessary.\n",
        "    - A single indexed lookup in Camera is very cheap\n",
        "- **Consistency**:\n",
        "    - If theres ever any need to adjust a camera's position or limit, it's possible to update at one place.\n",
        "\n",
        "### Violation to Vehicle\n",
        "- **Cardinality**: Many violations reference One vehicle\n",
        "- **Read and Write patterns**:\n",
        "    - **Writes**: Records just the `car_plate` (`_id`) in Violation\n",
        "    - **Reads**: For law enforcement or traffic authority use cases, vehicle information needs to be quickly accessible when reviewing a violation. This requires frequent reads of vehicle metadata when querying violations.\n",
        "- **Embed or Reference**: Data Reference, because police officers frequently need to retrieve detailed vehicle information during enforcement or investigation, making joins necessary to ensure flexibility and Violation's retention policy still in play.\n",
        "- **Data Duplication vs Join Cost**:\n",
        "    - Owner name, address, vehicle type, and registration date can be worth hundreds of bytes, so duplicating that on every violation quickly inflates storage.\n",
        "    - A good indexed lookup on `car_plate` (`_id`) is very cheap in MongoDB\n",
        "- **Consistency**:\n",
        "    - Important that Collection Vehicle must be up-to-update and correct in term of vehicle ownership. This is why we use reference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e797ff17",
      "metadata": {
        "id": "e797ff17"
      },
      "source": [
        "### Task 1.3 Discussion\n",
        "\n",
        "In this task, discuss whether your model supports\n",
        "* Consistency and Idempotency\n",
        "    * Does it support idempotent writes?\n",
        "    * Explain any upsert pattern in `violation` collection\n",
        "* Scalability and Fault-Tolerance\n",
        "    * Can your data model support high ingest rates?\n",
        "    * Can your data model support low-latency lookups?\n",
        "\n",
        "Justify and explain the trade-off made in your design."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BnatQrFtv7Ti",
      "metadata": {
        "id": "BnatQrFtv7Ti"
      },
      "source": [
        "### Idempotency\n",
        "Yes, the model supports idempotent writes, as it enforced using the compound unique index on (car_plate: 1, date: 1) plus an upsert operation in MongoDB. That means if Spark retries the same event, it will hit the same daily document rather than creating a new one. For example, writing a violation record with the same data (same car_plate, same date, camera_id_start, etc.) multiple times will have no effect on the existing record but still making sure reference still making further drill down operation possible.\n",
        "\n",
        "### Upsert Pattern\n",
        "Through filtering and select of the eligible window (one day) we can append new row into the sharded collections. As the update and merge operation are handled by the Spark cluster there is no need to update as a part of system design\n",
        "\n",
        "### High Ingest Rate\n",
        "Collection Violation should be able to support this since the allowed operation is append and drop per batch, since filter, merge is already handled by the PySpark Cluster.\n",
        "\n",
        "### Low Latency Lookup\n",
        "Through a sharded key design we added, query based on the car plate is efficient and to join it with the Collection Vehicle and still able to search through different indexes.\n",
        "\n",
        "### Trade off\n",
        "- Scalability and Complexity: to support high ingest rate and Query based on car plate we added complexity through a sharded key\n",
        "\n",
        "- Dependent on Spark Cluster: to enable idempotency & high ingest rate, we are dependent on a consistent and efficient Spark operations to filter and merge properly, making sure there is no duplicate write.\n",
        "\n",
        "- Performance:The system is optimized for write-heavy workloads, which is important for handling a high rate of incoming violations. By using batch processing with Spark and append-only writes, we can process high volume of data. However, the complexity introduced by sharding and Spark's batch processing should be monitored to ensure performance remains optimal."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eafe9a9",
      "metadata": {
        "id": "4eafe9a9"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Part 2: Streaming Application</span>\n",
        "\n",
        "This section consists of 2 sub-questions.\n",
        "\n",
        "In this task, you will implement a streaming application to simulate the AWAS system. Figure 2 illustrates an overview of the streaming architecture that is to be developed to simulate AWAS. Implementation is expected to be following programming standards with high readability (supported with documentation)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b51a153",
      "metadata": {
        "id": "5b51a153"
      },
      "source": [
        "<div style=\"text-align: center\">\n",
        "    <img src=\"FIT3182_A2_Fig_2.png\"></img>\n",
        "    <p style=\"text-align: center\">Figure 2 - Overview of streaming application to simulate AWAS </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "474fc3a8",
      "metadata": {
        "id": "474fc3a8"
      },
      "source": [
        "### Task 2.1 Data Stream Processing\n",
        "\n",
        "In this task, you will implement multiple **Apache Kafka** producers to simulate the real-time streaming of the data, which will be processed by **Apache Spark Structured Streaming** client and then inserted into MongoDB.\n",
        "\n",
        "*<span style=\"color:red\">Important note:</span> You are expected to use the same data model from Task 1. To make the streaming data consistent for the model, you may need to make some changes to the streaming data before building the model or inserting it to MongoDB.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a989065f",
      "metadata": {
        "id": "a989065f"
      },
      "source": [
        "#### Event Producer\n",
        "\n",
        "**Event Producer A**: Write a python program that loads all the data from `camera_event_A.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_a.ipynb**, where **xxx** represents the student IDs of the group members.\n",
        "\n",
        "**Event Producer B**: Write a python program that loads all the data from `camera_event_B.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_b.ipynb**, where **xxx** represents the student IDs of the group members.\n",
        "\n",
        "**Event Producer C**: Write a python program that loads all the data from `camera_event_C.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_c.ipynb**, where **xxx** represents the student IDs of the group members."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_eaXhH3iD_KW",
      "metadata": {
        "id": "_eaXhH3iD_KW"
      },
      "source": [
        "### assignment02_producer_a.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vby1IVhnEFGW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "vby1IVhnEFGW",
        "outputId": "b1aa7a45-3c02-4499-c6b7-ed706d264e2c"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'kafka3'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9f73614decd7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkafka3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKafkaProducer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kafka3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Imported libraries\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "from kafka import KafkaProducer\n",
        "from datetime import datetime as dt\n",
        "\n",
        "# CONFIGURATION:\n",
        "# Path to the CSV file\n",
        "CSV_PATH      = 'data/camera_event_A.csv'\n",
        "# Kafka bootstrap server\n",
        "KAFKA_SERVER  = '172.22.32.1'\n",
        "# Kafka topic\n",
        "TOPIC         = 'camera_events_a'\n",
        "# Interval between batches, in seconds\n",
        "BATCH_INTERVAL = 5\n",
        "# Identifier to tag each message with the producer name\n",
        "PRODUCER_ID    = 'A'\n",
        "\n",
        "def connect_kafka_producer(bootstrap_server: str) -> KafkaProducer:\n",
        "    \"\"\"\n",
        "    Create and return a KafkaProducer instance.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        producer = KafkaProducer(\n",
        "            bootstrap_servers=[bootstrap_server],\n",
        "            api_version=(0, 10),\n",
        "            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
        "            key_serializer=lambda k: k.encode('utf-8')\n",
        "        )\n",
        "        return producer\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to connect to Kafka at {bootstrap_server}: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_events(csv_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load the camera events CSV into a DataFrame.\n",
        "    We parse the timestamp column so we can convert it back to ISO.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(\n",
        "        csv_path,\n",
        "        sep=',',\n",
        "        parse_dates=['timestamp'],\n",
        "        dtype={'batch_id': int}\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def produce_batches(df: pd.DataFrame,\n",
        "                    producer: KafkaProducer,\n",
        "                    topic: str,\n",
        "                    interval: float,\n",
        "                    producer_id: str):\n",
        "    \"\"\"\n",
        "    Iterate through each batch_id in order, wrap each record\n",
        "    as a dict + producer tag, and send to Kafka. Then sleep.\n",
        "    \"\"\"\n",
        "    for batch_id in sorted(df['batch_id'].unique()):\n",
        "        batch_df = df[df['batch_id'] == batch_id]\n",
        "        print(f\"[INFO] Publishing batch #{batch_id} ({len(batch_df)} records)...\")\n",
        "        for _, row in batch_df.iterrows():\n",
        "            event = row.to_dict()\n",
        "            # Ensure timestamp is serializable\n",
        "            event['timestamp'] = event['timestamp'].isoformat()\n",
        "            # Tag with producer identity\n",
        "            event['producer'] = producer_id\n",
        "            # Record when the batch is sent exactly\n",
        "            event['sent_at'] = dt.now().isoformat()\n",
        "\n",
        "            try:\n",
        "                producer.send(\n",
        "                    topic,\n",
        "                    key=event['car_plate'],\n",
        "                    value=event,\n",
        "                    timestamp_ms=int(time.time()*1000)\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] Failed to send event {event['event_id']}: {e}\")\n",
        "\n",
        "        # Force all buffered messages out\n",
        "        producer.flush()\n",
        "        print(f\"[INFO] Batch #{batch_id} sent. Sleeping {interval}s...\")\n",
        "        # can comment the below line if you don't need to see the output\n",
        "        print(f\"[DATA] Batch #{batch_id}:\\n{batch_df}\\n\")\n",
        "        time.sleep(interval)\n",
        "\n",
        "    # Loop has finished\n",
        "    producer.flush()\n",
        "    producer.close()\n",
        "\n",
        "def main():\n",
        "    # 1) Connect to Kafka broker\n",
        "    producer = connect_kafka_producer(KAFKA_SERVER)\n",
        "    # 2) Load all events from CSV\n",
        "    df = load_events(CSV_PATH)\n",
        "    # 3) Produce them in time-spaced batches\n",
        "    produce_batches(df, producer, TOPIC, BATCH_INTERVAL, PRODUCER_ID)\n",
        "\n",
        "    print(\"[DONE] All batches published.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7n0ei9p2E8hU",
      "metadata": {
        "id": "7n0ei9p2E8hU"
      },
      "source": [
        "### assignment02_producer_b.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cJQCvCKBE_UI",
      "metadata": {
        "id": "cJQCvCKBE_UI"
      },
      "outputs": [],
      "source": [
        "# Imported libraries\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "from kafka3 import KafkaProducer\n",
        "from datetime import datetime as dt\n",
        "\n",
        "# CONFIGURATION:\n",
        "# Path to the CSV file\n",
        "CSV_PATH      = 'data/camera_event_B.csv'\n",
        "# Kafka bootstrap server\n",
        "KAFKA_SERVER  = '172.22.32.1'\n",
        "# Kafka topic\n",
        "TOPIC         = 'camera_events_b'\n",
        "# Interval between batches, in seconds\n",
        "BATCH_INTERVAL = 5\n",
        "# Identifier to tag each message with the producer name\n",
        "PRODUCER_ID    = 'B'\n",
        "\n",
        "def connect_kafka_producer(bootstrap_server: str) -> KafkaProducer:\n",
        "    \"\"\"\n",
        "    Create and return a KafkaProducer instance.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        producer = KafkaProducer(\n",
        "            bootstrap_servers=[bootstrap_server],\n",
        "            api_version=(0, 10),\n",
        "            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
        "            key_serializer=lambda k: k.encode('utf-8')\n",
        "        )\n",
        "        return producer\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to connect to Kafka at {bootstrap_server}: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_events(csv_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load the camera events CSV into a DataFrame.\n",
        "    We parse the timestamp column so we can convert it back to ISO.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(\n",
        "        csv_path,\n",
        "        sep=',',\n",
        "        parse_dates=['timestamp'],\n",
        "        dtype={'batch_id': int}\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def produce_batches(df: pd.DataFrame,\n",
        "                    producer: KafkaProducer,\n",
        "                    topic: str,\n",
        "                    interval: float,\n",
        "                    producer_id: str):\n",
        "    \"\"\"\n",
        "    Iterate through each batch_id in order, wrap each record\n",
        "    as a dict + producer tag, and send to Kafka. Then sleep.\n",
        "    \"\"\"\n",
        "    for batch_id in sorted(df['batch_id'].unique()):\n",
        "        batch_df = df[df['batch_id'] == batch_id]\n",
        "        print(f\"[INFO] Publishing batch #{batch_id} ({len(batch_df)} records)...\")\n",
        "        for _, row in batch_df.iterrows():\n",
        "            event = row.to_dict()\n",
        "            # Ensure timestamp is serializable\n",
        "            event['timestamp'] = event['timestamp'].isoformat()\n",
        "            # Tag with producer identity\n",
        "            event['producer'] = producer_id\n",
        "            # Record when the batch is sent exactly\n",
        "            event['sent_at'] = dt.now().isoformat()\n",
        "\n",
        "            try:\n",
        "                producer.send(\n",
        "                    topic,\n",
        "                    key=event['car_plate'],\n",
        "                    value=event,\n",
        "                    timestamp_ms=int(time.time()*1000)\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] Failed to send event {event['event_id']}: {e}\")\n",
        "\n",
        "        # Force all buffered messages out\n",
        "        producer.flush()\n",
        "        print(f\"[INFO] Batch #{batch_id} sent. Sleeping {interval}s...\")\n",
        "        # can comment the below line if you don't need to see the output\n",
        "        print(f\"[DATA] Batch #{batch_id}:\\n{batch_df}\\n\")\n",
        "        time.sleep(interval)\n",
        "\n",
        "    # Loop has finished\n",
        "    producer.flush()\n",
        "    producer.close()\n",
        "\n",
        "def main():\n",
        "    # 1) Connect to Kafka broker\n",
        "    producer = connect_kafka_producer(KAFKA_SERVER)\n",
        "    # 2) Load all events from CSV\n",
        "    df = load_events(CSV_PATH)\n",
        "    # 3) Produce them in time-spaced batches\n",
        "    produce_batches(df, producer, TOPIC, BATCH_INTERVAL, PRODUCER_ID)\n",
        "\n",
        "    print(\"[DONE] All batches published.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TFPrybl5FEGS",
      "metadata": {
        "id": "TFPrybl5FEGS"
      },
      "source": [
        "### assignment02_producer_c.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mU5qtOpuFL1w",
      "metadata": {
        "id": "mU5qtOpuFL1w"
      },
      "outputs": [],
      "source": [
        "# Imported libraries\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "from kafka3 import KafkaProducer\n",
        "from datetime import datetime as dt\n",
        "\n",
        "# CONFIGURATION:\n",
        "# Path to the CSV file\n",
        "CSV_PATH      = 'data/camera_event_C.csv'\n",
        "# Kafka bootstrap server\n",
        "KAFKA_SERVER  = '172.22.32.1'\n",
        "# Kafka topic\n",
        "TOPIC         = 'camera_events_c'\n",
        "# Interval between batches, in seconds\n",
        "BATCH_INTERVAL = 5\n",
        "# Identifier to tag each message with the producer name\n",
        "PRODUCER_ID    = 'C'\n",
        "\n",
        "def connect_kafka_producer(bootstrap_server: str) -> KafkaProducer:\n",
        "    \"\"\"\n",
        "    Create and return a KafkaProducer instance.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        producer = KafkaProducer(\n",
        "            bootstrap_servers=[bootstrap_server],\n",
        "            api_version=(0, 10),\n",
        "            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
        "            key_serializer=lambda k: k.encode('utf-8')\n",
        "        )\n",
        "        return producer\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to connect to Kafka at {bootstrap_server}: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_events(csv_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load the camera events CSV into a DataFrame.\n",
        "    We parse the timestamp column so we can convert it back to ISO.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(\n",
        "        csv_path,\n",
        "        sep=',',\n",
        "        parse_dates=['timestamp'],\n",
        "        dtype={'batch_id': int}\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def produce_batches(df: pd.DataFrame,\n",
        "                    producer: KafkaProducer,\n",
        "                    topic: str,\n",
        "                    interval: float,\n",
        "                    producer_id: str):\n",
        "    \"\"\"\n",
        "    Iterate through each batch_id in order, wrap each record\n",
        "    as a dict + producer tag, and send to Kafka. Then sleep.\n",
        "    \"\"\"\n",
        "    for batch_id in sorted(df['batch_id'].unique()):\n",
        "        batch_df = df[df['batch_id'] == batch_id]\n",
        "        print(f\"[INFO] Publishing batch #{batch_id} ({len(batch_df)} records)...\")\n",
        "        for _, row in batch_df.iterrows():\n",
        "            event = row.to_dict()\n",
        "            # Ensure timestamp is serializable\n",
        "            event['timestamp'] = event['timestamp'].isoformat()\n",
        "            # Tag with producer identity\n",
        "            event['producer'] = producer_id\n",
        "            # Record when the batch is sent exactly\n",
        "            event['sent_at'] = dt.now().isoformat()\n",
        "\n",
        "            try:\n",
        "                producer.send(\n",
        "                    topic,\n",
        "                    key=event['car_plate'],\n",
        "                    value=event,\n",
        "                    timestamp_ms=int(time.time()*1000)\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] Failed to send event {event['event_id']}: {e}\")\n",
        "\n",
        "        # Force all buffered messages out\n",
        "        producer.flush()\n",
        "        print(f\"[INFO] Batch #{batch_id} sent. Sleeping {interval}s...\")\n",
        "        # can comment the below line if you don't need to see the output\n",
        "        print(f\"[DATA] Batch #{batch_id}:\\n{batch_df}\\n\")\n",
        "        time.sleep(interval)\n",
        "\n",
        "    # Loop has finished\n",
        "    producer.flush()\n",
        "    producer.close()\n",
        "\n",
        "def main():\n",
        "    # 1) Connect to Kafka broker\n",
        "    producer = connect_kafka_producer(KAFKA_SERVER)\n",
        "    # 2) Load all events from CSV\n",
        "    df = load_events(CSV_PATH)\n",
        "    # 3) Produce them in time-spaced batches\n",
        "    produce_batches(df, producer, TOPIC, BATCH_INTERVAL, PRODUCER_ID)\n",
        "\n",
        "    print(\"[DONE] All batches published.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0c7a85d",
      "metadata": {
        "id": "d0c7a85d"
      },
      "source": [
        "#### Streaming Application\n",
        "\n",
        "Write a streaming application using Apache Spark Structured Streaming API which processes data in batches. Each batch should contain 0 or more camera event (from event producer). The streaming application should process the data as follows.\n",
        "* Join the streaming data from the producers and determine if a vehicle should be flagged as violation. You should drop any data pair if the timestamp and the order of the camera does not match.\n",
        "* If there is a violation detected, store it into MongoDB straight away.\n",
        "* If there is no violation detected, drop the record.\n",
        "* Due to the dynamic nature of moving vehicle, the time of vehicle completing the camera segment may vary and you should decide how many records and how long the records should be stored in the buffer until a pair is identified.\n",
        "\n",
        "##### Violation Detection Rule\n",
        "A vehicle is flagged as violating the speed limit if any one of the following happens.\n",
        "* Instantaneous speed of vehicle exceed the speed limit of the recording camera\n",
        "* Average speed of vehicle exceed the speed limit of the ending camera\n",
        "\n",
        "<span style=\"color:red\">Important Note:</span> *Only one record for a car per day is recorded in the database. If the car violates at **different cameras**, the record should be merged together into a single record to be stored in the database.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8d11c4a",
      "metadata": {
        "id": "e8d11c4a"
      },
      "source": [
        "### Task 2.2 Data Visualisation\n",
        "\n",
        "In this task, you will implement a program to visualize the joined streaming data. For the incoming camera event(s),\n",
        "* plot the number of violation against arrival time. You need to label some interesting points such as maximum and minimum values.\n",
        "* In addition to that, plot the speed against arrival time. You need to include some interesting points such as average and maximum values.\n",
        "\n",
        "For visualization on the data stored in the database, you have to plot a map using camera location. On the map, annotate\n",
        "* number of violations between the checkpoints\n",
        "* identify hotspot (e.g. when number of violations exceed certain threshold within a time in a day)\n",
        "\n",
        "Explain and justify the plots and the inclusion of the interesting points. Set your own threshold for the hotspot.\n",
        "\n",
        "If you are running this task in a separate Jupyter notebook file, save the file as **xxx_assignment02_visualisation.ipynb**, where **xxx** represents the student IDs of the group members."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f748b82f",
      "metadata": {
        "id": "f748b82f"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Part 3: Documentation and comments to describe the proposed solution in the submitted notebook</span>\n",
        "\n",
        "You should include sufficient comments and explanation Tasks 1 and 2 to describe your algorithm and/or code implementation. Please add additional markdown cells to explain your work. Adding extra illustrations to describe your method will also add to the marks in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f29b622",
      "metadata": {
        "id": "1f29b622"
      },
      "source": [
        "## <span style=\"color:#0b486b\">Part 4: Code demo and interview</span>\n",
        "\n",
        "In this task, you will present and showcase the simulation. After the assignment due date, you will be asked to attend an interview/demo session to showcase your application. Your interviewer will ask you a few questions in relation to your application and assess your understanding.\n",
        "\n",
        "During the code demo, your work will be evaluated and assessed based on the marking guideline. Group members will obtain the same marks based on the code demo, unless there is an imbalance in contributions between students in a team. Additionally, each team member will be interviewed to explain the submitted work. The interview represents an individual assessment and a score between 0 and 1 will be awarded, which is then multipled with the marks obtained during the code demo.\n",
        "\n",
        "Interviews for Assignment-2 will be conducted during Week 12 lab sessions. If you are granted an extension from special consideration, the interview will be conducted during SWOT-VAC week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0vMU09gMgOTY",
      "metadata": {
        "id": "0vMU09gMgOTY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
