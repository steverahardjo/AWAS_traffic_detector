{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28accc5",
   "metadata": {
    "id": "d28accc5"
   },
   "source": [
    "# <span style=\"color:#0b486b\"> FIT3182: Big Data Management and Processing (2025) </span>\n",
    "---\n",
    "\n",
    "Teaching Team:\n",
    "\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "* A/Prof. David Taniar (Chief Examiner) | david.taniar@monash.edu\n",
    "\n",
    "School of Information Technology, Monash University, Malaysia\n",
    "* Vishnu Monn (Unit Coordinator) | vishnu.monn@monash.edu\n",
    "* Shageenderan Sapai | shageenderan.sapai@monash.edu\n",
    "* Henry Quan Bi Pay | quan.pay@monash.edu\n",
    "* Ruturaj Reddy | ruturaj.reddy@monash.edu\n",
    "* Chai Wai Jin (Class Assistant) | wcha0106@student.monash.edu\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af82432",
   "metadata": {
    "id": "8af82432"
   },
   "source": [
    "# <span style=\"color:#0b486b\">  Group Information</span>\n",
    "---\n",
    "Note: Group members need to be enrolled in the same tutorial day and time slot.\n",
    "\n",
    "Your tutorial day and time: **2:00pm Friday**     <br/>\n",
    "\n",
    "1st group member\n",
    "\n",
    "Surname: **Chow**  <br/>\n",
    "Firstname: **Louis Meng Hoe**    <br/>\n",
    "Student ID: **32937350**    <br/>\n",
    "Email: **lcho0013@student.monash.edu**    <br/>\n",
    "\n",
    "2nd group member\n",
    "\n",
    "Surname: **[Enter your surname here]**  <br/>\n",
    "Firstname: **[Enter your firstname here ]**    <br/>\n",
    "Student ID: **[Enter your ID here ]**    <br/>\n",
    "Email: **[Enter your email  here ]**    <br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10027af0",
   "metadata": {
    "id": "10027af0"
   },
   "source": [
    "# Streaming Application\n",
    "### Due: <span style=\"color:red\">11:55pm MYT, 27th May 2025</span>  (Tuesday)\n",
    "\n",
    "#### <span style=\"color:red\">Important note:</span> This is an **group** assignment with two students (max) per group. You or your group partner can share the code and outcomes of this assignment. However, you should not attempt to post questions on EdForum or any other online platform seeking solutions to the answers. If you require clarification on the assignment questions, you can post a post on EdForum or seek consultation from the tutors. In addition, AI and generative tools may be used in Guided ways.  However, students will be required to demonstrate a comprehensive understanding of the submitted work, failing which significant marks will be deducted from the submitted work. Even though this is a group work, each student is required to submit the assignment work in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb43d2",
   "metadata": {
    "id": "51cb43d2"
   },
   "source": [
    "## Instructions\n",
    "This notebook has been prepared for you to complete Assignment 2. The theme of this assignment is about practical knowledge and skills in streaming application using Spark and Kafka. **The total marks for this notebook is 30 marks, which is equivalent to 30 percentage points of the total coursework marks for this unit.**\n",
    "\n",
    "* Before getting started, you should read the entire notebook carefully once to understand what you need to do.\n",
    "\n",
    "* Always use the data from the provided `.csv` files to answer the questions unless stated otherwise.\n",
    "\n",
    "This assignment contain **3 parts**:\n",
    "\n",
    "* **Part 1**: MongoDB Data Model (5 Marks)\n",
    "* **Part 2**: Streaming Application (20 Marks)\n",
    "* **Part 3**: Documentation and comments to describe the proposed solution in the submitted notebook (5 Marks)\n",
    "* **Part 4**: Code demo and interview (Negative marking)\n",
    "\n",
    "Required Software:\n",
    "\n",
    "* You will be using Python 3. Answer all questions inside this Jupyter Notebook\n",
    "* Please use the provided Docker to load the Jupyter Notebook\n",
    "\n",
    "**Hint**: This assignment was essentially designed based on the seminars and applied sessions covered from Week 6 to Week 11. You are strongly encouraged to go through these contents thoroughly which might help you to complete the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d6830c",
   "metadata": {
    "id": "b2d6830c"
   },
   "source": [
    "### Assignment Marking\n",
    "\n",
    "The marking of this assignment is based on quality of work you have submitted rather than just quantity. Marking starts from 0 and goes up based on tasks you have successfully completed and their quality, for example, how well the code submitted follows programming standards, code documentation, presentation of the assignment, readability of the code, organization of the code and so on. Please find the PEP 8 -- Style Guide for Python Code [here](https://www.python.org/dev/peps/pep-0008/) for your reference. Please refer to marking guidelines in Moodle for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652a7b4",
   "metadata": {
    "id": "9652a7b4"
   },
   "source": [
    "### What to Submit\n",
    "\n",
    "This assignment is to be completed individually and submitted to Moodle unit site. By the due date, you are required to submit the following files to the corresponding Assignment (Dropbox) in Moodle.\n",
    "\n",
    "* **xxx_assignment02_data_design_streaming.ipynb**: this is your main Python notebook solution source file (the data design and streaming application).\n",
    "* **xxx_assignment02_producer_a/b/c.ipynb**: this is your Python notebook solution to run the Kafka producer that reads from one of the camera event files. If you are running multiple producers concurrently in the main notebook, then this file is optional.\n",
    "* **xxx_assignment02_visualisation.ipynb**: this is your Python notebook solution containing the data visualisation.\n",
    "* **xxx_assignment02_code.zip** (if applicable): this is a zip file that contains python files with custom-defined classes and functions to be used in notebook.\n",
    "\n",
    "where `xxx` represents the student ID of each group member. For example, if your student ID is <span style=\"color:red\">12345</span> and your group partner's ID is is <span style=\"color:red\">54321</span>, then your submission file name would be <span style=\"color:red\">12345_54321_assignment02_data_design_streaming.ipynb</span>. Please do the same for all of the submission files.\n",
    "\n",
    "Your assignment will be assessed based on the content of the submitted files in Moodle. We will use the same docker image as provided in this unit when marking your assignment. **If you used additional libraries, please include pip commands in your Jupyter notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f836d",
   "metadata": {
    "id": "f37f836d"
   },
   "source": [
    "### Plagiarism and Collusion\n",
    "\n",
    "Plagiarism and collusion are serious academic offenses at Monash University. Students must not share their work with any student. Students should consult policy linked [here](https://www.monash.edu/students/academic/policies/academic-integrity) for more information. See also the video linked on the Moodle page under the Assignment block.\n",
    "\n",
    "The submitted notebook files will be checked for collusion or plagiarism. Students suspected of colluding or plagiarising the assignment will be reported to the Student Conduct and Complaints Department for academic misconduct. Consequently, your grade for this unit will be withheld until the investigation is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99713db4",
   "metadata": {
    "id": "99713db4"
   },
   "source": [
    "### Generative AI usage\n",
    "\n",
    "AI & Generative AI tools may be used in GUIDED ways within this assessment / task as per the guidelines provided.\n",
    "\n",
    "In this task, AI can be used as specified for one or more parts of the assessment task as per the instructions.\n",
    "You may use AI to help you learn how to solve the assignment.\n",
    "\n",
    "Where used, AI must be used responsibly, clearly documented and appropriately acknowledged (see [Learn HQ](https://www.monash.edu/student-academic-success/build-digital-capabilities/create-online/acknowledging-the-use-of-generative-artificial-intelligence)).\n",
    "\n",
    "Any work submitted for a mark must:\n",
    "represent a sincere demonstration of your human efforts, skills and subject knowledge that you will be accountable for.\n",
    "adhere to the guidelines for AI use set for the assessment task.\n",
    "reflect the University’s commitment to academic integrity and ethical behaviour.\n",
    "Inappropriate AI use and/or AI use without acknowledgement will be considered a breach of academic integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496b475",
   "metadata": {
    "id": "6496b475"
   },
   "source": [
    "### Late submissions\n",
    "Extensions and other individual alterations to the assessment regime will only be considered using the University’s [Special Consideration Policy](https://www.monash.edu/students/admin/exams/changes/special-consideration). There is a 10% penalty per day, including weekends, for late submission. Please note that short extensions are not allowed for group submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe384a47",
   "metadata": {
    "id": "fe384a47"
   },
   "source": [
    "## <span style=\"color:#0b486b\">Preliminary</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbe6061",
   "metadata": {
    "id": "ddbe6061"
   },
   "source": [
    "### Scenario Background\n",
    "\n",
    "Malaysia’s road network consistently ranks among the busiest and most accident‑prone in Southeast Asia. Federal roads alone account for a significant proportion of traffic incidents, particularly during peak travel periods and festive seasons, when speed limits of up to 90 km/h (and 110 km/h on expressways) are frequently exceeded in an effort to cover long distances quickly. Since 2012, the Automated Enforcement System (AES) has deployed static speed‑light and red‑light cameras at fixed points to deter speeding and dangerous cornering. However, these point‑capture devices suffer from well‑documented loopholes: drivers can simply decelerate when approaching a camera and then accelerate immediately afterward, rendering enforcement uneven and often ineffective.\n",
    "\n",
    "To address these shortcomings, the Malaysian Government has begun rolling out the Automated Awareness Safety System (AWAS), a point‑to‑point average‑speed enforcement mechanism (Jamil et al., 2022). Figure 1 illustrates an overview of the AWAS system. AWAS leverages pairs of Ekin Spotter modular cameras equipped with 360° video surveillance and Automatic Number Plate Recognition (ANPR) to record each vehicle’s passage at two distinct checkpoints along a highway segment. By logging the exact timestamps at “Point A” and “Point B,” the system computes the travel time over a known distance (typically 1–5 km) and derives the average speed. Any average exceeding the legal limit (e.g., 110 km/h on expressways) automatically triggers a violation notice, regardless of momentary decelerations.\n",
    "\n",
    "While AWAS promises more consistent enforcement, it also introduces significant data‑processing challenges. Each camera pair generates a continuous stream of high‑volume events—potentially thousands per minute during peak hours—that must be matched by license plate, ordered by event time, and joined across streams to compute speeds in near real time. The system must tolerate out‑of‑order or late‑arriving events (e.g., network delays), bound state growth via watermarks, and guarantee end‑to‑end exactly‑once processing to prevent duplicate violation records. These requirements make AWAS an ideal case study for a streaming Big Data architecture using Apache Kafka for ingestion, Apache Spark Structured Streaming for stateful stream–stream joins, and MongoDB for scalable storage of both raw events and flagged violations.\n",
    "\n",
    "Reference:\n",
    "\n",
    "Jamil, H. M., Shabadin, A., & Ibrahim, M. K. A. (2022). Automated Awareness Safety System (AwAS) for Red Light Running in Malaysia: An Analysis of Four-year Data on Its Effectiveness. Journal of the Society of Automotive Engineers Malaysia, 6(1), 19-29."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc8e2b",
   "metadata": {
    "id": "d4fc8e2b"
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"FIT3182_A2_Fig_1.png\"></img>\n",
    "    <p style=\"text-align: center\">Figure 1 - Overview of AWAS</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9965391",
   "metadata": {
    "id": "b9965391"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "In this assignment, you are provided the following `.csv` files to help you simulate the AWAS streaming application. The following details the information about the dataset.\n",
    "\n",
    "#### vehicle.csv\n",
    "* car_plate (a string-based unique identifier to each vehicle)\n",
    "* owner_name (a string that contains name of the owner)\n",
    "* owner_addr (a string that contains the address of the owner)\n",
    "* vechicle_type (a string that represents the vechile model)\n",
    "* registration_date (date and time when the vehicle was registered)\n",
    "\n",
    "#### camera.csv\n",
    "* camera_id (an integer-based unique identifier to camera location)\n",
    "* latitude (a float value representing latitude of camera)\n",
    "* longitude (a float value representing longitude of camera)\n",
    "* position (a float value tells at which kilometer point is the camera)\n",
    "* speed_limit (a float value of maximum legal speed for the segment)\n",
    "\n",
    "#### camera_event.csv\n",
    "* event_id (a string-based unique identifier to camera reading)\n",
    "* batch_id (a integer-based identifier to batch reading)\n",
    "* car_plate (a string-based unique identifier to each vehicle)\n",
    "* camera_id (an integer-based unique identifier to camera location)\n",
    "* timestamp (a string that tells the timestamp when the vehicle passed the camera)\n",
    "* speed_reading (a float value that tells the instantaneous speed, recorded in km/h, by that camera)\n",
    "\n",
    "#### camera_event_historic.csv\n",
    "* violation_id (a string-based unique identifier for violation record)\n",
    "* car_plate (a string-based unique identifier to each vehicle)\n",
    "* camera_id_start (an integer-based unique identifier to starting camera location)\n",
    "* camera_id_end (an integer-based unique identifier to ending camera location)\n",
    "* timestamp_start (a string that tells the timestamp when the vehicle passed the starting camera)\n",
    "* timestamp_end (a string that tells the timestamp when the vehicle passed the ending camera)\n",
    "* speed_reading (a float value that tells the average speed, recorded in km/h, within the camera segment)\n",
    "\n",
    "<span style=\"color:red\">Important note:</span> Multiple files of camera_event.csv will be provided, each corresponds to a camera respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080a9aab",
   "metadata": {
    "id": "080a9aab"
   },
   "source": [
    "### Required Imports\n",
    "\n",
    "Import necessary Python modules in the cell below. Include `pip` statement if external libraries/modules are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b0dee",
   "metadata": {
    "id": "727b0dee"
   },
   "source": [
    "## <span style=\"color:#0b486b\">Part 1: MongoDB Data Model</span>\n",
    "\n",
    "This section consists of 3 sub-questions\n",
    "\n",
    "In this task, you will study the data model of a streaming application. You will demonstrate the theoretical knowledge by designing appropriate data model based on the provided dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97125d",
   "metadata": {
    "id": "3a97125d"
   },
   "source": [
    "### Task 1.1 Collection Design\n",
    "\n",
    "In this task, design **at least** the following 3 collections. Add other collections if they are necessary.\n",
    "* Vehicle (Store static metadata about each vehicle)\n",
    "* Camera (Store static definitions of each camera)\n",
    "* Violation (Records of flagged violations)\n",
    "\n",
    "For each collection, provide\n",
    "* 1-2 sentence description of why this collection exists\n",
    "* document schema and a sample document\n",
    "* indexes (if any) by specifying\n",
    "    * Fields (and sort order if applicable)\n",
    "    * Type\n",
    "    * Purpose of the index\n",
    "* shard key strategy (if any) by specifying\n",
    "    * Chosen shard key\n",
    "    * Shard key type\n",
    "    * Rationale\n",
    "* data retention policy (if applicable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9NsYb_gbB9GP",
   "metadata": {
    "id": "9NsYb_gbB9GP"
   },
   "source": [
    "+# Collections\n",
    "## Camera\n",
    "### Schema\n",
    "```\n",
    "{\n",
    "  \"_id\"; int, // camera_id\n",
    "  \"lat\": float,\n",
    "  \"long\":float,\n",
    "  \"pos\":float,\n",
    "  \"speed_limit\":int\n",
    "}\n",
    "```\n",
    "### Example\n",
    "```\n",
    "{\n",
    "  \"_id\"; 1,\n",
    "  \"lat\": 2.157730731,\n",
    "  \"long\":102.6601002,\n",
    "  \"pos\":152.5,\n",
    "  \"speed_limit\":110\n",
    "}\n",
    "```\n",
    "- Purpose: Holds the static definition of every AWAS checkpoint (including its position on the highway and speed limit), to keep track of the location of the camera whenever a vehicle violated collected in Collection Violation\n",
    "- Indexes:\n",
    "    - _id (single-field unique): To make it easier to search through start and end via fast lookup during distance calculations\n",
    "    - pos (single-field): To support \"next-camera\" queries whenever we want to dynamically find the next camera downstream\n",
    "- Shard Key: None\n",
    "    - Reason: This collection is small and mostly static; would introduce additional complexity if sharded\n",
    "___\n",
    "## Vehicle\n",
    "### Schema\n",
    "```\n",
    "{\n",
    "    \"_id\": str, // car_plate\n",
    "    \"owner_name\":str,\n",
    "    \"owner_addr\":str,\n",
    "    \"vehicle_type\": {\n",
    "        type: String,\n",
    "        enum: ['Sedan\", \"SUV\", \"Coupe\", \"Hatchback\", \"Truck\"],\n",
    "        required: true\n",
    "    },\n",
    "    \"registration_date\": ISODate\n",
    "}\n",
    "```\n",
    "### Example\n",
    "```\n",
    "{\n",
    "    \"_id\": CJ 9,\n",
    "    \"owner_name\": \"John Doe\",\n",
    "    \"owner_addr\": \"123 Maple Street\",\n",
    "    \"vehicle_type\": \"Coupe\",\n",
    "    \"registration_date\": ISODate(\"2023-04-25T10:30:00Z\")\n",
    "}\n",
    "```\n",
    "- Purpose: Holds the static registry of car plates and their details (owner info, vehicle type, etc.), which later can be retrieved to track the owner of a violation\n",
    "- Indexes:\n",
    "    - _id (implicit): Direct plate to document lookup\n",
    "- Shard Key: None\n",
    "    - Reason: This collection is also small and mostly static; lookups by plate are mostly cheap as the primary key\n",
    "___\n",
    "## Violation\n",
    "### Schema\n",
    "```\n",
    "{\n",
    "    \"violation_id\": string,\n",
    "    \"car_plate\": string,\n",
    "    \"date\": ISODate, // date at midnight UTC (or locally)\n",
    "    \"violations\": [\n",
    "        {\n",
    "            \"type\": {\n",
    "                type: string,\n",
    "                enum: [\"instantaneous\",\"average\"],\n",
    "                required: true\n",
    "            },\n",
    "            \"camera_id_start\": int,\n",
    "            \"camera_id_end\": int, // will be null if instantenous\n",
    "            \"timestamp_start\": ISODate,\n",
    "            \"timestamp_end\": ISODate, // will be null if instantenous\n",
    "            \"measured_speed\": float,\n",
    "            \"speed_limit\": int\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "### Example\n",
    "```\n",
    "{\n",
    "    \"violation_id\": \"afa0f670-41b2-4f73-85e7-c8968ca1752d\",\n",
    "    \"car_plate\": \"ZZY 1\",\n",
    "    \"date\": ISODate(\"2025-05-14T00:00:00Z\"),\n",
    "    \"violations\": [\n",
    "        {\n",
    "            \"type\": \"instantaneous\"\n",
    "            \"camera_id_start\": 2,\n",
    "            \"camera_id_end\": null,\n",
    "            \"timestamp_start\": ISODate(\"2025-05-14T08:10:03Z\"),\n",
    "            \"timestamp_end\": null,\n",
    "            \"measured_speed\": 125.2,\n",
    "            \"speed_limit\": 110\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"average\"\n",
    "            \"camera_id_start\": 2,\n",
    "            \"camera_id_end\": 3,\n",
    "            \"timestamp_start\": ISODate(\"2025-05-14T09:15:10Z\"),\n",
    "            \"timestamp_end\": ISODate(\"2025-05-14T09:16:40Z\"),\n",
    "            \"measured_speed\": 112.5,\n",
    "            \"speed_limit\": 110\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "- Purpose: Each document summarizes all speed-limit violations for one vehicle on one calendar day. Instantaneous and camera-to-camera (average) violations both live in an embedded array, and a unique constraint on (car_plate, date) guarantees at most one document per vehicle per day.\n",
    "- Indexes:\n",
    "    - (car_plate (ascending), date (descending)) (compound, unique): Prevents two seperate documents from having the same car and date, so the data streaming logic can push extra infractions on that day into the `violations` field instead. Also allows fast lookup of a car's most recent violation document.\n",
    "    - violation_id (ascending) (unique): To allow quick fetching of individual violations themself\n",
    "    - date (ascending) (single-field): Allows speedup of queries what wants to locate violations between a certain period of time\n",
    "    - violations.camera_id_start (ascending): Allows quick find of all violations at a specific starting camera\n",
    "    - violations.camera_id_end (ascending): Allows quick find of all violations at a specific ending camera (applicable only to average violations)\n",
    "    - (date (ascending), violations.measured_speed (descending)): Allows analytics for top speeds during the day\n",
    "- Shard Key: (car_plate: hashed: date: range):\n",
    "    - Reason: Hashing `car_plate` spreads write evenly across the whole cluster (as plates are high-cardinality), while sharding date allows efficiency when targetting a date or date range when needed\n",
    "- Data Retention Policy: Data expires after 5 years, so a TTL index on `date` will be applied to remove any affected docs after 5 years, using `expireAfterSeconds: 5 * 365 * 24 * 60 * 60`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9db5c3",
   "metadata": {
    "id": "ed9db5c3"
   },
   "source": [
    "### Task 1.2 Collection Relationship\n",
    "\n",
    "In this task, specify the relationships between collections and explain whether you choose to embed data or store references. Justify your choice in terms of:\n",
    "* Read/write patterns\n",
    "* Data duplication versus Join cost\n",
    "* Consistency requirements (if applicable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R6Xys-rnwBtg",
   "metadata": {
    "id": "R6Xys-rnwBtg"
   },
   "source": [
    "### Violation to Camera\n",
    "- **Cardinality**: Many violations reference One camera\n",
    "- **Read and Write patterns**:\n",
    "    - **Writes**: Only store `camera_id` in Violation whenever an instantaneous or average violation has been performed\n",
    "    - **Reads**: Camera metadata (geolocation, speed_limit, etc.) might occasionally be looked up when displaying or analyzing a violation.\n",
    "- **Embed or Reference**: Data Reference, since there is little application where we need to get camera metadata even though data points is small and static\n",
    "- **Data Duplication vs Join Cost**:\n",
    "    - Camera documents are small and static; if embedded the camera's metadata would duplicate that data in every violation document, which is wastefl and unnecessary.\n",
    "    - A single indexed lookup in Camera is very cheap\n",
    "- **Consistency**:\n",
    "    - If theres ever any need to adjust a camera's position or limit, it's possible to update at one place.\n",
    "\n",
    "### Violation to Vehicle\n",
    "- **Cardinality**: Many violations reference One vehicle\n",
    "- **Read and Write patterns**:\n",
    "    - **Writes**: Records just the `car_plate` (`_id`) in Violation\n",
    "    - **Reads**: For law enforcement or traffic authority use cases, vehicle information needs to be quickly accessible when reviewing a violation. This requires frequent reads of vehicle metadata when querying violations.\n",
    "- **Embed or Reference**: Data Reference, because police officers frequently need to retrieve detailed vehicle information during enforcement or investigation, making joins necessary to ensure flexibility and Violation's retention policy still in play.\n",
    "- **Data Duplication vs Join Cost**:\n",
    "    - Owner name, address, vehicle type, and registration date can be worth hundreds of bytes, so duplicating that on every violation quickly inflates storage.\n",
    "    - A good indexed lookup on `car_plate` (`_id`) is very cheap in MongoDB\n",
    "- **Consistency**:\n",
    "    - Important that Collection Vehicle must be up-to-update and correct in term of vehicle ownership. This is why we use reference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e797ff17",
   "metadata": {
    "id": "e797ff17"
   },
   "source": [
    "### Task 1.3 Discussion\n",
    "\n",
    "In this task, discuss whether your model supports\n",
    "* Consistency and Idempotency\n",
    "    * Does it support idempotent writes?\n",
    "    * Explain any upsert pattern in `violation` collection\n",
    "* Scalability and Fault-Tolerance\n",
    "    * Can your data model support high ingest rates?\n",
    "    * Can your data model support low-latency lookups?\n",
    "\n",
    "Justify and explain the trade-off made in your design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BnatQrFtv7Ti",
   "metadata": {
    "id": "BnatQrFtv7Ti"
   },
   "source": [
    "### Idempotency\n",
    "Yes, the model supports idempotent writes, as it enforced using the compound unique index on (car_plate: 1, date: 1) plus an upsert operation in MongoDB. That means if Spark retries the same event, it will hit the same daily document rather than creating a new one. For example, writing a violation record with the same data (same car_plate, same date, camera_id_start, etc.) multiple times will have no effect on the existing record but still making sure reference still making further drill down operation possible.\n",
    "\n",
    "### Upsert Pattern\n",
    "Through filtering and select of the eligible window (one day) we can append new row into the sharded collections. As the update and merge operation are handled by the Spark cluster there is no need to update as a part of system design\n",
    "\n",
    "### High Ingest Rate\n",
    "Collection Violation should be able to support this since the allowed operation is append and drop per batch, since filter, merge is already handled by the PySpark Cluster.\n",
    "\n",
    "### Low Latency Lookup\n",
    "Through a sharded key design we added, query based on the car plate is efficient and to join it with the Collection Vehicle and still able to search through different indexes.\n",
    "\n",
    "### Trade off\n",
    "- Scalability and Complexity: to support high ingest rate and Query based on car plate we added complexity through a sharded key\n",
    "\n",
    "- Dependent on Spark Cluster: to enable idempotency & high ingest rate, we are dependent on a consistent and efficient Spark operations to filter and merge properly, making sure there is no duplicate write.\n",
    "\n",
    "- Performance:The system is optimized for write-heavy workloads, which is important for handling a high rate of incoming violations. By using batch processing with Spark and append-only writes, we can process high volume of data. However, the complexity introduced by sharding and Spark's batch processing should be monitored to ensure performance remains optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eafe9a9",
   "metadata": {
    "id": "4eafe9a9"
   },
   "source": [
    "## <span style=\"color:#0b486b\">Part 2: Streaming Application</span>\n",
    "\n",
    "This section consists of 2 sub-questions.\n",
    "\n",
    "In this task, you will implement a streaming application to simulate the AWAS system. Figure 2 illustrates an overview of the streaming architecture that is to be developed to simulate AWAS. Implementation is expected to be following programming standards with high readability (supported with documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51a153",
   "metadata": {
    "id": "5b51a153"
   },
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"FIT3182_A2_Fig_2.png\"></img>\n",
    "    <p style=\"text-align: center\">Figure 2 - Overview of streaming application to simulate AWAS </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d7403",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "474fc3a8",
   "metadata": {
    "id": "474fc3a8"
   },
   "source": [
    "### Task 2.1 Data Stream Processing\n",
    "\n",
    "In this task, you will implement multiple **Apache Kafka** producers to simulate the real-time streaming of the data, which will be processed by **Apache Spark Structured Streaming** client and then inserted into MongoDB.\n",
    "\n",
    "*<span style=\"color:red\">Important note:</span> You are expected to use the same data model from Task 1. To make the streaming data consistent for the model, you may need to make some changes to the streaming data before building the model or inserting it to MongoDB.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989065f",
   "metadata": {
    "id": "a989065f"
   },
   "source": [
    "#### Event Producer\n",
    "\n",
    "**Event Producer A**: Write a python program that loads all the data from `camera_event_A.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_a.ipynb**, where **xxx** represents the student IDs of the group members.\n",
    "\n",
    "**Event Producer B**: Write a python program that loads all the data from `camera_event_B.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_b.ipynb**, where **xxx** represents the student IDs of the group members.\n",
    "\n",
    "**Event Producer C**: Write a python program that loads all the data from `camera_event_C.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_c.ipynb**, where **xxx** represents the student IDs of the group members."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c7a85d",
   "metadata": {
    "id": "d0c7a85d"
   },
   "source": [
    "#### Streaming Application\n",
    "\n",
    "Write a streaming application using Apache Spark Structured Streaming API which processes data in batches. Each batch should contain 0 or more camera event (from event producer). The streaming application should process the data as follows.\n",
    "* Join the streaming data from the producers and determine if a vehicle should be flagged as violation. You should drop any data pair if the timestamp and the order of the camera does not match.\n",
    "* If there is a violation detected, store it into MongoDB straight away.\n",
    "* If there is no violation detected, drop the record.\n",
    "* Due to the dynamic nature of moving vehicle, the time of vehicle completing the camera segment may vary and you should decide how many records and how long the records should be stored in the buffer until a pair is identified.\n",
    "\n",
    "##### Violation Detection Rule\n",
    "A vehicle is flagged as violating the speed limit if any one of the following happens.\n",
    "* Instantaneous speed of vehicle exceed the speed limit of the recording camera\n",
    "* Average speed of vehicle exceed the speed limit of the ending camera\n",
    "\n",
    "<span style=\"color:red\">Important Note:</span> *Only one record for a car per day is recorded in the database. If the car violates at **different cameras**, the record should be merged together into a single record to be stored in the database.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d11c4a",
   "metadata": {
    "id": "e8d11c4a"
   },
   "source": [
    "### Task 2.2 Data Visualisation\n",
    "\n",
    "In this task, you will implement a program to visualize the joined streaming data. For the incoming camera event(s),\n",
    "* plot the number of violation against arrival time. You need to label some interesting points such as maximum and minimum values.\n",
    "* In addition to that, plot the speed against arrival time. You need to include some interesting points such as average and maximum values.\n",
    "\n",
    "For visualization on the data stored in the database, you have to plot a map using camera location. On the map, annotate\n",
    "* number of violations between the checkpoints\n",
    "* identify hotspot (e.g. when number of violations exceed certain threshold within a time in a day)\n",
    "\n",
    "Explain and justify the plots and the inclusion of the interesting points. Set your own threshold for the hotspot.\n",
    "\n",
    "If you are running this task in a separate Jupyter notebook file, save the file as **xxx_assignment02_visualisation.ipynb**, where **xxx** represents the student IDs of the group members.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b668a",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3562591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IP USED THROUGHOUT THE ASSIGNMENT\n",
    "hostip = \"172.22.32.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456e6c7",
   "metadata": {},
   "source": [
    "## MongoDB Collections Creation\n",
    "Before starting with Kafka Streaming and Spark Job queries, we need to prepare datsets we have into mongoDB collections to access during the queries, Sink writer, and vizualisation creation. The schema, index choices and reasoning are outlined in the task 1. \n",
    "But first, we instantiate the MongoClient variables.\n",
    "\n",
    "For further information on operations we did,  refer to:\n",
    "- [PyMongo Getting Started Guide](https://www.mongodb.com/docs/languages/python/pymongo-driver/current/get-started/)\n",
    "- [PyMongo Indexing Docs](https://www.mongodb.com/docs/languages/python/pymongo-driver/current/indexes/#std-label-pymongo-indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83d95168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo \n",
    "from pymongo import MongoClient\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "### Instantiate the MongoClient needed firstly.\n",
    "client = MongoClient(hostip, 27017)\n",
    "db = client.fit3182_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e41834b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensured index on 'pos': pos_idx\n",
      "Inserted 3 camera documents.\n",
      "Current indexes on Camera:\n",
      " • _id_: [('_id', 1)]\n",
      " • pos_idx: [('pos', 1)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CAMERA.CSV BULK IMPORT TO COLLECTION CAMERA\n",
    "-----------------------------------------------\n",
    "This cell are created to bulk upload of camera information into a mongoDB collectin\n",
    "key steps:\n",
    "- Checking whether Camera collection has been populated before\n",
    "- parse through the file using library csv and create convert them into:\n",
    "        - `_id`: camera ID (used as a unique identifier)\n",
    "        - `lat`: latitude of the camera\n",
    "        - `long`: longitude of the camera\n",
    "        - `pos`: position in the sequence\n",
    "        - `speed_limit`: speed limit at the camera location\n",
    "        \n",
    "Output:\n",
    "- Log whether we can import or upload has been done succesfully.\n",
    "\"\"\"\n",
    "# ── Your starter connection ──\n",
    "camera_coll = db.Camera\n",
    "\n",
    "#clear out the collection first \n",
    "deleted = camera_coll.drop({})\n",
    "print(f\"Cleared collection.\")\n",
    "\n",
    "# ── Skip if already imported ──\n",
    "if camera_coll.estimated_document_count() > 0:\n",
    "    print(\"Camera collection already contains data. Skipping import.\")\n",
    "else:\n",
    "    idx_name = camera_coll.create_index(\n",
    "        [(\"pos\", pymongo.ASCENDING)],\n",
    "        name=\"pos_idx\"\n",
    "    )\n",
    "    print(f\"Ensured index on 'pos': {idx_name}\")\n",
    "    \n",
    "    # ── Load CSV and insert ──\n",
    "    csv_path = 'data/camera.csv'\n",
    "    docs = []\n",
    "    with open(csv_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            docs.append({\n",
    "                \"_id\":        int(row['camera_id']),\n",
    "                \"lat\":        float(row['latitude']),\n",
    "                \"long\":       float(row['longitude']),\n",
    "                \"pos\":        float(row['position']),\n",
    "                \"speed_limit\": int(row['speed_limit'])\n",
    "            })\n",
    "    \n",
    "    if docs:\n",
    "        result = camera_coll.insert_many(docs)\n",
    "        print(f\"Inserted {len(result.inserted_ids)} camera documents.\")\n",
    "        print(\"Current indexes on Camera:\")\n",
    "        for name, info in camera_coll.index_information().items():\n",
    "            print(f\" • {name}: {info['key']}\")\n",
    "    else:\n",
    "        print(\"No camera records found in CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "272485e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared collection.\n",
      "Inserted 9844 new vehicle documents.\n",
      "Updated 69 existing vehicle documents.\n",
      "Current indexes on Vehicle:\n",
      " • _id_: [('_id', 1)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "VEHICLE BULK UPLOAD TO MONGODB\n",
    "-------------------------------\n",
    "This cell are implemented to bulk upload into collection Vehicle, \n",
    "because of the nature of the vehicle.csv dataset, there are bound to be \n",
    "duplicate car plate which tells us there are same vehicle that change ownership \n",
    "throughout the year, because of this we implement a updating mechanism \n",
    "\n",
    "key steps:\n",
    "- Deletes all documents in the `Vehicle` collection at the start for a clean slate.\n",
    "- Parse through the Vehicle.csv and convert them into: \n",
    "    - car_plate → used as _id in MongoDB\n",
    "    - owner_name\n",
    "    - owner_addr\n",
    "    - vehicle_type\n",
    "    - registration_date` (to ISO format)\n",
    "- insert and update using insert_one(), update_one()\n",
    "- output to log if we update or insert\n",
    "\"\"\"\n",
    "\n",
    "# ── Connection ──\n",
    "vehicle_coll = db.Vehicle\n",
    "\n",
    "#clear out the collection first \n",
    "deleted = vehicle_coll.drop()\n",
    "print(f\"Cleared collection.\")\n",
    "\n",
    "if vehicle_coll.estimated_document_count() > 0:\n",
    "    print(\"Vehicle collection already contains data. Skipping import.\")\n",
    "else:\n",
    "    # ── Prepare sets & counters ──\n",
    "    existing_ids = set(vehicle_coll.distinct('_id'))\n",
    "    seen_in_file = set()\n",
    "    docs_to_insert = []\n",
    "    update_count = 0\n",
    "    added_count = 0\n",
    "\n",
    "    csv_path = 'data/vehicle.csv'\n",
    "    with open(csv_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            plate = row['car_plate']\n",
    "\n",
    "            # Parse the incoming registration_date\n",
    "            ts = row['registration_date'].rstrip(\"Z\")\n",
    "            reg_date = datetime.fromisoformat(ts)\n",
    "\n",
    "            if plate in seen_in_file:\n",
    "                # Plate already in DB → check whether to update\n",
    "                existing = vehicle_coll.find_one(\n",
    "                    {\"_id\": plate}\n",
    "                )\n",
    "                if existing and reg_date > existing['registration_date']:\n",
    "                    # Only update if the CSV date is newer\n",
    "                    vehicle_coll.update_one(\n",
    "                        {\"_id\": plate},\n",
    "                        {\"$set\": {\n",
    "                            \"registration_date\": reg_date,\n",
    "                            \"owner_name\":        row['owner_name'],\n",
    "                            \"owner_addr\":        row['owner_addr'],\n",
    "                            \"vehicle_type\":      row['vehicle_type']\n",
    "                        }}\n",
    "                    )\n",
    "                    update_count += 1\n",
    "            else:\n",
    "                seen_in_file.add(plate)\n",
    "                # Brand-new plate → schedule for insert\n",
    "                vehicle_coll.insert_one({\n",
    "                    \"_id\":               plate,\n",
    "                    \"owner_name\":        row['owner_name'],\n",
    "                    \"owner_addr\":        row['owner_addr'],\n",
    "                    \"vehicle_type\":      row['vehicle_type'],\n",
    "                    \"registration_date\": reg_date\n",
    "                })\n",
    "                added_count += 1\n",
    "\n",
    "    # ── Do the batch insert, if any ──\n",
    "    if added_count > 0:\n",
    "        print(f\"Inserted {added_count} new vehicle documents.\")\n",
    "    else:\n",
    "        print(\"No new vehicle records to insert.\")\n",
    "\n",
    "    # ── Report on any updates we made ──\n",
    "    if update_count:\n",
    "        print(f\"Updated {update_count} existing vehicle document{'s' if update_count>1 else ''}.\")\n",
    "\n",
    "    # ── (Optional) show your indexes ──\n",
    "    print(\"Current indexes on Vehicle:\")\n",
    "    for name, info in vehicle_coll.index_information().items():\n",
    "        print(f\" • {name}: {info['key']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "255487a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared collection..\n",
      "Inserted 50000 violation documents.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "INSTANTIATE COLLECTION VIOLATION AND BULK UPLOAD HISTORIC VIOLATION\n",
    "-------------------------------------------------------------------\n",
    "This script initializes the `Violation` collection in the `fit3182_db` MongoDB database\n",
    "and bulk imports historical average-speed violation data from the CSV file: `camera_event_historic.csv`.\n",
    "Key steps:\n",
    "- drop the violation_coll to make sure we start with clean slate\n",
    "- create indexes as specified in the task 1.\n",
    "- parse through the camera_event_historic.csv using library csv, convert them into:\n",
    "    - timestamp_start (ISO format)\n",
    "    - timestamp_end (ISO format)\n",
    "    - date buckets from timestamp_start\n",
    "    - parse through violation point into a list inside a row.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ── MongoDB Connection ──\n",
    "violation_coll = db.Violation\n",
    "\n",
    "# Clear out the collection first \n",
    "violation_coll.drop()\n",
    "print(f\"Cleared collection.\")\n",
    "\n",
    "# ── Index Creation ──\n",
    "violation_coll.create_index([(\"violations.violation_id\", 1)], name=\"idx_violation_id\")\n",
    "violation_coll.create_index([(\"date\", 1)], name=\"idx_date\")\n",
    "violation_coll.create_index([(\"violations.camera_id_start\", 1)], name=\"idx_camera_start\")\n",
    "violation_coll.create_index([(\"violations.camera_id_end\", 1)], name=\"idx_camera_end\")\n",
    "violation_coll.create_index([(\"date\", 1), (\"violations.measured_speed\", -1)], name=\"idx_measured_speed\")\n",
    "violation_coll.create_index([(\"violations.timestamp_start\", 1)], name=\"idx_timestamp_start\")\n",
    "\n",
    "\n",
    "# ── CSV Read ──\n",
    "csv_path = \"data/camera_event_historic.csv\"\n",
    "docs = []\n",
    "\n",
    "with open(csv_path, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        # Parse timestamp_start to datetime\n",
    "        if row.get('timestamp_start'):\n",
    "            timestamp_start = datetime.fromisoformat(row['timestamp_start'].rstrip(\"Z\"))\n",
    "        else:\n",
    "            # skip row if no timestamp_start\n",
    "            continue\n",
    "\n",
    "        timestamp_end = None\n",
    "        if row.get('timestamp_end'):\n",
    "            timestamp_end = datetime.fromisoformat(row['timestamp_end'].rstrip(\"Z\"))\n",
    "\n",
    "        # Create date bucket from timestamp_start (just the date part)\n",
    "        date_bucket = datetime(timestamp_start.year, timestamp_start.month, timestamp_start.day)\n",
    "\n",
    "        # Construct document\n",
    "        violation_doc = {\n",
    "            \"car_plate\": row['car_plate'],\n",
    "            \"date\": date_bucket,\n",
    "            \"violations\": [\n",
    "                {\n",
    "                    \"violation_id\": str(uuid.uuid4()),\n",
    "                    \"type\": \"average\",\n",
    "                    \"camera_id_start\": row['camera_id_start'],\n",
    "                    \"camera_id_end\": row['camera_id_end'] if row.get('camera_id_end') else None,\n",
    "                    \"timestamp_start\": timestamp_start,\n",
    "                    \"timestamp_end\": timestamp_end,\n",
    "                    \"measured_speed\": float(row['speed_reading']) if row.get('speed_reading') else None,\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        docs.append(violation_doc)\n",
    "\n",
    "# ── Insert All Documents ──\n",
    "if docs:\n",
    "    violation_coll.insert_many(docs)\n",
    "    print(f\"Inserted {len(docs)} violation documents.\")\n",
    "else:\n",
    "    print(\"No documents to insert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3883bc",
   "metadata": {},
   "source": [
    "## Implement Kafka Producers\n",
    "To enable steraming, we need to take use of Kafka streaming, creating 3 different streams for each of:\n",
    "- Camera A ([source](data/camera_event_A.csv))\n",
    "- Camera B ([source](data/camera_event_B.csv))\n",
    "- Camera C ([source](data/camera_event_C.csv))\n",
    "\n",
    "Kafka stream consist of two components, Producers and in this case, 1 Consumer, communicating to stream data from different camera through Zookeeper.\n",
    "A Producer are instantiated by inputting:\n",
    "- kafka server ip address and port\n",
    "- topic assigned to the stream\n",
    "- batch interval being transported\n",
    "\n",
    "Additionally, we attach metadata to every message to help us track and debug the data flow. This metadata includes:\n",
    "\n",
    "- `sent_at`: Timestamp indicating when the message was sent\n",
    "\n",
    "- `producer_id`: Identifier for the specific producer\n",
    "\n",
    "Since we need to run separate producers for each camera (three producers with topics: `camera-a`, `camera-b`, and `camera-c`), we use Python's threading library to execute all producers concurrently from a single script and kernel. Implementation can be seen in [producer_all.ipynb](producer_all.ipynb) for the implementation details.\n",
    "\n",
    "For more around kafka, refer to the official [doc. ](https://kafka.apache.org/documentation/#producerapi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5f42e",
   "metadata": {},
   "source": [
    "## Spark Instance Processing of 3 different camera streams\n",
    "In this setup, a single Spark Structured Streaming application processes three separate Kafka streams, each corresponding to a different traffic camera feed (Camera A, B, and C). These streams deliver real-time vehicle event data such as car plate number, timestamp, and speed readings.\n",
    "\n",
    "---\n",
    "### Key Operations\n",
    "- Stream Joins: For average violation detection, joins are performed between camera streams (e.g., between Camera A and B or B and C) based on car plate numbers and timestamps. This allows for the correlation of entries to compute travel duration and average speed between checkpoints.\n",
    "- Violation Detection: Each stream is independently analyzed to detect two types of violations:\n",
    "    - Instant Violation: When a vehicle exceeds the speed limit at a single camera.\n",
    "    - Average Violation: When a vehicle’s average speed between two cameras exceeds the limit.\n",
    "- Filtering: After initial violation tagging, the application applies filter operations to drop messages that do not meet required conditions — for instance, records that are missing a valid timestamp_start or fall outside of a defined watermark window.\n",
    "\n",
    "---\n",
    "### Different Componenets\n",
    "Utimately, these are the different components used in this part:\n",
    "- 3 [Kafka](\"produce_all.ipynb\") streams (topic: camera_a, camera_b, camera_c)\n",
    "- Instantiated spark session\n",
    "- MongoDB client to read and write\n",
    "- [DBwriter](\"Operations.py\") to write the results\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"docs/stream_processing.png\"></img>\n",
    "    <p style=\"text-align: center\">Figure 4 - Flow Chart of Stream Processing </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "422aff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql.types import BooleanType\n",
    "import pandas as pd\n",
    "from Operations import SparkInst\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1 pyspark-shell'\n",
    "spark_job=SparkInst(\"AWAS SYSTEM\", 5, kafka_output_topic=\"violations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a135d",
   "metadata": {},
   "source": [
    "### Enable Instant Violation Flagging\n",
    "In here, we are required to flag which car plate violate instant violation based on their speed reasing given in each camera, compared with the speed limit listed in each camera we saved inside the MongoDB collection's `Camera`.\n",
    "\n",
    "As this task require us to access and read through a small static data from a mongoDB collection, we decided to implement Pyspark's `broacast` Broadcasting allows us to distribute small, read-only datasets to all worker nodes, ensuring that each executor can access this data locally without repeated shuffling or lookups from the driver. This is particularly useful in our use case where we need to use a small amount of data (`camera_id` & `speed_limit`) over and over. To do this, we need to turn `Camera` into a pySpark's `DataFrame`.\n",
    "\n",
    "---\n",
    "### Implementation Steps\n",
    "- To do this, we need to turn `Camera` into a pySpark's `DataFrame`.\n",
    "- run pySpark's `broadcast()`\n",
    "- Transformed into a UDF called `speeding_udf()`\n",
    "- Trigger `speeding_udf()` inside another function.\n",
    "---\n",
    "### Design Justification\n",
    "- By embedding key transformation logic and reference data (e.g., violation thresholds) directly into the stream processing pipeline—rather than querying an external database like MongoDB at runtime—we **reduce network overhead and latency**. This approach eliminates costly remote lookups during high-throughput scenarios and ensures more consistent processing times.\n",
    "\n",
    "\n",
    "---\n",
    "For more info refer to:\n",
    "- [broadcast pyspark guide](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.broadcast.html)\n",
    "- [UDF pyspark guide](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html)\n",
    "- [Pandas to Dataframe gpyspark guide](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/pandas_pyspark.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84b0ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BROADCAST AND UDF INSTANTIATION\n",
    "------------------------------------\n",
    "This cell are created to enable brodcasting of collection Camera dataframe \n",
    "and UDF function that create specific column to the operation being done (average / instant)\n",
    "this UDF function going to mark which row violate or not.\n",
    "\"\"\"\n",
    "#read through the mongoDB's collection and transformed into Pandas\n",
    "camera_coll=client.fit3182_db.Camera\n",
    "cursor = camera_coll.find()\n",
    "df_pd = pd.DataFrame(list(cursor))\n",
    "\n",
    "#rename mongo index _id as camera_id\n",
    "if '_id' in df_pd.columns:\n",
    "    df_pd.rename(columns={'_id': 'camera_id'}, inplace=True)\n",
    "    \n",
    "# Convert the pandas DataFrame into a Spark DataFrame\n",
    "spark_df = spark_job.get_session().createDataFrame(df_pd)\n",
    "\n",
    "#broacast just as a dictionary into all nodes\n",
    "speed_limit_map = {row['camera_id']: row['speed_limit'] for row in spark_df.select(\"camera_id\", \"speed_limit\").collect()}\n",
    "broadcast_map = spark_job.essentialData_broadcast(spark_df)\n",
    "\n",
    "def mark_speeding(camera_id: str, speed: float, ops: str) -> bool:\n",
    "    \"\"\"\n",
    "    Marks whether a given speed reading is a violation based on the speed limit \n",
    "    for the specified camera for given operations (e.g instant & average)\n",
    "\n",
    "    Input:\n",
    "    - camera_id (str): The ID of the camera that recorded the speed.\n",
    "    - speed (float): The speed measured by the camera.\n",
    "    - ops (str): The type of operation (\"instant\" or \"average\").\n",
    "\n",
    "    Output:\n",
    "    - bool: True if the speed exceeds the limit (i.e., it's a violation), False otherwise.\n",
    "    \"\"\"\n",
    "    limit = broadcast_map.value.get(camera_id)\n",
    "    if limit is not None and ops == \"instant\":\n",
    "        return True if speed > limit else False\n",
    "    elif limit is not None and ops == \"average\":\n",
    "        return True  if speed > limit else False\n",
    "    return False\n",
    "\n",
    "#turn mark_speeding(camera_id, speed, ops)\n",
    "speeding_udf = udf(mark_speeding, BooleanType())\n",
    "\n",
    "# Step 5: Apply UDF to each streaming dataframe\n",
    "def add_speed_flag(df, ops: str):\n",
    "    \"\"\"\n",
    "    Function that trigger the mark_speeding() and transform the streaming processed\n",
    "    into different columns based on the operation\n",
    "    Input:\n",
    "        - df (DataFrame): Spark DataFrame containing 'camera_id' and 'speed' columns.\n",
    "        - ops (str): The type of operation (\"instant\" or \"average\").\n",
    "    Output:\n",
    "        - a spark Dataframe with additional column, speed_flag_instant / speed_flag_average\n",
    "   \n",
    "    \"\"\"\n",
    "    return df.withColumn(f\"speed_flag_{ops}\", speeding_udf(col(\"camera_id\"), col(\"speed_reading\"), lit(ops)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcd481b",
   "metadata": {},
   "source": [
    "## Applying Instant Violation & Prepare for Join\n",
    "This section outlines the ingestion and initial preprocessing steps for three real-time traffic camera streams using Apache Kafka and PySpark Structured Streaming. Each Kafka topic represents a distinct traffic camera, and the data includes vehicle speed readings, timestamps, and other relevant metadata. \n",
    "\n",
    "---\n",
    "### Implemented Steps\n",
    "- Attach 3 streams into Spark Session `attach_kafka_stream(topic_id:str, kafka_server:str, watermark:str)`\n",
    "- Add instant violation to using the UDf function we created `add_speed_flag(df:DataFrame, ops:str)` we implement earlier.\n",
    "- Renaming stream camera a, stream camera b, camera c except the car_plate. \n",
    "\n",
    "---\n",
    "### Design Justification\n",
    "- We apply a **24-hour watermark** to each input stream to handle late-arriving data effectively. This design choice aligns with the system specifications, which require that events be considered valid for up to 24 hours after their timestamp. By setting this watermark, we ensure that events delayed are still eligible to be joined within the defined 10-minute interval window between cameras. This enhances the completeness and accuracy of downstream aggregations and flagging logic during streaming joins.\n",
    "\n",
    "- Renaming and maintaining distinct aliases for each stream (e.g., `camera_id_a`, `timestamp_b`, etc.) ensures traceability and debuggability across all processing stages. It prevents field name collisions and makes the query logic more readable and maintainable. Additionally, when invoking `PySpark’s query()` execution plan or reviewing physical and logical plans, having clearly differentiated fields minimizes confusion and helps verify that the data has not been accidentally overwritten or misjoined. \n",
    "\n",
    "\n",
    "---\n",
    "For more information refer to:\n",
    "- [pySpark's Kafka Integration Guide](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)\n",
    "- [pySpark Expression Guide](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.expr.html)\n",
    "- [pySpark Watermarking Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "788d2107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, lit\n",
    "\n",
    "# Attach Kafka streams into Spark Session we instantiate\n",
    "stream_a = spark_job.attach_kafka_stream(\"camera_event_a\", hostip, \"24 hours\")\n",
    "stream_b = spark_job.attach_kafka_stream(\"camera_event_b\", hostip, \"24 hours\")\n",
    "stream_c = spark_job.attach_kafka_stream(\"camera_event_c\", hostip, \"24 hours\")\n",
    "\n",
    "# Flag and drop unnecessary fields\n",
    "stream_a_flagged = add_speed_flag(stream_a.drop(\"event_id\", \"sent_at\", \"batch_id\"), \"instant\")\n",
    "stream_b_flagged = add_speed_flag(stream_b.drop(\"event_id\", \"sent_at\", \"batch_id\"), \"instant\")\n",
    "stream_c_flagged = add_speed_flag(stream_c.drop(\"event_id\", \"sent_at\", \"batch_id\"), \"instant\")\n",
    "\n",
    "# Rename a  for joining\n",
    "a = stream_a_flagged.selectExpr(\n",
    "    \"car_plate\",\n",
    "    \"camera_id as camera_id_a\",\n",
    "    \"timestamp as timestamp_a\",\n",
    "    \"speed_reading as speed_reading_a\",\n",
    "    \"producer_id as producer_a\",\n",
    "    \"speed_flag_instant as speed_flag_instant_a\"\n",
    ")\n",
    "# Rename b for joining\n",
    "b = stream_b_flagged.selectExpr(\n",
    "    \"car_plate\",\n",
    "    \"camera_id as camera_id_b\",\n",
    "    \"timestamp as timestamp_b\",\n",
    "    \"speed_reading as speed_reading_b\",\n",
    "    \"producer_id as producer_b\",\n",
    "    \"speed_flag_instant as speed_flag_instant_b\"\n",
    ")\n",
    "# Rename c for joining\n",
    "c = stream_c_flagged.selectExpr(\n",
    "    \"car_plate\",\n",
    "    \"camera_id as camera_id_c\",\n",
    "    \"timestamp as timestamp_c\",\n",
    "    \"speed_reading as speed_reading_c\",\n",
    "    \"producer_id as producer_c\",\n",
    "    \"speed_flag_instant as speed_flag_instant_c\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d5516e",
   "metadata": {},
   "source": [
    "## Multi-Camera Stream Join and Filtering\n",
    "\n",
    "This section describes the process of joining traffic event streams from three independent cameras (A, B, and C) to reconstruct a vehicle’s movement path and evaluate both instant and average speed violations across checkpoints. Inside the join operations, we also fulfilled the drop specification outlined in the system requirement.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Join Between Camera A and Camera B\n",
    "\n",
    "An inner join is performed between streams `a` and `b` based on the following conditions:\n",
    "\n",
    "- The `car_plate` value must match to ensure both events correspond to the same vehicle.\n",
    "- The timestamp from Camera A must occur before the timestamp from Camera B (`timestamp_a > timestamp_b`).\n",
    "- The time difference between these two events must be less than or equal to the window, 10 minutes. \n",
    "\n",
    "After the join, the following fields are selected:\n",
    "\n",
    "- Metadata from Camera A and Camera B.\n",
    "- The average speed between the two observations\n",
    "- An average speed violation flag (`speed_flag_average_ab`) determined using a custom user-defined function (`speeding_udf`).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Join Between AB Join Result and Camera C\n",
    "\n",
    "The intermediate result from the A-B join (`ab_join`) is further joined with Camera C's stream using the following criteria:\n",
    "\n",
    "- The `car_plate` must match to track the same vehicle.\n",
    "- The timestamp from Camera B must occur before the timestamp from Camera C (`timestamp_b > timestamp_c`).\n",
    "- The time difference between these two events must be less than or equal to the window, 10 minutes. \n",
    "\n",
    "This second join yields:\n",
    "- A complete trace of the vehicle across all three cameras.\n",
    "- The average speed between Camera B and C labelled as `average_speed_bc`\n",
    "- An average speed violation flag (`speed_flag_average_bc`) determined using a custom user-defined function (`speeding_udf`). \n",
    "\n",
    "---\n",
    "\n",
    "### Design Choice Justification\n",
    "\n",
    "- We decide to use a`\"10 minutes\"` tumbling window since we know the system sent data based on batch interval of 5 seconds, so we see this 10 minutes as sufficient enough to catch even a late batch to join, of course, there is a 24-hour watermark that can guarantee.\n",
    "- We include the drop (filtering) action directly in the join to remove irrelevant records early. This reduces data processing and memory use, improves performance by limiting intermediate data, and simplifies the overall pipeline, while logging can still be done inside the Spark Session.\n",
    "\n",
    "---\n",
    "\n",
    "For more information refer to:\n",
    "- [pySpark's Basic Operation Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-operations---selection-projection-aggregation)\n",
    "- [pySpark Stream-Stream Join Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#stream-stream-joins)\n",
    "- [pySpark Windowingt Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#types-of-time-windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48187e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join A & B\n",
    "abc_join = b.alias(\"b\").join(\n",
    "    #alias of entity camera a\n",
    "    a.alias(\"a\"),\n",
    "    #join where car plate is the same\n",
    "    (col(\"a.car_plate\") == col(\"b.car_plate\")) &\n",
    "    #join when the time crossing camera a < crossing camera b\n",
    "    (col(\"a.timestamp_a\") < col(\"b.timestamp_b\")) &\n",
    "    #the vehicle must past b after 10 min of camera a\n",
    "    (col(\"b.timestamp_b\") <= col(\"a.timestamp_a\") + expr(\"interval 10 minutes\")),\n",
    "    #this will drop anyone who don't meet the requirements\n",
    "    \"inner\"\n",
    ").select(\n",
    "    #Select the different columns\n",
    "    col(\"a.car_plate\"),\n",
    "    col(\"a.camera_id_a\"),\n",
    "    col(\"a.timestamp_a\"),\n",
    "    col(\"a.speed_reading_a\"),\n",
    "    col(\"a.speed_flag_instant_a\"),\n",
    "    #Create average of speed of camera 1 and b \n",
    "    ((col(\"a.speed_reading_a\") + col(\"b.speed_reading_b\")) / 2).alias(\"avg_speed_reading_ab\"),\n",
    "    #Flag the average speed we found using UDF implemented earlier\n",
    "    speeding_udf(\n",
    "        col(\"b.camera_id_b\"),\n",
    "        ((col(\"a.speed_reading_a\") + col(\"b.speed_reading_b\")) / 2),\n",
    "        lit(\"average\")\n",
    "    ).alias(\"speed_flag_average_ab\"),\n",
    "    col(\"b.camera_id_b\"),\n",
    "    col(\"b.timestamp_b\"),\n",
    "    col(\"b.speed_reading_b\"),\n",
    "    col(\"b.speed_flag_instant_b\")\n",
    ").alias(\"ab\").join(\n",
    "    #ali\n",
    "    c.alias(\"c\"),\n",
    "    #join where car plate is the same\n",
    "    (col(\"ab.car_plate\") == col(\"c.car_plate\")) &\n",
    "    #join when vehicle pass b then c\n",
    "    (col(\"c.timestamp_c\") > col(\"ab.timestamp_b\")) &\n",
    "    #the vehicle must past c after 10 min of camera b\n",
    "    (col(\"c.timestamp_c\") <= col(\"ab.timestamp_b\") + expr(\"interval 10 minutes\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"ab.*\"),\n",
    "    ((col(\"ab.speed_reading_b\") + col(\"c.speed_reading_c\")) / 2).alias(\"avg_speed_reading_bc\"),\n",
    "    speeding_udf(\n",
    "        col(\"c.camera_id_c\"),\n",
    "        ((col(\"ab.speed_reading_b\") + col(\"c.speed_reading_c\")) / 2),\n",
    "        lit(\"average\")\n",
    "    ).alias(\"speed_flag_average_bc\"),\n",
    "    col(\"c.camera_id_c\"),\n",
    "    col(\"c.timestamp_c\"),\n",
    "    col(\"c.speed_reading_c\"),\n",
    "    col(\"c.speed_flag_instant_c\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e627101",
   "metadata": {},
   "source": [
    "## Output Writing to MongoDB Violation Collection\n",
    "\n",
    "This section details the final stage of the streaming pipeline, where enriched and validated data is persisted to MongoDB for downstream usage, monitoring, and auditing.\n",
    "\n",
    "We have a schema we designed previously in task 1, which is really different with the output created by the final join we implemented.\n",
    "\n",
    "Join Final Result Schema:\n",
    "* `car_plate`: String\n",
    "* `camera_id_a`: String\n",
    "* `timestamp_a`: Timestamp\n",
    "* `speed_reading_a`: Float/Double\n",
    "* `speed_flag_instant_a`: Boolean\n",
    "* `avg_speed_reading_ab`: Float/Double\n",
    "* `speed_flag_average_ab`: Boolean\n",
    "* `camera_id_b`: String\n",
    "* `timestamp_b`: Timestamp\n",
    "* `speed_reading_b`: Float/Double\n",
    "* `speed_flag_instant_b`: Boolean\n",
    "* `avg_speed_reading_bc`: Float/Double\n",
    "* `speed_flag_average_bc`: Boolean\n",
    "* `camera_id_c`: String\n",
    "* `timestamp_c`: Timestamp\n",
    "* `speed_reading_c`: Float/Double\n",
    "* `speed_flag_instant_c`: Boolean\n",
    "\n",
    "MongoDB collection Violations Schema:\n",
    "```\n",
    "{\n",
    "    \"car_plate\": string,\n",
    "    \"date\": ISODate, // date at midnight UTC (or locally)\n",
    "    \"violations\": [\n",
    "        {\n",
    "            \"violation_id\": string,\n",
    "            \"type\": {\n",
    "                type: string,\n",
    "                enum: [\"instantaneous\",\"average\"],\n",
    "                required: true\n",
    "            },\n",
    "            \"camera_id_start\": int,\n",
    "            \"camera_id_end\": int, // will be null if instantenous\n",
    "            \"timestamp_start\": ISODate,\n",
    "            \"timestamp_end\": ISODate, // will be null if instantenous\n",
    "            \"measured_speed\": float,\n",
    "            \"speed_limit\": int\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "As you can see from here, we need to apply some transformation on the raw PySpark data by creating a date to indicate the day of the record (d/m/y) and a nested list of violations.\n",
    "\n",
    "---\n",
    "### Implementation Steps\n",
    "To do all of this, we implement different methods in this order:\n",
    "- we implemented a DBwriter that's compatible with pySpark's `forEach()`\n",
    "    - Connect to mongoDB server we initialized in `init()`\n",
    "    - implement an `open()` that's called by the `forEach()`\n",
    "    - Parse through timestmap_start for each streams (e.g. a, b, c) into a date format\n",
    "    - we create an if function to detect 5 violations (3 instant violations & 2 average violations) \n",
    "    - if violations found, update existing collection when document exist, if not, insert a new document.\n",
    "- To align it with pySpark's `forEachBatch()`, we need to wrap it with a function.\n",
    "- Implement a sinkWriter that trigger previous function we implemented:\n",
    "    - initialize the connection with a mongoFDB instance\n",
    "    - clear out checkpoints directory.\n",
    "    - `.format(\"console\")` to show any process and error in console\n",
    "    - `.outputMode(\"append\")`\n",
    "    - `.forEachbatch(func)` to trigger the dbWriter we wrapped with a function in batching fashion\n",
    "    - `.option(\"truncate\", False)` do not truncate the result in console.\n",
    "    - `.start()` to start the sink Writer.\n",
    "\n",
    "---\n",
    "\n",
    "### Design Choice Justification\n",
    "\n",
    "- MongoDB ensures the processed violation records are retained persistently, supporting post-processing tasks such as reporting or legal enforcement.\n",
    "- By using `forEachBatch()`, we enable a much more and structured sinking process where instead of making a connection in each results outputted, its done/batch.\n",
    "\n",
    "---\n",
    "\n",
    "For more information refer to:\n",
    "- [MongoDB Spark Connector Documentation](https://www.mongodb.com/docs/spark-connector/current/)\n",
    "- [pySpark forEach Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.foreach.html) and [pySpark forEachBatch Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.foreachBatch.html)\n",
    "- [Structured Streaming Output Sinks](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b29241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No violations detected for YRV 3 from 2024-01-01 08:08:05 to 2024-01-01 08:09:53.655566\n",
      "No violations detected for WX 49 from 2024-01-01 08:19:44 to 2024-01-01 08:21:14.975844\n",
      "No violations detected for FA 35 from 2024-01-01 08:00:03 to 2024-01-01 08:01:29.666333\n",
      "No violations detected for BQN 88 from 2024-01-01 08:19:48 to 2024-01-01 08:21:43.579488\n",
      "No violations detected for VC 45 from 2024-01-01 08:25:20 to 2024-01-01 08:26:48.172857\n",
      "No violations detected for QZ 81 from 2024-01-01 08:08:03 to 2024-01-01 08:09:58.988288\n",
      "No violations detected for KV 17 from 2024-01-01 08:08:05 to 2024-01-01 08:09:25.802177\n",
      "No violations detected for QJ 53 from 2024-01-01 08:08:02 to 2024-01-01 08:09:29.843998\n",
      "No violations detected for WPV 1456 from 2024-01-01 08:08:01 to 2024-01-01 08:09:32.626411\n",
      "No violations detected for QE 1820 from 2024-01-01 08:00:03 to 2024-01-01 08:01:50.367291\n",
      "No violations detected for KOB 22 from 2024-01-01 08:25:20 to 2024-01-01 08:27:02.016169\n",
      "No violations detected for AJB 6689 from 2024-01-01 08:08:05 to 2024-01-01 08:09:59.513428\n",
      "No violations detected for VLY 61 from 2024-01-01 08:19:45 to 2024-01-01 08:21:08.699219\n",
      "No violations detected for XYE 28 from 2024-01-01 08:19:45 to 2024-01-01 08:21:23.023195\n",
      "No violations detected for WOQ 4 from 2024-01-01 08:19:44 to 2024-01-01 08:21:30.700352\n",
      "No violations detected for VKZ 5377 from 2024-01-01 08:13:12 to 2024-01-01 08:14:43.211632\n",
      "No violations detected for GI 029 from 2024-01-01 08:13:10 to 2024-01-01 08:15:09.338907\n",
      "No violations detected for KRN 7 from 2024-01-01 08:00:04 to 2024-01-01 08:01:42.077427\n",
      "No violations detected for DJ 9 from 2024-01-01 08:13:10 to 2024-01-01 08:14:32.755618\n",
      "No violations detected for HLZ 1649 from 2024-01-01 08:19:47 to 2024-01-01 08:21:10.109678\n",
      "No violations detected for YY 765 from 2024-01-01 08:19:49 to 2024-01-01 08:21:24.142692\n",
      "No violations detected for BUB 647 from 2024-01-01 08:42:42 to 2024-01-01 08:44:40.862263\n",
      "No violations detected for DAW 165 from 2024-01-01 08:25:21 to 2024-01-01 08:26:55.452108\n",
      "No violations detected for ZZ 8 from 2024-01-01 08:13:08 to 2024-01-01 08:15:05.585777\n",
      "No violations detected for DBF 451 from 2024-01-01 08:42:41 to 2024-01-01 08:44:42.096650\n",
      "No violations detected for CU 131 from 2024-01-01 08:42:39 to 2024-01-01 08:44:05.714894\n",
      "No violations detected for BPV 0250 from 2024-01-01 08:42:41 to 2024-01-01 08:44:10.723828\n",
      "No violations detected for MC 2760 from 2024-01-01 08:42:41 to 2024-01-01 08:44:20.522451\n",
      "No violations detected for IU 033 from 2024-01-01 08:25:24 to 2024-01-01 08:26:55.462725\n",
      "No violations detected for XSG 781 from 2024-01-01 08:35:20 to 2024-01-01 08:37:03.846482\n",
      "No violations detected for HX 98 from 2024-01-01 08:35:20 to 2024-01-01 08:37:11.485230\n",
      "No violations detected for IXF 6187 from 2024-01-01 08:35:22 to 2024-01-01 08:36:47.888036\n",
      "No violations detected for ZHC 847 from 2024-01-01 08:42:42 to 2024-01-01 08:44:18.215971\n",
      "No violations detected for OH 2 from 2024-01-01 08:42:37 to 2024-01-01 08:44:14.805659\n",
      "No violations detected for CXD 617 from 2024-01-01 08:42:41 to 2024-01-01 08:44:17.828667\n",
      "No violations detected for YO 4 from 2024-01-01 08:51:29 to 2024-01-01 08:52:48.977003\n",
      "No violations detected for DFV 91 from 2024-01-01 08:51:29 to 2024-01-01 08:52:56.989335\n",
      "No violations detected for AL 006 from 2024-01-01 08:51:34 to 2024-01-01 08:53:03.393403\n",
      "No violations detected for CKI 86 from 2024-01-01 08:42:41 to 2024-01-01 08:44:29.706929\n",
      "No violations detected for UMZ 673 from 2024-01-01 08:42:41 to 2024-01-01 08:44:10.244219\n",
      "No violations detected for NE 205 from 2024-01-01 08:51:29 to 2024-01-01 08:53:12.905776\n",
      "No violations detected for AWP 66 from 2024-01-01 08:58:11 to 2024-01-01 08:59:52.082719\n",
      "No violations detected for WLD 5 from 2024-01-01 09:07:05 to 2024-01-01 09:08:38.877022\n",
      "No violations detected for WVU 913 from 2024-01-01 09:07:07 to 2024-01-01 09:08:28.374015\n",
      "No violations detected for NGE 1 from 2024-01-01 08:51:29 to 2024-01-01 08:53:08.190588\n",
      "No violations detected for NR 26 from 2024-01-01 08:51:30 to 2024-01-01 08:53:20.893719\n",
      "No violations detected for NO 7 from 2024-01-01 08:58:08 to 2024-01-01 08:59:29.800876\n",
      "No violations detected for SAR 94 from 2024-01-01 08:58:11 to 2024-01-01 08:59:37.206468\n",
      "No violations detected for VXK 69 from 2024-01-01 09:07:03 to 2024-01-01 09:08:23.278996\n",
      "No violations detected for KO 9 from 2024-01-01 08:58:10 to 2024-01-01 09:00:00.390452\n",
      "No violations detected for GK 2663 from 2024-01-01 09:07:04 to 2024-01-01 09:08:36.960660\n",
      "No violations detected for DQQ 13 from 2024-01-01 09:14:53 to 2024-01-01 09:16:46.256128\n",
      "No violations detected for SGH 2689 from 2024-01-01 08:58:09 to 2024-01-01 08:59:54.545729\n",
      "No violations detected for RJ 63 from 2024-01-01 09:24:48 to 2024-01-01 09:26:20.189722\n",
      "No violations detected for KO 4 from 2024-01-01 09:14:54 to 2024-01-01 09:16:43.438570\n",
      "No violations detected for TIF 93 from 2024-01-01 09:24:45 to 2024-01-01 09:26:33.729569\n",
      "No violations detected for WCI 74 from 2024-01-01 09:07:06 to 2024-01-01 09:08:32.057395\n",
      "No violations detected for DGP 8287 from 2024-01-01 09:07:07 to 2024-01-01 09:08:45.225344\n",
      "No violations detected for SB 571 from 2024-01-01 09:14:53 to 2024-01-01 09:16:25.048482\n",
      "No violations detected for SS 5 from 2024-01-01 09:24:50 to 2024-01-01 09:26:26.801682\n",
      "No violations detected for EF 950 from 2024-01-01 09:07:06 to 2024-01-01 09:09:00.191246\n",
      "No violations detected for HPY 8 from 2024-01-01 09:24:47 to 2024-01-01 09:26:27.124820\n",
      "No violations detected for ZE 26 from 2024-01-01 09:14:49 to 2024-01-01 09:16:21.187853\n",
      "No violations detected for UZ 167 from 2024-01-01 09:31:22 to 2024-01-01 09:33:07.321941\n",
      "No violations detected for VD 65 from 2024-01-01 09:14:52 to 2024-01-01 09:16:34.099239\n",
      "No violations detected for ZB 19 from 2024-01-01 09:07:03 to 2024-01-01 09:08:32.425138\n",
      "No violations detected for HM 258 from 2024-01-01 09:31:22 to 2024-01-01 09:33:22.173081\n",
      "No violations detected for NT 0 from 2024-01-01 09:44:42 to 2024-01-01 09:46:36.961380\n",
      "No violations detected for EY 3628 from 2024-01-01 09:38:30 to 2024-01-01 09:40:23.527766\n",
      "No violations detected for FS 94 from 2024-01-01 09:14:49 to 2024-01-01 09:16:24.573844\n",
      "No violations detected for ARN 23 from 2024-01-01 09:31:19 to 2024-01-01 09:32:54.597564\n",
      "No violations detected for CN 85 from 2024-01-01 09:38:25 to 2024-01-01 09:39:57.558032\n",
      "No violations detected for URT 4237 from 2024-01-01 09:24:48 to 2024-01-01 09:26:31.692370\n",
      "No violations detected for HZ 826 from 2024-01-01 09:31:21 to 2024-01-01 09:33:06.777382\n",
      "No violations detected for KQ 470 from 2024-01-01 09:31:18 to 2024-01-01 09:32:46.363078\n",
      "No violations detected for QK 8143 from 2024-01-01 09:51:13 to 2024-01-01 09:52:47.522040\n",
      "No violations detected for WTD 8 from 2024-01-01 09:38:29 to 2024-01-01 09:40:09.424122\n",
      "No violations detected for ZNX 49 from 2024-01-01 09:38:25 to 2024-01-01 09:39:58.062193\n",
      "No violations detected for XO 462 from 2024-01-01 09:44:44 to 2024-01-01 09:46:22.637843\n",
      "No violations detected for ZCW 6 from 2024-01-01 10:09:43 to 2024-01-01 10:11:27.011161\n",
      "No violations detected for WLO 61 from 2024-01-01 09:51:10 to 2024-01-01 09:52:46.839998\n",
      "No violations detected for WBC 7 from 2024-01-01 09:44:44 to 2024-01-01 09:46:14.413893\n",
      "No violations detected for IP 49 from 2024-01-01 09:44:47 to 2024-01-01 09:46:17.609828\n",
      "No violations detected for SY 2 from 2024-01-01 09:44:45 to 2024-01-01 09:46:29.444589\n",
      "No violations detected for GC 2241 from 2024-01-01 09:38:29 to 2024-01-01 09:40:02.887226\n",
      "No violations detected for MU 6092 from 2024-01-01 09:59:46 to 2024-01-01 10:01:18.546726\n",
      "No violations detected for GX 956 from 2024-01-01 09:51:14 to 2024-01-01 09:52:38.003514\n",
      "No violations detected for HW 2 from 2024-01-01 09:44:45 to 2024-01-01 09:46:10.658384\n",
      "No violations detected for RZH 63 from 2024-01-01 09:59:47 to 2024-01-01 10:01:37.269663\n",
      "No violations detected for IPJ 010 from 2024-01-01 09:59:45 to 2024-01-01 10:01:18.995556\n",
      "No violations detected for WH 2992 from 2024-01-01 09:51:15 to 2024-01-01 09:52:36.228596\n",
      "No violations detected for YP 93 from 2024-01-01 09:59:48 to 2024-01-01 10:01:14.950644\n",
      "No violations detected for WU 24 from 2024-01-01 10:19:10 to 2024-01-01 10:20:34.554968\n",
      "No violations detected for DBV 2 from 2024-01-01 09:59:48 to 2024-01-01 10:01:34.844811\n",
      "No violations detected for KMA 15 from 2024-01-01 10:09:45 to 2024-01-01 10:11:07.028699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No violations detected for XOL 399 from 2024-01-01 10:19:10 to 2024-01-01 10:20:30.127475\n",
      "No violations detected for YM 2344 from 2024-01-01 10:09:44 to 2024-01-01 10:11:04.471446\n",
      "No violations detected for TU 8737 from 2024-01-01 09:59:44 to 2024-01-01 10:01:22.583131\n",
      "No violations detected for TN 6 from 2024-01-01 10:19:13 to 2024-01-01 10:20:35.276910\n",
      "No violations detected for AYD 39 from 2024-01-01 10:36:19 to 2024-01-01 10:37:59.597383\n",
      "No violations detected for RM 4259 from 2024-01-01 10:19:11 to 2024-01-01 10:21:02.035144\n",
      "No violations detected for YXX 41 from 2024-01-01 10:36:16 to 2024-01-01 10:37:46.529102\n",
      "No violations detected for WVA 683 from 2024-01-01 10:27:28 to 2024-01-01 10:29:08.404093\n",
      "No violations detected for AE 51 from 2024-01-01 10:09:44 to 2024-01-01 10:11:33.910247\n",
      "No violations detected for HY 5839 from 2024-01-01 10:36:18 to 2024-01-01 10:37:39.174796\n",
      "No violations detected for PAV 948 from 2024-01-01 10:09:44 to 2024-01-01 10:11:33.889015\n",
      "No violations detected for DU 62 from 2024-01-01 10:27:27 to 2024-01-01 10:29:00.052847\n",
      "No violations detected for ZMB 31 from 2024-01-01 10:36:16 to 2024-01-01 10:37:46.710860\n",
      "No violations detected for XA 937 from 2024-01-01 10:36:18 to 2024-01-01 10:38:21.134849\n",
      "No violations detected for OUV 7317 from 2024-01-01 10:19:10 to 2024-01-01 10:20:38.850869\n",
      "No violations detected for FX 031 from 2024-01-01 10:27:28 to 2024-01-01 10:29:25.887590\n",
      "No violations detected for WO 5201 from 2024-01-01 10:50:15 to 2024-01-01 10:51:45.049822\n",
      "No violations detected for UH 650 from 2024-01-01 10:27:27 to 2024-01-01 10:29:27.554274\n",
      "No violations detected for VY 1120 from 2024-01-01 10:36:15 to 2024-01-01 10:37:57.230386\n",
      "No violations detected for AP 124 from 2024-01-01 10:50:15 to 2024-01-01 10:51:40.050276\n",
      "No violations detected for VV 7408 from 2024-01-01 10:36:15 to 2024-01-01 10:37:41.489574\n",
      "No violations detected for PQP 642 from 2024-01-01 10:43:34 to 2024-01-01 10:45:31.444118\n",
      "No violations detected for OPO 58 from 2024-01-01 10:50:16 to 2024-01-01 10:51:50.465847\n",
      "No violations detected for QF 7428 from 2024-01-01 10:58:36 to 2024-01-01 11:00:13.600657\n",
      "No violations detected for KCQ 605 from 2024-01-01 10:43:35 to 2024-01-01 10:45:09.421537\n",
      "No violations detected for NWJ 9 from 2024-01-01 10:43:31 to 2024-01-01 10:44:53.593878\n",
      "No violations detected for RE 6923 from 2024-01-01 10:50:14 to 2024-01-01 10:51:38.317064\n",
      "No violations detected for SS 78 from 2024-01-01 10:58:33 to 2024-01-01 11:00:00.525587\n",
      "No violations detected for QH 4901 from 2024-01-01 10:43:35 to 2024-01-01 10:45:08.341723\n",
      "No violations detected for XMI 6882 from 2024-01-01 11:06:29 to 2024-01-01 11:07:52.267203\n",
      "No violations detected for IN 3520 from 2024-01-01 10:50:16 to 2024-01-01 10:51:56.334615\n",
      "No violations detected for CD 874 from 2024-01-01 10:50:16 to 2024-01-01 10:52:08.034586\n",
      "No violations detected for VQ 75 from 2024-01-01 10:58:36 to 2024-01-01 11:00:10.894044\n",
      "No violations detected for JS 9 from 2024-01-01 11:13:12 to 2024-01-01 11:14:50.247463\n",
      "No violations detected for DJL 1 from 2024-01-01 11:06:25 to 2024-01-01 11:08:08.670253\n",
      "No violations detected for QSE 4079 from 2024-01-01 11:13:09 to 2024-01-01 11:14:36.418577\n",
      "No violations detected for CLJ 52 from 2024-01-01 10:58:34 to 2024-01-01 11:00:24.930585\n",
      "No violations detected for QX 369 from 2024-01-01 11:13:10 to 2024-01-01 11:14:51.694788\n",
      "No violations detected for JEK 5890 from 2024-01-01 11:13:08 to 2024-01-01 11:15:00.028808\n",
      "No violations detected for PCL 47 from 2024-01-01 11:21:47 to 2024-01-01 11:23:15.713844\n",
      "No violations detected for EFW 664 from 2024-01-01 11:21:50 to 2024-01-01 11:23:44.249002\n",
      "No violations detected for QCZ 24 from 2024-01-01 11:21:46 to 2024-01-01 11:23:19.339211\n",
      "No violations detected for COX 4 from 2024-01-01 11:13:10 to 2024-01-01 11:14:37.440204\n",
      "No violations detected for DR 6 from 2024-01-01 11:06:27 to 2024-01-01 11:07:59.177366\n",
      "No violations detected for ZI 72 from 2024-01-01 11:21:50 to 2024-01-01 11:23:39.353370\n",
      "No violations detected for ZC 146 from 2024-01-01 11:06:29 to 2024-01-01 11:08:08.446826\n",
      "No violations detected for NKM 8 from 2024-01-01 11:06:29 to 2024-01-01 11:07:58.301812\n",
      "No violations detected for ZZI 6 from 2024-01-01 11:06:30 to 2024-01-01 11:08:29.896830\n",
      "No violations detected for DHW 01 from 2024-01-01 11:21:47 to 2024-01-01 11:23:16.903354\n",
      "No violations detected for RT 3 from 2024-01-01 11:13:09 to 2024-01-01 11:14:49.042629\n",
      "No violations detected for DQ 346 from 2024-01-01 11:21:49 to 2024-01-01 11:23:12.762112\n",
      "No violations detected for ZKP 5064 from 2024-01-01 11:13:07 to 2024-01-01 11:14:27.934755\n",
      "No violations detected for HE 9767 from 2024-01-01 11:21:49 to 2024-01-01 11:23:20.967359\n",
      "No violations detected for XJK 5882 from 2024-01-01 11:21:51 to 2024-01-01 11:23:08.917142\n",
      "No violations detected for WRY 7156 from 2024-01-01 11:21:51 to 2024-01-01 11:23:43.853132\n",
      "No violations detected for XYF 6209 from 2024-01-01 11:27:42 to 2024-01-01 11:29:02.280237\n",
      "No violations detected for WKL 349 from 2024-01-01 11:40:01 to 2024-01-01 11:41:57.584105\n",
      "No violations detected for CB 0 from 2024-01-01 11:34:08 to 2024-01-01 11:35:34.445463\n",
      "No violations detected for DQG 6317 from 2024-01-01 11:34:11 to 2024-01-01 11:35:52.506514\n",
      "No violations detected for QQ 64 from 2024-01-01 11:34:08 to 2024-01-01 11:35:43.469418\n",
      "No violations detected for RFQ 34 from 2024-01-01 11:34:13 to 2024-01-01 11:35:37.815018\n",
      "No violations detected for AL 329 from 2024-01-01 11:21:50 to 2024-01-01 11:23:35.524997\n",
      "No violations detected for MGF 9 from 2024-01-01 11:21:51 to 2024-01-01 11:23:42.714078\n",
      "No violations detected for NTA 92 from 2024-01-01 11:27:44 to 2024-01-01 11:29:25.731214\n",
      "No violations detected for AC 372 from 2024-01-01 11:34:12 to 2024-01-01 11:35:31.958080\n",
      "No violations detected for DN 7151 from 2024-01-01 11:27:42 to 2024-01-01 11:29:26.316050\n",
      "No violations detected for VDL 6 from 2024-01-01 11:40:04 to 2024-01-01 11:41:23.369586\n",
      "No violations detected for BDT 257 from 2024-01-01 11:34:11 to 2024-01-01 11:36:01.689861\n",
      "No violations detected for NJ 51 from 2024-01-01 11:34:12 to 2024-01-01 11:36:04.548811\n",
      "No violations detected for TM 153 from 2024-01-01 11:39:59 to 2024-01-01 11:41:21.632547\n",
      "No violations detected for WM 7013 from 2024-01-01 11:27:42 to 2024-01-01 11:28:58.003520\n",
      "No violations detected for AZH 16 from 2024-01-01 11:40:04 to 2024-01-01 11:41:33.872967\n",
      "No violations detected for OSP 455 from 2024-01-01 11:49:43 to 2024-01-01 11:51:22.167755\n",
      "No violations detected for UUK 911 from 2024-01-01 11:40:00 to 2024-01-01 11:41:54.009380\n",
      "No violations detected for BN 1719 from 2024-01-01 11:40:01 to 2024-01-01 11:41:25.137464\n",
      "No violations detected for SR 5 from 2024-01-01 11:57:48 to 2024-01-01 11:59:41.832711\n",
      "No violations detected for JIV 489 from 2024-01-01 11:57:52 to 2024-01-01 11:59:20.283714\n",
      "No violations detected for TAU 52 from 2024-01-01 11:40:02 to 2024-01-01 11:41:36.263218\n",
      "No violations detected for AK 11 from 2024-01-01 11:57:50 to 2024-01-01 11:59:23.067825\n",
      "No violations detected for QET 1940 from 2024-01-01 11:57:53 to 2024-01-01 11:59:47.942415\n",
      "No violations detected for WM 20 from 2024-01-01 12:04:41 to 2024-01-01 12:06:21.241369\n",
      "No violations detected for CPF 11 from 2024-01-01 11:49:43 to 2024-01-01 11:51:32.880235\n",
      "No violations detected for IJ 321 from 2024-01-01 11:57:52 to 2024-01-01 11:59:24.251366\n",
      "No violations detected for XIM 470 from 2024-01-01 11:40:00 to 2024-01-01 11:41:55.176166\n",
      "No violations detected for AT 323 from 2024-01-01 12:04:41 to 2024-01-01 12:06:10.155564\n",
      "No violations detected for UT 1645 from 2024-01-01 11:57:53 to 2024-01-01 11:59:24.484888\n",
      "No violations detected for KA 6281 from 2024-01-01 11:57:50 to 2024-01-01 11:59:35.084166\n",
      "No violations detected for QN 296 from 2024-01-01 11:57:48 to 2024-01-01 11:59:31.999238\n",
      "No violations detected for GG 215 from 2024-01-01 12:04:38 to 2024-01-01 12:05:59.299581\n",
      "No violations detected for QK 91 from 2024-01-01 12:04:41 to 2024-01-01 12:06:15.359379\n",
      "No violations detected for GTA 009 from 2024-01-01 12:04:39 to 2024-01-01 12:06:33.792788\n",
      "No violations detected for BV 909 from 2024-01-01 11:57:51 to 2024-01-01 11:59:19.912448\n",
      "No violations detected for FW 15 from 2024-01-01 12:19:50 to 2024-01-01 12:21:25.591081\n",
      "No violations detected for SJ 69 from 2024-01-01 12:04:41 to 2024-01-01 12:06:38.448378\n",
      "No violations detected for TJ 4 from 2024-01-01 12:10:14 to 2024-01-01 12:12:02.884916\n",
      "No violations detected for MKA 437 from 2024-01-01 12:04:41 to 2024-01-01 12:06:17.803177\n",
      "No violations detected for BU 690 from 2024-01-01 12:10:12 to 2024-01-01 12:11:39.874546\n",
      "No violations detected for DYA 819 from 2024-01-01 12:04:42 to 2024-01-01 12:06:29.752819\n",
      "No violations detected for GU 6259 from 2024-01-01 12:10:12 to 2024-01-01 12:11:48.214957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No violations detected for MW 3929 from 2024-01-01 12:19:49 to 2024-01-01 12:21:24.185263\n",
      "No violations detected for HX 4 from 2024-01-01 12:10:12 to 2024-01-01 12:11:49.058433\n",
      "No violations detected for TI 673 from 2024-01-01 12:19:53 to 2024-01-01 12:21:39.701158\n",
      "No violations detected for GZA 639 from 2024-01-01 12:19:52 to 2024-01-01 12:21:28.190315\n",
      "No violations detected for DLU 67 from 2024-01-01 12:19:54 to 2024-01-01 12:21:50.565183\n",
      "No violations detected for KM 4 from 2024-01-01 12:10:09 to 2024-01-01 12:11:50.458632\n",
      "No violations detected for EEF 29 from 2024-01-01 12:19:50 to 2024-01-01 12:21:32.916014\n",
      "No violations detected for HSY 0040 from 2024-01-01 12:10:09 to 2024-01-01 12:11:59.725994\n",
      "No violations detected for RK 1342 from 2024-01-01 12:19:52 to 2024-01-01 12:21:23.952155\n",
      "No violations detected for NZ 097 from 2024-01-01 12:10:10 to 2024-01-01 12:12:04.075952\n",
      "No violations detected for YW 1420 from 2024-01-01 12:26:35 to 2024-01-01 12:28:02.918185\n",
      "No violations detected for OZC 8603 from 2024-01-01 12:32:17 to 2024-01-01 12:34:00.501531\n",
      "No violations detected for HU 7 from 2024-01-01 12:26:33 to 2024-01-01 12:28:02.783027\n",
      "No violations detected for RI 39 from 2024-01-01 12:32:18 to 2024-01-01 12:34:16.440505\n",
      "No violations detected for EXF 9 from 2024-01-01 12:32:17 to 2024-01-01 12:33:39.986905\n",
      "No violations detected for NL 841 from 2024-01-01 12:32:19 to 2024-01-01 12:33:53.838839\n",
      "No violations detected for UNS 84 from 2024-01-01 12:47:02 to 2024-01-01 12:48:39.822456\n",
      "No violations detected for RR 618 from 2024-01-01 12:32:20 to 2024-01-01 12:33:48.155131\n",
      "No violations detected for NKF 7638 from 2024-01-01 12:47:02 to 2024-01-01 12:48:59.485315\n",
      "No violations detected for GW 0 from 2024-01-01 12:55:48 to 2024-01-01 12:57:25.909539\n",
      "No violations detected for NG 1269 from 2024-01-01 12:39:46 to 2024-01-01 12:41:08.625361\n",
      "No violations detected for HU 54 from 2024-01-01 12:39:44 to 2024-01-01 12:41:18.678256\n",
      "No violations detected for BGS 6588 from 2024-01-01 12:39:42 to 2024-01-01 12:41:00.002999\n",
      "No violations detected for HZC 387 from 2024-01-01 12:55:50 to 2024-01-01 12:57:31.354134\n",
      "No violations detected for KQE 06 from 2024-01-01 12:55:47 to 2024-01-01 12:57:32.238601\n",
      "No violations detected for UOF 49 from 2024-01-01 12:55:49 to 2024-01-01 12:57:42.423115\n",
      "No violations detected for VW 06 from 2024-01-01 13:02:57 to 2024-01-01 13:04:21.052763\n",
      "No violations detected for WGL 51 from 2024-01-01 12:47:02 to 2024-01-01 12:48:28.149006\n",
      "No violations detected for SE 553 from 2024-01-01 12:47:03 to 2024-01-01 12:48:42.561676\n",
      "No violations detected for HOW 201 from 2024-01-01 13:02:58 to 2024-01-01 13:04:26.059191\n",
      "No violations detected for VO 8622 from 2024-01-01 12:47:02 to 2024-01-01 12:48:28.628009\n",
      "No violations detected for HD 489 from 2024-01-01 13:02:57 to 2024-01-01 13:04:42.888512\n",
      "No violations detected for DBX 5 from 2024-01-01 13:02:58 to 2024-01-01 13:04:47.648475\n",
      "No violations detected for BZL 84 from 2024-01-01 12:55:50 to 2024-01-01 12:57:18.071269\n",
      "No violations detected for JQF 5781 from 2024-01-01 12:55:49 to 2024-01-01 12:57:20.904684\n",
      "No violations detected for HI 894 from 2024-01-01 13:03:00 to 2024-01-01 13:04:56.205814\n",
      "No violations detected for DIC 7 from 2024-01-01 13:09:09 to 2024-01-01 13:10:28.465842\n",
      "No violations detected for UU 4 from 2024-01-01 13:03:01 to 2024-01-01 13:04:30.344126\n",
      "No violations detected for ARK 5069 from 2024-01-01 13:09:09 to 2024-01-01 13:10:43.862684\n",
      "No violations detected for XST 3 from 2024-01-01 13:18:25 to 2024-01-01 13:19:49.002625\n",
      "No violations detected for JMU 8687 from 2024-01-01 13:09:11 to 2024-01-01 13:10:57.047241\n",
      "No violations detected for CWZ 06 from 2024-01-01 13:09:09 to 2024-01-01 13:10:36.962525\n",
      "No violations detected for GDM 114 from 2024-01-01 12:55:47 to 2024-01-01 12:57:21.663868\n",
      "No violations detected for ML 6630 from 2024-01-01 13:09:10 to 2024-01-01 13:10:39.078757\n",
      "No violations detected for PNC 9597 from 2024-01-01 13:18:26 to 2024-01-01 13:19:50.698770\n",
      "No violations detected for UZ 3167 from 2024-01-01 13:18:21 to 2024-01-01 13:20:02.672631\n",
      "No violations detected for MZ 85 from 2024-01-01 13:33:18 to 2024-01-01 13:34:41.281597\n",
      "No violations detected for NW 5280 from 2024-01-01 13:33:16 to 2024-01-01 13:35:03.903393\n",
      "No violations detected for OXT 873 from 2024-01-01 13:33:18 to 2024-01-01 13:34:55.903782\n",
      "No violations detected for UBY 6240 from 2024-01-01 13:33:15 to 2024-01-01 13:35:13.105201\n",
      "No violations detected for JOA 4 from 2024-01-01 13:39:49 to 2024-01-01 13:41:08.215055\n",
      "No violations detected for UU 1536 from 2024-01-01 13:18:24 to 2024-01-01 13:19:56.520889\n",
      "No violations detected for TL 523 from 2024-01-01 13:26:44 to 2024-01-01 13:28:25.217271\n",
      "No violations detected for EZC 553 from 2024-01-01 13:18:23 to 2024-01-01 13:20:14.960958\n",
      "No violations detected for VXG 6451 from 2024-01-01 13:26:41 to 2024-01-01 13:28:03.398253\n",
      "No violations detected for FU 46 from 2024-01-01 13:26:46 to 2024-01-01 13:28:42.595722\n",
      "No violations detected for DH 620 from 2024-01-01 13:33:15 to 2024-01-01 13:34:41.508924\n",
      "No violations detected for ZER 994 from 2024-01-01 13:26:45 to 2024-01-01 13:28:11.584972\n",
      "No violations detected for TV 1291 from 2024-01-01 13:33:18 to 2024-01-01 13:34:49.776737\n",
      "No violations detected for GH 7 from 2024-01-01 13:33:14 to 2024-01-01 13:34:45.159956\n",
      "No violations detected for WD 1620 from 2024-01-01 13:26:44 to 2024-01-01 13:28:43.092360\n",
      "No violations detected for UM 5589 from 2024-01-01 13:39:49 to 2024-01-01 13:41:35.640880\n",
      "No violations detected for TO 9 from 2024-01-01 13:33:16 to 2024-01-01 13:35:11.419845\n",
      "No violations detected for BVX 7 from 2024-01-01 13:33:16 to 2024-01-01 13:34:51.666448\n",
      "No violations detected for YYU 9 from 2024-01-01 13:39:47 to 2024-01-01 13:41:30.443466\n",
      "No violations detected for AW 4335 from 2024-01-01 13:45:57 to 2024-01-01 13:47:19.492200\n",
      "No violations detected for OMY 58 from 2024-01-01 13:46:00 to 2024-01-01 13:47:50.750139\n",
      "No violations detected for FLA 480 from 2024-01-01 13:39:51 to 2024-01-01 13:41:36.428794\n",
      "No violations detected for YQI 6899 from 2024-01-01 13:53:05 to 2024-01-01 13:55:01.225869\n",
      "No violations detected for NQ 30 from 2024-01-01 13:53:07 to 2024-01-01 13:54:28.316034\n",
      "No violations detected for QWU 7 from 2024-01-01 13:39:52 to 2024-01-01 13:41:36.042945\n",
      "No violations detected for VL 99 from 2024-01-01 13:45:59 to 2024-01-01 13:47:34.559509\n",
      "No violations detected for NG 712 from 2024-01-01 13:46:01 to 2024-01-01 13:47:43.974477\n",
      "No violations detected for RTS 6189 from 2024-01-01 13:45:56 to 2024-01-01 13:47:23.895302\n",
      "No violations detected for AYM 093 from 2024-01-01 14:02:55 to 2024-01-01 14:04:19.238812\n",
      "No violations detected for NR 5791 from 2024-01-01 13:53:06 to 2024-01-01 13:54:53.278592\n",
      "No violations detected for ND 66 from 2024-01-01 13:46:00 to 2024-01-01 13:47:41.030511\n",
      "No violations detected for KP 1428 from 2024-01-01 14:03:00 to 2024-01-01 14:04:43.744655\n",
      "No violations detected for CA 6 from 2024-01-01 13:53:03 to 2024-01-01 13:54:36.527314\n",
      "No violations detected for XJL 42 from 2024-01-01 13:53:07 to 2024-01-01 13:54:33.191829\n",
      "No violations detected for FT 4 from 2024-01-01 13:53:03 to 2024-01-01 13:54:25.132709\n",
      "No violations detected for ZJE 97 from 2024-01-01 13:53:04 to 2024-01-01 13:54:31.685889\n",
      "No violations detected for QI 035 from 2024-01-01 14:03:00 to 2024-01-01 14:04:28.945070\n",
      "No violations detected for BI 8588 from 2024-01-01 14:09:43 to 2024-01-01 14:11:10.900735\n",
      "No violations detected for HX 861 from 2024-01-01 14:09:39 to 2024-01-01 14:11:09.583764\n",
      "No violations detected for MRS 2523 from 2024-01-01 14:19:41 to 2024-01-01 14:21:03.678530\n",
      "No violations detected for NJ 27 from 2024-01-01 14:02:57 to 2024-01-01 14:04:59.599872\n",
      "No violations detected for HN 52 from 2024-01-01 13:53:05 to 2024-01-01 13:54:45.872880\n",
      "No violations detected for QA 9 from 2024-01-01 14:09:42 to 2024-01-01 14:11:04.424657\n",
      "No violations detected for SGA 48 from 2024-01-01 14:09:38 to 2024-01-01 14:11:00.789061\n",
      "No violations detected for RJ 09 from 2024-01-01 14:19:43 to 2024-01-01 14:21:45.778921\n",
      "No violations detected for UY 29 from 2024-01-01 14:19:44 to 2024-01-01 14:21:21.075870\n",
      "No violations detected for GM 003 from 2024-01-01 14:19:42 to 2024-01-01 14:21:09.298757\n",
      "No violations detected for PTI 35 from 2024-01-01 14:09:38 to 2024-01-01 14:11:28.595687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No violations detected for XM 6920 from 2024-01-01 14:38:35 to 2024-01-01 14:40:12.315185\n",
      "No violations detected for UM 812 from 2024-01-01 14:09:41 to 2024-01-01 14:11:11.995504\n",
      "No violations detected for HAQ 0 from 2024-01-01 14:19:44 to 2024-01-01 14:21:11.030426\n",
      "No violations detected for AW 8 from 2024-01-01 14:19:45 to 2024-01-01 14:21:11.326330\n",
      "No violations detected for RO 165 from 2024-01-01 14:44:33 to 2024-01-01 14:46:07.552900\n",
      "No violations detected for QD 2319 from 2024-01-01 14:29:13 to 2024-01-01 14:30:39.450445\n",
      "No violations detected for CBV 1 from 2024-01-01 14:38:35 to 2024-01-01 14:40:02.395509\n",
      "No violations detected for QSR 162 from 2024-01-01 14:38:36 to 2024-01-01 14:40:02.593771\n",
      "No violations detected for KC 02 from 2024-01-01 14:51:31 to 2024-01-01 14:53:26.001901\n",
      "No violations detected for PPR 83 from 2024-01-01 14:38:36 to 2024-01-01 14:40:25.009733\n",
      "No violations detected for FF 6109 from 2024-01-01 14:29:16 to 2024-01-01 14:30:54.056560\n",
      "No violations detected for ABY 4 from 2024-01-01 14:44:32 to 2024-01-01 14:46:18.893874\n",
      "No violations detected for VVK 7 from 2024-01-01 14:38:35 to 2024-01-01 14:40:08.988132\n",
      "No violations detected for QT 4100 from 2024-01-01 14:38:39 to 2024-01-01 14:40:24.282228\n",
      "No violations detected for HNP 810 from 2024-01-01 14:44:29 to 2024-01-01 14:46:04.085216\n",
      "No violations detected for ZTK 429 from 2024-01-01 14:38:39 to 2024-01-01 14:40:21.255985\n",
      "No violations detected for TO 6 from 2024-01-01 14:44:29 to 2024-01-01 14:45:50.944614\n",
      "No violations detected for WC 261 from 2024-01-01 14:51:29 to 2024-01-01 14:53:11.343387\n",
      "No violations detected for QI 469 from 2024-01-01 14:51:30 to 2024-01-01 14:52:49.258331\n",
      "No violations detected for AFW 912 from 2024-01-01 14:44:29 to 2024-01-01 14:45:50.151806\n",
      "No violations detected for TTU 82 from 2024-01-01 14:44:31 to 2024-01-01 14:45:50.938748\n",
      "No violations detected for NR 354 from 2024-01-01 14:44:33 to 2024-01-01 14:46:29.304762\n",
      "No violations detected for XWR 85 from 2024-01-01 14:51:32 to 2024-01-01 14:53:05.526039\n",
      "No violations detected for QBE 4 from 2024-01-01 14:44:32 to 2024-01-01 14:46:00.452367\n",
      "No violations detected for TY 61 from 2024-01-01 14:44:29 to 2024-01-01 14:46:06.777103\n",
      "No violations detected for WF 7892 from 2024-01-01 14:44:32 to 2024-01-01 14:45:57.478384\n",
      "No violations detected for DVN 6 from 2024-01-01 14:51:29 to 2024-01-01 14:52:50.679891\n",
      "No violations detected for BZ 646 from 2024-01-01 14:59:50 to 2024-01-01 15:01:17.331895\n",
      "No violations detected for NE 6 from 2024-01-01 14:51:32 to 2024-01-01 14:53:04.550830\n",
      "No violations detected for DJU 2435 from 2024-01-01 14:59:53 to 2024-01-01 15:01:24.486973\n",
      "No violations detected for TS 615 from 2024-01-01 14:51:29 to 2024-01-01 14:52:56.919501\n",
      "No violations detected for ZNU 7618 from 2024-01-01 14:51:34 to 2024-01-01 14:53:15.690846\n",
      "No violations detected for AS 2 from 2024-01-01 14:51:32 to 2024-01-01 14:53:23.903091\n",
      "No violations detected for WT 621 from 2024-01-01 15:05:02 to 2024-01-01 15:06:27.092025\n",
      "No violations detected for EFP 6705 from 2024-01-01 14:59:49 to 2024-01-01 15:01:33.863788\n",
      "No violations detected for QD 92 from 2024-01-01 14:59:48 to 2024-01-01 15:01:41.186624\n",
      "No violations detected for KG 1609 from 2024-01-01 14:59:49 to 2024-01-01 15:01:27.146690\n",
      "No violations detected for PII 3 from 2024-01-01 15:04:58 to 2024-01-01 15:06:25.728506\n",
      "No violations detected for AJN 5 from 2024-01-01 14:59:48 to 2024-01-01 15:01:41.220487\n",
      "No violations detected for HGU 3 from 2024-01-01 15:20:24 to 2024-01-01 15:22:18.843558\n",
      "No violations detected for DPN 333 from 2024-01-01 14:59:48 to 2024-01-01 15:01:08.555909\n",
      "No violations detected for DSH 0 from 2024-01-01 15:05:03 to 2024-01-01 15:06:56.244445\n",
      "No violations detected for DTX 510 from 2024-01-01 15:14:42 to 2024-01-01 15:16:40.900389\n",
      "No violations detected for QU 03 from 2024-01-01 15:14:41 to 2024-01-01 15:16:12.432896\n",
      "No violations detected for DQS 7420 from 2024-01-01 15:14:41 to 2024-01-01 15:16:10.648031\n",
      "No violations detected for YR 25 from 2024-01-01 15:20:23 to 2024-01-01 15:22:04.919849\n",
      "No violations detected for BV 2 from 2024-01-01 15:14:39 to 2024-01-01 15:16:13.626103\n",
      "No violations detected for CYQ 410 from 2024-01-01 15:14:37 to 2024-01-01 15:16:07.645771\n",
      "No violations detected for ZX 3257 from 2024-01-01 15:14:42 to 2024-01-01 15:16:25.429581\n",
      "No violations detected for WMW 9635 from 2024-01-01 15:14:40 to 2024-01-01 15:16:25.327894\n",
      "No violations detected for JU 348 from 2024-01-01 15:20:24 to 2024-01-01 15:21:57.924579\n",
      "No violations detected for KWF 4945 from 2024-01-01 15:26:14 to 2024-01-01 15:27:45.668216\n",
      "No violations detected for CU 7148 from 2024-01-01 15:20:23 to 2024-01-01 15:21:56.683736\n",
      "No violations detected for CDK 37 from 2024-01-01 15:26:13 to 2024-01-01 15:27:52.668679\n",
      "No violations detected for NJB 1278 from 2024-01-01 15:20:23 to 2024-01-01 15:22:02.910457\n",
      "No violations detected for SI 2 from 2024-01-01 15:20:24 to 2024-01-01 15:21:58.078511\n",
      "No violations detected for SD 341 from 2024-01-01 15:20:25 to 2024-01-01 15:22:12.435624\n",
      "No violations detected for UI 3 from 2024-01-01 15:20:20 to 2024-01-01 15:21:47.845111\n",
      "No violations detected for YNS 868 from 2024-01-01 15:35:41 to 2024-01-01 15:37:27.792841\n",
      "No violations detected for WI 9 from 2024-01-01 15:26:14 to 2024-01-01 15:27:35.055587\n",
      "No violations detected for YL 7 from 2024-01-01 15:35:39 to 2024-01-01 15:37:19.389291\n",
      "No violations detected for RE 0576 from 2024-01-01 15:35:41 to 2024-01-01 15:37:05.995128\n",
      "No violations detected for VM 06 from 2024-01-01 15:26:15 to 2024-01-01 15:27:39.399522\n",
      "No violations detected for QJ 495 from 2024-01-01 15:26:17 to 2024-01-01 15:28:10.305117\n",
      "No violations detected for DAK 5 from 2024-01-01 15:26:17 to 2024-01-01 15:28:07.565598\n",
      "No violations detected for JT 357 from 2024-01-01 15:42:36 to 2024-01-01 15:44:32.677934\n",
      "No violations detected for BOD 5962 from 2024-01-01 15:42:36 to 2024-01-01 15:44:09.167889\n",
      "No violations detected for ASW 042 from 2024-01-01 15:35:41 to 2024-01-01 15:37:41.093899\n",
      "No violations detected for PZZ 744 from 2024-01-01 15:35:43 to 2024-01-01 15:37:13.196120\n",
      "No violations detected for NWG 9845 from 2024-01-01 15:47:41 to 2024-01-01 15:49:08.945521\n",
      "No violations detected for XTM 0102 from 2024-01-01 15:42:34 to 2024-01-01 15:44:13.971789\n",
      "No violations detected for YJ 3 from 2024-01-01 15:54:14 to 2024-01-01 15:55:55.353756\n",
      "No violations detected for CT 9753 from 2024-01-01 15:42:37 to 2024-01-01 15:44:34.204041\n",
      "No violations detected for FWE 3231 from 2024-01-01 15:47:45 to 2024-01-01 15:49:14.835421\n",
      "No violations detected for NB 4 from 2024-01-01 16:00:01 to 2024-01-01 16:01:29.349473\n",
      "No violations detected for XYG 0471 from 2024-01-01 16:00:02 to 2024-01-01 16:01:28.493372\n",
      "No violations detected for AG 2 from 2024-01-01 15:47:41 to 2024-01-01 15:49:40.133353\n",
      "No violations detected for TSB 1 from 2024-01-01 15:54:16 to 2024-01-01 15:55:35.320431\n",
      "No violations detected for SNL 06 from 2024-01-01 15:54:15 to 2024-01-01 15:55:57.218208\n",
      "No violations detected for UKQ 6 from 2024-01-01 16:00:02 to 2024-01-01 16:01:27.052862\n",
      "No violations detected for GEH 6 from 2024-01-01 15:54:16 to 2024-01-01 15:55:50.941645\n",
      "No violations detected for UC 8 from 2024-01-01 15:54:17 to 2024-01-01 15:56:00.452612\n",
      "No violations detected for AZE 1 from 2024-01-01 15:54:13 to 2024-01-01 15:56:04.134784\n",
      "No violations detected for KF 726 from 2024-01-01 15:47:44 to 2024-01-01 15:49:22.753753\n",
      "No violations detected for IJ 7 from 2024-01-01 15:54:13 to 2024-01-01 15:55:47.484648\n",
      "No violations detected for JSD 7 from 2024-01-01 16:18:17 to 2024-01-01 16:19:48.562639\n",
      "No violations detected for PL 19 from 2024-01-01 16:08:21 to 2024-01-01 16:10:04.777822\n",
      "No violations detected for WZM 818 from 2024-01-01 16:08:21 to 2024-01-01 16:10:12.308214\n",
      "No violations detected for MB 368 from 2024-01-01 16:08:20 to 2024-01-01 16:09:49.146074\n",
      "No violations detected for KL 4845 from 2024-01-01 15:54:15 to 2024-01-01 15:55:40.900462\n",
      "No violations detected for FK 4505 from 2024-01-01 16:08:19 to 2024-01-01 16:09:50.091767\n",
      "No violations detected for UVN 92 from 2024-01-01 16:18:19 to 2024-01-01 16:19:40.534861\n",
      "No violations detected for IUB 5734 from 2024-01-01 16:08:17 to 2024-01-01 16:10:05.197334\n",
      "No violations detected for DXQ 942 from 2024-01-01 16:08:22 to 2024-01-01 16:09:49.977624\n",
      "No violations detected for SQO 83 from 2024-01-01 16:18:22 to 2024-01-01 16:19:45.988269\n",
      "No violations detected for QQR 938 from 2024-01-01 16:08:20 to 2024-01-01 16:09:50.104295\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.streaming import StreamingQueryException\n",
    "from Operations import DbWriter\n",
    "\n",
    "\"\"\"\n",
    "Streaming Pipeline for Speed Violation Detection\n",
    "------------------------------------------------\n",
    "This script:\n",
    "1. Cleans up existing Spark streaming checkpoint directory.\n",
    "2. Defines a per-batch data processing function using a custom MongoDB writer.\n",
    "3. Starts a streaming query with custom per-record logic using DbWriter.\n",
    "4. Handles graceful shutdown and error handling.\n",
    "\"\"\"\n",
    "\n",
    "# === Configuration Section ===\n",
    "checkpoint_dir = \"./stream_checkpoints\"  # Directory to store streaming metadata\n",
    "mongo_host = hostip           # MongoDB host IP\n",
    "mongo_port = 27017                       # MongoDB port\n",
    "mongo_db   = \"fit3182_db\"                # Target MongoDB database\n",
    "mongo_coll = \"Violation\"            # Target MongoDB collection\n",
    "\n",
    "# === 1. Clean Up Checkpoint Directory ===\n",
    "# Ensures that each run starts fresh by removing old checkpoints\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    print(f\"Deleted existing checkpoint directory: {checkpoint_dir}\")\n",
    "\n",
    "# === 2. Define Batch Processing Function ===\n",
    "def process_batch(batch_df: DataFrame, batch_id: int):\n",
    "    \"\"\"\n",
    "    Processes each micro-batch of data from the stream using DbWriter and wrapped them\n",
    "    to be compatible with forEachbatch().\n",
    "\n",
    "    Input:\n",
    "        batch_df (DataFrame): Micro-batch dataframe from Spark stream.\n",
    "        batch_id (int): Unique ID assigned by Spark to this batch.\n",
    "    \"\"\"\n",
    "    writer = DbWriter(\n",
    "        mongo_host=mongo_host,\n",
    "        mongo_port=mongo_port,\n",
    "        mongo_db=mongo_db,\n",
    "        mongo_coll=mongo_coll\n",
    "    )\n",
    "\n",
    "    writer.open(partition_id=str(batch_id), epoch_id=str(batch_id))\n",
    "\n",
    "    # Convert DataFrame to RDD for row-level processing\n",
    "    for row in batch_df.rdd.collect():\n",
    "        writer.process(row)\n",
    "\n",
    "    writer.close(None)\n",
    "\n",
    "# Start the stream\n",
    "# The actual streaming query begins here.\n",
    "# Uses DbWriter as a custom foreach sink.\n",
    "query = (\n",
    "    abc_join.writeStream\n",
    "    .format(\"console\")  # Also logs to console for debugging\n",
    "    .option(\"checkpointLocation\", checkpoint_dir)  # Required by Spark for fault tolerance\n",
    "    .outputMode(\"append\")  # Only new rows will be written\n",
    "    .foreachBatch(process_batch)\n",
    "    .option(\"numRows\", 1000)\n",
    "    .option(\"truncate\", False)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Run the Stream and run until its terminated\n",
    "try:\n",
    "    query.awaitTermination()  # Block the thread until terminated\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by CTRL-C. Stopping query.\")\n",
    "except StreamingQueryException as exc:\n",
    "    print(f\"Streaming error: {exc}\")\n",
    "finally:\n",
    "    query.stop()\n",
    "    print(\"Query stopped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce60ed8",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## Data Visualization\n",
    "The data visualization component of the AWAS system is crucial for presenting insights derived from the processed data. We'll leverage both dynamic streaming plots and map visualizations to provide comprehensive and intuitive views of traffic violations and related information. Generally, we can divided this into two type of visualization; **real-time stream visualization** and **map visualization**. \n",
    "\n",
    "----\n",
    "### Real-Time Streaming Visualization\n",
    "In here, we implements a real-time visualization into two-subplot time-series visualization with timestamp_start in as X axis and 2 different kind of y axis; number of violations and measured speed.\n",
    "\n",
    "#### Implement Real-Time Data Streaming\n",
    "Before doing anything we need to enable real-timeness of data from mongoDB when being inserted by the sink writer we implemented earlier through a specialized Kafka stream pipeline. \n",
    "steps:\n",
    "- First, we need to create a **Kafka Producer** that trigger a `send_message()` action everytime a document is inserted to the Collection `Violations`.\n",
    "- Create a **polling** method where every 5 seconds, we search through the collection to get the latest `violations.timestamp_start`.\n",
    "- send the message to consumers.\n",
    "\n",
    "Go to this [file](stream_viz_producer.ipynb) for the implementation\n",
    "\n",
    "#### Message Consuming and Preprocessing\n",
    "After we implement, we need to create a consumer in the [stream visualization](\"stream_visualization.ipynb\") notebook to receive the message, which later we need to preprocess messages to extract violations time, speed, and datetime.\n",
    "Preprocessing steps are outlined here:\n",
    "-  Extract mongoDB document from producer.\n",
    "-  Loop through the list violations\n",
    "-  sort through the timestamp_start\n",
    "-  get the date from timestamp_start and parse it\n",
    "-  append the counter for count only for 10 violations\n",
    "-  append speed into the list speed_container for 10 violations.\n",
    "\n",
    "#### Number of Violations againts TimeStamp\n",
    "Here is all the details we add in here:\n",
    "- X-axis: Displays the \"Hour\" (formatted as `YYYY-MM-DD HH`), representing the aggregated time intervals.\n",
    "- Y-axis: Shows the \"Count\" of violations recorded within each respective hour.\n",
    "- Visualization: A line plot with markers is used to visualize the trend of hourly violations.\n",
    "\n",
    "Annotations added:\n",
    "- Min/Max: The `annotate_count` utility function dynamically labels the minimum and maximum violation counts observed within the current 10-hour window on the plot, providing quick reference points.\n",
    "- Spike Detection: The plot includes logic to detect and annotate `Spike` events. A spike is identified if the current hour's violation count is more than double the `previous_hour_count` (the count from the hour immediately preceding the current latest hour), provided the `previous_hour_count` was greater than zero. A violet arrow points to the spike, highlighting significant increases in violation frequency.\n",
    "\n",
    "#### Measured Speed of Violations against Timestamp\n",
    "\n",
    "Here is all the details we add in here:\n",
    "-   X-axis: Displays the `Timestamp` of each violation, formatted as \"YYYY-MM-DD HH:MM:SS\" to show the precise time of the event.\n",
    "-   Y-axis: Shows the `Speed` of the vehicle at the time of violation, measured in km/h.\n",
    "-   Visualization: A line plot with markers is used to visualize the speed trend over time.\n",
    "\n",
    "Annotations added:\n",
    "-   Min/Max/Avg: The `annotate_speed` utility function dynamically labels the minimum, maximum, and average speed values observed within the current window. A horizontal line also represents the average speed.\n",
    "-   Percentiles: The 50th (median), 75th, and 90th percentiles of the speed data are calculated and displayed as horizontal lines with labels, providing insights into the distribution of speeds.\n",
    "-   Speed Anomalies:\n",
    "    -   High-speed violations (speed > 200 km/h) are marked with a `r*`.\n",
    "    -   Low-speed violations (speed < 120 km/h) are marked with a `b*`.\n",
    "- Sudden speed spikes (current speed > 1.5 \\* previous rolling average) and drops (current speed < 0.6 \\* previous rolling average) are annotated with \"Speed Spike!\" (lime arrow) and \"Speed Drop!\" (red arrow) labels, respectively.\n",
    "\n",
    "[Input an Image in here](\"link\")\n",
    "\n",
    "#### Justification\n",
    "We add this different annotated points such as min/max/avg to show beahviour within certain windows. Of course as the windows are fixed size in this case, we add another spike detection in violations count to see any anomalies between hour to flag any anomalies between or inside the 10-hour window. Visual cues such as spike detection and percentiles are important to give a wide view of behaviours in a otherwise changing streamed plots. \n",
    "\n",
    "Correlations between 2 plots:\n",
    "- Examining the percentile lines and the overall distribution of speeds on the speed plot during hours with high violation counts can provide further context. \n",
    "- Occurence of low-speed violations clustered in a high count period may indicate a high-traffic pattern which can incentivize driver to violate slightly.\n",
    "- Occurence of a high-speed violations with a 90th percentile speed data in horizontal lines reveal what time period drivers tend to drive recklessly, going overboard from the predetermined limit. This allows for targeted analysis of factors contributing to extreme speeding during those specific hours.\n",
    "- Occurence of Violation Spikes with a rolling average of speed indicate there is a sudden influx in driving speed within the traffic flow contributes to a higher number of violations.\n",
    "- Speed anomalies being marked are paired with an increase in average of violation count or a sudden spike.\n",
    "\n",
    "Future Analytical Work:\n",
    "- Instead of hourly violation counts, we could aggregate the data to show daily, weekly, or even monthly trends in violation frequency. This would reveal longer-term patterns and seasonal variations.\n",
    "-  Use historical data (counts and potentially speed patterns) to build time series models (like ARIMA or Prophet) to forecast future violation counts at different granularities (hourly, daily).\n",
    "- Create a flagging violation system based on car and repeat offenders where repat offendeer over 5 are a labelled as higher priority.\n",
    "- Do an external analysis and correlate in with external dataset such as weather data and holiday data.\n",
    "\n",
    "\n",
    "Documentation to refer for this:\n",
    "-   [Python notebook for visualization](\"stream_visualization.ipynb\")\n",
    "-   [Matplotlib official documentation](\"https://matplotlib.org/\")\n",
    "-   [numpy official documentation](\"https://numpy.org/doc/stable/reference/routines.statistics.html\")\n",
    "\n",
    "----\n",
    "## Map Visualization\n",
    "This section details the implementation of a map visualization using the Folium library to display camera locations, annotate the number of violations between checkpoints, and identify violation hotspots.\n",
    " \n",
    "### Implementation Steps\n",
    "following steps we do in here:\n",
    "- Instanitate connection with a mongoDB collections\n",
    "- Prepare the collection similar to the previous one.\n",
    "- Load the camera location from the collection `Camera`\n",
    "- Fetch violation based on the date inside the collection `Violation`\n",
    "- Compute where the location of the map to zoom in from the camera location we collect\n",
    "- Annotate Violations Between Checkpoints as a cluster.\n",
    "- Add segment polylines.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748b82f",
   "metadata": {
    "id": "f748b82f"
   },
   "source": [
    "## <span style=\"color:#0b486b\">Part 3: Documentation and comments to describe the proposed solution in the submitted notebook</span>\n",
    "\n",
    "You should include sufficient comments and explanation Tasks 1 and 2 to describe your algorithm and/or code implementation. Please add additional markdown cells to explain your work. Adding extra illustrations to describe your method will also add to the marks in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f29b622",
   "metadata": {
    "id": "1f29b622"
   },
   "source": [
    "## <span style=\"color:#0b486b\">Part 4: Code demo and interview</span>\n",
    "\n",
    "In this task, you will present and showcase the simulation. After the assignment due date, you will be asked to attend an interview/demo session to showcase your application. Your interviewer will ask you a few questions in relation to your application and assess your understanding.\n",
    "\n",
    "During the code demo, your work will be evaluated and assessed based on the marking guideline. Group members will obtain the same marks based on the code demo, unless there is an imbalance in contributions between students in a team. Additionally, each team member will be interviewed to explain the submitted work. The interview represents an individual assessment and a score between 0 and 1 will be awarded, which is then multipled with the marks obtained during the code demo.\n",
    "\n",
    "Interviews for Assignment-2 will be conducted during Week 12 lab sessions. If you are granted an extension from special consideration, the interview will be conducted during SWOT-VAC week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0vMU09gMgOTY",
   "metadata": {
    "id": "0vMU09gMgOTY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
