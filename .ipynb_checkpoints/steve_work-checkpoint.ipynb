{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# add the folder where util.py lives\n",
    "sys.path.append(os.path.join(os.getcwd(), 'fit3182-a2'))\n",
    "sys.path.insert(0, os.path.abspath(os.getcwd()))\n",
    "\n",
    "from pyspark.sql.functions import udf, col, window\n",
    "from pyspark.sql.types import StringType\n",
    "from util import kafkaConsumer\n",
    "import pandas as pd\n",
    "from Operations import SparkInst, DbWriter, ConsoleWriter\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.streaming import StreamingQueryException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_job=SparkInst(\"AWAS SYSTEM\", 5, \"violations\")\n",
    "hostip = \"172.22.32.1\"\n",
    "port_spark = 9092\n",
    "port_mongo = 27017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_pd = pd.read_csv(\"data/camera.csv\")\n",
    "if '_id' in df_pd.columns:\n",
    "    df_pd.drop(columns=['_id'], inplace=True)\n",
    "spark_df = spark_job.get_session().createDataFrame(df_pd)\n",
    "\n",
    "# Step 3: Broadcast your speed limit map\n",
    "speed_limit_map = {row['camera_id']: row['speed_limit'] for row in spark_df.select(\"camera_id\", \"speed_limit\").collect()}\n",
    "broadcast_map = spark_job.essentialData_broadcast(spark_df)\n",
    "# Step 4: Define your UDF using broadcast variable\n",
    "def mark_speeding(camera_id:str, speed:float, ops:str):\n",
    "    limit = broadcast_map.value.get(camera_id)\n",
    "    if limit is not None and ops == \"instant\":\n",
    "        return \"INSTANT_VIOLATION\" if speed > limit else None\n",
    "    elif limit is not None and ops == \"average\":\n",
    "        return \"AVERAGE_VIOLATION\" if speed > limit else None\n",
    "    return \"NONE\"\n",
    "\n",
    "speeding_udf = udf(mark_speeding, StringType())\n",
    "\n",
    "# Step 5: Apply UDF to each streaming dataframe\n",
    "def add_speed_flag(df, ops: str):\n",
    "  return df.withColumn(\"speed_flag\",\n",
    "                       speeding_udf(col(\"camera_id\"), col(\"speed_reading\"), lit(ops)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "# Attach Kafka streams\n",
    "stream_a = spark_job.attach_kafka_stream(\"camera_event_a\", hostip, \"5 minutes\",port_spark)\n",
    "stream_b = spark_job.attach_kafka_stream(\"camera_event_b\", hostip, \"5 minutes\",port_spark)\n",
    "stream_c = spark_job.attach_kafka_stream(\"camera_event_c\", hostip, \"5 minutes\",port_spark)\n",
    "\n",
    "# Apply transformation\n",
    "stream_a_flagged = add_speed_flag(stream_a, \"instant\")\n",
    "stream_b_flagged = add_speed_flag(stream_b, \"instant\")\n",
    "stream_c_flagged = add_speed_flag(stream_c, \"instant\")\n",
    "\n",
    "# Duplicate stream_b\n",
    "dup1_b = stream_b\n",
    "dup2_b = stream_b\n",
    "\n",
    "# Apply watermarks\n",
    "stream_a_watermarked  = stream_a.withWatermark(\"timestamp\",  \"5 minutes\")\n",
    "stream_b1_watermarked = dup1_b.withWatermark(\"timestamp\",  \"5 minutes\")\n",
    "stream_c_watermarked  = stream_c.withWatermark(\"timestamp\",  \"5 minutes\")\n",
    "stream_b2_watermarked = dup2_b.withWatermark(\"timestamp\", \"5 minutes\")\n",
    "\n",
    "# Join A and B on car_plate and timestamp condition\n",
    "ab_join = (\n",
    "    stream_a_watermarked.alias(\"A\")\n",
    "    .join(\n",
    "        stream_b1_watermarked.alias(\"B\"),\n",
    "        (col(\"A.car_plate\") == col(\"B.car_plate\")),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"A.*\"),\n",
    "        col(\"B.batch_id\").alias(\"batch_id_B\"),\n",
    "        col(\"B.car_plate\").alias(\"car_plate_B\"),\n",
    "        col(\"B.camera_id\").alias(\"camera_id_B\"),\n",
    "        col(\"B.timestamp\").alias(\"timestamp_B\"),\n",
    "        col(\"B.speed_reading\").alias(\"speed_reading_B\"),\n",
    "        col(\"B.sent_at\").alias(\"timestamp_B\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# ab_join = (\n",
    "#     stream_a_watermarked.alias(\"A\")\n",
    "#       .join(\n",
    "#           stream_b1_watermarked.alias(\"B\"),\n",
    "#           (col(\"A.car_plate\") == col(\"B.car_plate\"))\n",
    "#           & (col(\"A.timestamp\") < col(\"B.timestamp\"))\n",
    "#           & (col(\"A.timestamp\") + expr(\"INTERVAL 5 MINUTES\") >= col(\"B.timestamp\")),\n",
    "#           how=\"inner\"\n",
    "#       )\n",
    "#       .withColumn(\n",
    "#           \"avg_measure_speed\",\n",
    "#           (col(\"A.speed_reading\") + col(\"B.speed_reading\")) / 2\n",
    "#       )\n",
    "#       .withColumn(\n",
    "#           \"speed_flag\",\n",
    "#           speeding_udf(col(\"B.camera_id\"), col(\"avg_measure_speed\"), lit(\"average\"))\n",
    "#       )\n",
    "# )\n",
    "\n",
    "# # Join B and C on car_plate and timestamp condition\n",
    "# bc_join = stream_b2_watermarked.join(\n",
    "#     stream_c_watermarked,\n",
    "#     expr(\"\"\"\n",
    "#         car_plate = car_plate AND\n",
    "#         timestamp_c > timestamp_b2 AND\n",
    "#         timestamp_c <= timestamp_b2 + interval 10 minutes\n",
    "#     \"\"\"),\n",
    "#     \"inner\"\n",
    "# ).withColumn(\n",
    "#     \"avg_measure_speed\",\n",
    "#     (col(\"measure_speed_a\") + col(\"measure_speed_b\")) / 2\n",
    "# ).withColumn(\"speed_flag\", speeding_udf(col(\"camera_id\"), col(\"speed_reading\"), \"average\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by CTRL-C. Stopped query\n"
     ]
    }
   ],
   "source": [
    "spark = ab_join\n",
    "mongo_uri = f\"mongodb://{hostip}:{port_mongo}\"\n",
    "db_name   = \"fit3182_db\"\n",
    "coll_name = \"Violation\"\n",
    "\n",
    "# writer = DbWriter(\n",
    "#     spark,\n",
    "#     mongo_uri,\n",
    "#     mongo_db=   db_name,\n",
    "#     mongo_collection= coll_name\n",
    "# )\n",
    "\n",
    "writer_stream = (\n",
    "    AB\n",
    "      .writeStream.format(\"Console\")\n",
    "      .option(\"checkpointLocation\", \"./stream_checkpoints\")\n",
    "      .outputMode(\"append\")\n",
    "      .foreach(ConsoleWriter())     \n",
    ")\n",
    "try:\n",
    "    query = writer_stream.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopped query')\n",
    "except StreamingQueryException as exc:\n",
    "    print(exc)\n",
    "finally:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bc_join' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ab_join\u001b[38;5;241m.\u001b[39munion(\u001b[43mbc_join\u001b[49m, window(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m24 hours\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mselect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bc_join' is not defined"
     ]
    }
   ],
   "source": [
    "ab_join.union(bc_join, window(col(\"timestamp\"), \"24 hours\")).dropna().select()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
